{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Airbnb-price-prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoohanko98/Airbnb-recommender/blob/main/Airbnb_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_srLJTlDa8Kx"
      },
      "source": [
        "# Airbnb-price-prediction\n",
        "### EE695 Final Project\n",
        "Yoohan Ko, Alex Walker, Marcin Wisniowski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsAz13iNfJvk"
      },
      "source": [
        "**Problem Statement**\n",
        "> Many Airbnb hosts may have a hard time deciding how to best price their rental in current market conditions, and recommendations are to manually examine competing host’s prices. Too high of a price may mean no renters and too low of a price could be potential income wasted. The group would like to look at Airbnb listings from March to September of 2020 in Jersey City to come up with models these hosts could use for setting an ideal price, and also factor in Covid cases as a parameter, if it ends up being applicable.\n",
        "\n",
        "**Implementation Plan**\n",
        "> Using these data sources, it will be possible to correlate the parameters of each listing to the price at each property and hopefully see a correlation between Airbnb listing prices and Covid cases. The group will look through the data at the beginning to try to find any variables that show correlation to prices from late October to Early November. Early to late November will include implementation and tweaking the Decision tree and SVM algorithms. Late November to early December will include implementing the neural network algorithm. All of these models will be supervised regression to a price value.\n",
        "\n",
        "**Team Members and Task Allocation**\n",
        "- All - Identify and propose important data parameters/clean data\n",
        "- Yoohan Ko - Specialize in SVM algorithm, secondary in Neural Network\n",
        "- Alex Walker - Specialize in Neural Network, secondary in Decision Tree\n",
        "- Marcin Wisniowski - Specialize in Decision Tree, secondary in SVM\n",
        "\n",
        "testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-OLMEsngMBD"
      },
      "source": [
        "# **Index**: \n",
        "*   [Pre-processing](#pre-processing)\n",
        "*   [Neural Network](#neural_network)\n",
        "*   [SVM](#svm)\n",
        "*   [Decision Tree](#dtc)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAZ6xPtNbOgX"
      },
      "source": [
        "#**Pre-processing** <a name=\"pre-processing\"></a>\n",
        "#### **Importing the data & libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xjj1Cypbi7l"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "rZ3QU0hib3J4",
        "outputId": "648e9434-fc44-4709-c8e1-add7c1e7930c"
      },
      "source": [
        "# data is stored in git repo\n",
        "url = 'https://raw.githubusercontent.com/yoohanko98/Airbnb-recommender/main/listings.csv'\n",
        "\n",
        "dataset = pd.read_csv(url)\n",
        "print(f\"The dataset contains {len(dataset)} Airbnb listings\")\n",
        "pd.set_option('display.max_columns', len(dataset.columns)) # View all columns\n",
        "pd.set_option('display.max_rows', 100)\n",
        "dataset.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset contains 1428 Airbnb listings\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>listing_url</th>\n",
              "      <th>scrape_id</th>\n",
              "      <th>last_scraped</th>\n",
              "      <th>name</th>\n",
              "      <th>description</th>\n",
              "      <th>neighborhood_overview</th>\n",
              "      <th>picture_url</th>\n",
              "      <th>host_id</th>\n",
              "      <th>host_url</th>\n",
              "      <th>host_name</th>\n",
              "      <th>host_since</th>\n",
              "      <th>host_location</th>\n",
              "      <th>host_about</th>\n",
              "      <th>host_response_time</th>\n",
              "      <th>host_response_rate</th>\n",
              "      <th>host_acceptance_rate</th>\n",
              "      <th>host_is_superhost</th>\n",
              "      <th>host_thumbnail_url</th>\n",
              "      <th>host_picture_url</th>\n",
              "      <th>host_neighbourhood</th>\n",
              "      <th>host_listings_count</th>\n",
              "      <th>host_total_listings_count</th>\n",
              "      <th>host_verifications</th>\n",
              "      <th>host_has_profile_pic</th>\n",
              "      <th>host_identity_verified</th>\n",
              "      <th>neighbourhood</th>\n",
              "      <th>neighbourhood_cleansed</th>\n",
              "      <th>neighbourhood_group_cleansed</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>property_type</th>\n",
              "      <th>room_type</th>\n",
              "      <th>accommodates</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>bathrooms_text</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>beds</th>\n",
              "      <th>amenities</th>\n",
              "      <th>price</th>\n",
              "      <th>minimum_nights</th>\n",
              "      <th>maximum_nights</th>\n",
              "      <th>minimum_minimum_nights</th>\n",
              "      <th>maximum_minimum_nights</th>\n",
              "      <th>minimum_maximum_nights</th>\n",
              "      <th>maximum_maximum_nights</th>\n",
              "      <th>minimum_nights_avg_ntm</th>\n",
              "      <th>maximum_nights_avg_ntm</th>\n",
              "      <th>calendar_updated</th>\n",
              "      <th>has_availability</th>\n",
              "      <th>availability_30</th>\n",
              "      <th>availability_60</th>\n",
              "      <th>availability_90</th>\n",
              "      <th>availability_365</th>\n",
              "      <th>calendar_last_scraped</th>\n",
              "      <th>number_of_reviews</th>\n",
              "      <th>number_of_reviews_ltm</th>\n",
              "      <th>number_of_reviews_l30d</th>\n",
              "      <th>first_review</th>\n",
              "      <th>last_review</th>\n",
              "      <th>review_scores_rating</th>\n",
              "      <th>review_scores_accuracy</th>\n",
              "      <th>review_scores_cleanliness</th>\n",
              "      <th>review_scores_checkin</th>\n",
              "      <th>review_scores_communication</th>\n",
              "      <th>review_scores_location</th>\n",
              "      <th>review_scores_value</th>\n",
              "      <th>license</th>\n",
              "      <th>instant_bookable</th>\n",
              "      <th>calculated_host_listings_count</th>\n",
              "      <th>calculated_host_listings_count_entire_homes</th>\n",
              "      <th>calculated_host_listings_count_private_rooms</th>\n",
              "      <th>calculated_host_listings_count_shared_rooms</th>\n",
              "      <th>reviews_per_month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40669</td>\n",
              "      <td>https://www.airbnb.com/rooms/40669</td>\n",
              "      <td>20201025134822</td>\n",
              "      <td>2020-10-26</td>\n",
              "      <td>Skyy’s Lounge / Cozy</td>\n",
              "      <td>&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Skyy’s Lounge ....Everyt...</td>\n",
              "      <td>The neighborhood is very diverse &amp; friendly sh...</td>\n",
              "      <td>https://a0.muscache.com/pictures/af7e4a45-0118...</td>\n",
              "      <td>175412</td>\n",
              "      <td>https://www.airbnb.com/users/show/175412</td>\n",
              "      <td>Skyy</td>\n",
              "      <td>2010-07-20</td>\n",
              "      <td>Jersey City, New Jersey, United States</td>\n",
              "      <td>I am the owner of a high end Nail Salon in the...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>https://a0.muscache.com/im/users/175412/profil...</td>\n",
              "      <td>https://a0.muscache.com/im/users/175412/profil...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>['email', 'phone', 'facebook', 'reviews']</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>Jersey City, New Jersey, United States</td>\n",
              "      <td>Ward C (councilmember Richard Boggiano)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.73742</td>\n",
              "      <td>-74.05255</td>\n",
              "      <td>Private room in condominium</td>\n",
              "      <td>Private room</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 shared bath</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[\"Carbon monoxide alarm\", \"Hair dryer\", \"Lugga...</td>\n",
              "      <td>$82.00</td>\n",
              "      <td>3</td>\n",
              "      <td>365</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>365</td>\n",
              "      <td>365</td>\n",
              "      <td>3.0</td>\n",
              "      <td>365.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>t</td>\n",
              "      <td>28</td>\n",
              "      <td>58</td>\n",
              "      <td>88</td>\n",
              "      <td>363</td>\n",
              "      <td>2020-10-26</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010-09-23</td>\n",
              "      <td>2019-10-12</td>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63282</td>\n",
              "      <td>https://www.airbnb.com/rooms/63282</td>\n",
              "      <td>20201025134822</td>\n",
              "      <td>2020-10-26</td>\n",
              "      <td>2bed/2bath,furnished,doorman, by NY</td>\n",
              "      <td>&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;MINIMUM STAY OF 5 MONTHS...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://a0.muscache.com/pictures/388465/eb5f4f...</td>\n",
              "      <td>304762</td>\n",
              "      <td>https://www.airbnb.com/users/show/304762</td>\n",
              "      <td>Gil</td>\n",
              "      <td>2010-11-30</td>\n",
              "      <td>New York, New York, United States</td>\n",
              "      <td>Very low-impact traveler. I'll treat your plac...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>https://a0.muscache.com/im/users/304762/profil...</td>\n",
              "      <td>https://a0.muscache.com/im/users/304762/profil...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['email', 'phone', 'jumio', 'offline_governmen...</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ward B (councilmember Mira Prinz-Arey)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.72813</td>\n",
              "      <td>-74.07037</td>\n",
              "      <td>Entire apartment</td>\n",
              "      <td>Entire home/apt</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2 baths</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[\"Carbon monoxide alarm\", \"Elevator\", \"Dryer\",...</td>\n",
              "      <td>$2,000.00</td>\n",
              "      <td>150</td>\n",
              "      <td>730</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>730</td>\n",
              "      <td>730</td>\n",
              "      <td>150.0</td>\n",
              "      <td>730.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>t</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>90</td>\n",
              "      <td>365</td>\n",
              "      <td>2020-10-26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>146144</td>\n",
              "      <td>https://www.airbnb.com/rooms/146144</td>\n",
              "      <td>20201025134822</td>\n",
              "      <td>2020-10-26</td>\n",
              "      <td>Shared Room</td>\n",
              "      <td>&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Hi,&lt;br /&gt;Well, this is a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://a0.muscache.com/pictures/923609/cf3964...</td>\n",
              "      <td>266070</td>\n",
              "      <td>https://www.airbnb.com/users/show/266070</td>\n",
              "      <td>Patricia</td>\n",
              "      <td>2010-10-19</td>\n",
              "      <td>Florence, Tuscany, Italy</td>\n",
              "      <td>I am Executive Director of a global health non...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>https://a0.muscache.com/im/users/266070/profil...</td>\n",
              "      <td>https://a0.muscache.com/im/users/266070/profil...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['email', 'phone', 'reviews', 'kba']</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ward E (councilmember James Solomon)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.71077</td>\n",
              "      <td>-74.03833</td>\n",
              "      <td>Shared room in apartment</td>\n",
              "      <td>Shared room</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>$200.00</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>t</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>90</td>\n",
              "      <td>365</td>\n",
              "      <td>2020-10-26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                          listing_url       scrape_id last_scraped  \\\n",
              "0   40669   https://www.airbnb.com/rooms/40669  20201025134822   2020-10-26   \n",
              "1   63282   https://www.airbnb.com/rooms/63282  20201025134822   2020-10-26   \n",
              "2  146144  https://www.airbnb.com/rooms/146144  20201025134822   2020-10-26   \n",
              "\n",
              "                                  name  \\\n",
              "0                 Skyy’s Lounge / Cozy   \n",
              "1  2bed/2bath,furnished,doorman, by NY   \n",
              "2                          Shared Room   \n",
              "\n",
              "                                         description  \\\n",
              "0  <b>The space</b><br />Skyy’s Lounge ....Everyt...   \n",
              "1  <b>The space</b><br />MINIMUM STAY OF 5 MONTHS...   \n",
              "2  <b>The space</b><br />Hi,<br />Well, this is a...   \n",
              "\n",
              "                               neighborhood_overview  \\\n",
              "0  The neighborhood is very diverse & friendly sh...   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "\n",
              "                                         picture_url  host_id  \\\n",
              "0  https://a0.muscache.com/pictures/af7e4a45-0118...   175412   \n",
              "1  https://a0.muscache.com/pictures/388465/eb5f4f...   304762   \n",
              "2  https://a0.muscache.com/pictures/923609/cf3964...   266070   \n",
              "\n",
              "                                   host_url host_name  host_since  \\\n",
              "0  https://www.airbnb.com/users/show/175412      Skyy  2010-07-20   \n",
              "1  https://www.airbnb.com/users/show/304762       Gil  2010-11-30   \n",
              "2  https://www.airbnb.com/users/show/266070  Patricia  2010-10-19   \n",
              "\n",
              "                            host_location  \\\n",
              "0  Jersey City, New Jersey, United States   \n",
              "1       New York, New York, United States   \n",
              "2                Florence, Tuscany, Italy   \n",
              "\n",
              "                                          host_about host_response_time  \\\n",
              "0  I am the owner of a high end Nail Salon in the...                NaN   \n",
              "1  Very low-impact traveler. I'll treat your plac...                NaN   \n",
              "2  I am Executive Director of a global health non...                NaN   \n",
              "\n",
              "  host_response_rate host_acceptance_rate host_is_superhost  \\\n",
              "0                NaN                  NaN                 f   \n",
              "1                NaN                  NaN                 f   \n",
              "2                NaN                  NaN                 f   \n",
              "\n",
              "                                  host_thumbnail_url  \\\n",
              "0  https://a0.muscache.com/im/users/175412/profil...   \n",
              "1  https://a0.muscache.com/im/users/304762/profil...   \n",
              "2  https://a0.muscache.com/im/users/266070/profil...   \n",
              "\n",
              "                                    host_picture_url host_neighbourhood  \\\n",
              "0  https://a0.muscache.com/im/users/175412/profil...                NaN   \n",
              "1  https://a0.muscache.com/im/users/304762/profil...                NaN   \n",
              "2  https://a0.muscache.com/im/users/266070/profil...                NaN   \n",
              "\n",
              "   host_listings_count  host_total_listings_count  \\\n",
              "0                    2                          2   \n",
              "1                    1                          1   \n",
              "2                    1                          1   \n",
              "\n",
              "                                  host_verifications host_has_profile_pic  \\\n",
              "0          ['email', 'phone', 'facebook', 'reviews']                    t   \n",
              "1  ['email', 'phone', 'jumio', 'offline_governmen...                    t   \n",
              "2               ['email', 'phone', 'reviews', 'kba']                    t   \n",
              "\n",
              "  host_identity_verified                           neighbourhood  \\\n",
              "0                      f  Jersey City, New Jersey, United States   \n",
              "1                      t                                     NaN   \n",
              "2                      t                                     NaN   \n",
              "\n",
              "                    neighbourhood_cleansed  neighbourhood_group_cleansed  \\\n",
              "0  Ward C (councilmember Richard Boggiano)                           NaN   \n",
              "1   Ward B (councilmember Mira Prinz-Arey)                           NaN   \n",
              "2     Ward E (councilmember James Solomon)                           NaN   \n",
              "\n",
              "   latitude  longitude                property_type        room_type  \\\n",
              "0  40.73742  -74.05255  Private room in condominium     Private room   \n",
              "1  40.72813  -74.07037             Entire apartment  Entire home/apt   \n",
              "2  40.71077  -74.03833     Shared room in apartment      Shared room   \n",
              "\n",
              "   accommodates  bathrooms bathrooms_text  bedrooms  beds  \\\n",
              "0             2        NaN  1 shared bath       1.0   0.0   \n",
              "1             4        NaN        2 baths       2.0   3.0   \n",
              "2             1        NaN            NaN       1.0   1.0   \n",
              "\n",
              "                                           amenities      price  \\\n",
              "0  [\"Carbon monoxide alarm\", \"Hair dryer\", \"Lugga...     $82.00   \n",
              "1  [\"Carbon monoxide alarm\", \"Elevator\", \"Dryer\",...  $2,000.00   \n",
              "2                                                 []    $200.00   \n",
              "\n",
              "   minimum_nights  maximum_nights  minimum_minimum_nights  \\\n",
              "0               3             365                       3   \n",
              "1             150             730                     150   \n",
              "2               2               2                       2   \n",
              "\n",
              "   maximum_minimum_nights  minimum_maximum_nights  maximum_maximum_nights  \\\n",
              "0                       3                     365                     365   \n",
              "1                     150                     730                     730   \n",
              "2                       2                       2                       2   \n",
              "\n",
              "   minimum_nights_avg_ntm  maximum_nights_avg_ntm  calendar_updated  \\\n",
              "0                     3.0                   365.0               NaN   \n",
              "1                   150.0                   730.0               NaN   \n",
              "2                     2.0                     2.0               NaN   \n",
              "\n",
              "  has_availability  availability_30  availability_60  availability_90  \\\n",
              "0                t               28               58               88   \n",
              "1                t               30               60               90   \n",
              "2                t               30               60               90   \n",
              "\n",
              "   availability_365 calendar_last_scraped  number_of_reviews  \\\n",
              "0               363            2020-10-26                 10   \n",
              "1               365            2020-10-26                  0   \n",
              "2               365            2020-10-26                  0   \n",
              "\n",
              "   number_of_reviews_ltm  number_of_reviews_l30d first_review last_review  \\\n",
              "0                      0                       0   2010-09-23  2019-10-12   \n",
              "1                      0                       0          NaN         NaN   \n",
              "2                      0                       0          NaN         NaN   \n",
              "\n",
              "   review_scores_rating  review_scores_accuracy  review_scores_cleanliness  \\\n",
              "0                 100.0                    10.0                       10.0   \n",
              "1                   NaN                     NaN                        NaN   \n",
              "2                   NaN                     NaN                        NaN   \n",
              "\n",
              "   review_scores_checkin  review_scores_communication  review_scores_location  \\\n",
              "0                   10.0                         10.0                    10.0   \n",
              "1                    NaN                          NaN                     NaN   \n",
              "2                    NaN                          NaN                     NaN   \n",
              "\n",
              "   review_scores_value  license instant_bookable  \\\n",
              "0                 10.0      NaN                f   \n",
              "1                  NaN      NaN                f   \n",
              "2                  NaN      NaN                f   \n",
              "\n",
              "   calculated_host_listings_count  \\\n",
              "0                               2   \n",
              "1                               1   \n",
              "2                               1   \n",
              "\n",
              "   calculated_host_listings_count_entire_homes  \\\n",
              "0                                            0   \n",
              "1                                            1   \n",
              "2                                            0   \n",
              "\n",
              "   calculated_host_listings_count_private_rooms  \\\n",
              "0                                             2   \n",
              "1                                             0   \n",
              "2                                             0   \n",
              "\n",
              "   calculated_host_listings_count_shared_rooms  reviews_per_month  \n",
              "0                                            0               0.08  \n",
              "1                                            0                NaN  \n",
              "2                                            1                NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywOZBF9Ii3m_"
      },
      "source": [
        "# Select columns\n",
        "cols_to_include = ['room_type', 'bathrooms_text','bedrooms','review_scores_rating', 'number_of_reviews', 'longitude', 'latitude', 'accommodates','price']\n",
        "\n",
        "def float_bathrooms_text(x):\n",
        "    if isinstance(x, float):\n",
        "        return None\n",
        "    \n",
        "    if ('half' in x.lower()):\n",
        "        return 0.5\n",
        "    else:\n",
        "        return float(x.split(' ')[0])\n",
        "\n",
        "testing_portion = 0.20\n",
        "\n",
        "# Read and select desired fields\n",
        "dataset = dataset[cols_to_include]\n",
        "\n",
        "# Process as average\n",
        "#dataset['review_scores_rating'].fillna((dataset['review_scores_rating'].mean()), inplace=True)\n",
        "\n",
        "# Process as floats\n",
        "dataset['bathrooms_text'] = dataset[\"bathrooms_text\"].apply(lambda x: float_bathrooms_text(x))\n",
        "dataset['room_type'] = dataset[\"room_type\"].map({'Entire home/apt': 1.0, 'Private room': 2.0, \n",
        "'Shared room': 3.0, 'Hotel room': 4.0})\n",
        "\n",
        "dataset['price'] = (dataset['price'].replace( '[\\$,)]','', regex=True )\n",
        "               .replace( '[(]','-',   regex=True ).astype(float))\n",
        "\n",
        "# Drop nulls\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "# Separate x and y\n",
        "x = dataset[['room_type', 'bathrooms_text', 'bedrooms','review_scores_rating', 'number_of_reviews', 'longitude', 'latitude', 'accommodates']]\n",
        "y = dataset['price']\n",
        "\n",
        "# Testing and training split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=testing_portion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYXd1Y9hezWL"
      },
      "source": [
        "## Neural Network <a name=\"neural_network\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A_fR4DqVPgt",
        "outputId": "190f1d9b-6a56-47ba-a270-cee1a08746bd"
      },
      "source": [
        "\n",
        "\n",
        "print('X Training')\n",
        "print(x_train.head)\n",
        "print('\\n')\n",
        "print('Y Training')\n",
        "print(y_train.head)\n",
        "print('\\n')\n",
        "print('X Testing')\n",
        "print(x_test.head)\n",
        "print('\\n')\n",
        "print('Y Testing')\n",
        "print(y_test.head)\n",
        "print('\\n')\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "print('X Train Scaled:')\n",
        "print(x_train)\n",
        "print('X Test Scaled:')\n",
        "print(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Training\n",
            "<bound method NDFrame.head of       room_type  bathrooms_text  bedrooms  ...  longitude  latitude  accommodates\n",
            "717         2.0             1.0       1.0  ...  -74.07642  40.71002             2\n",
            "496         2.0             1.0       1.0  ...  -74.09234  40.69183             2\n",
            "357         1.0             2.0       2.0  ...  -74.03596  40.72391             5\n",
            "794         1.0             1.0       2.0  ...  -74.05843  40.73346             8\n",
            "1223        1.0             2.0       2.0  ...  -74.04860  40.72473             4\n",
            "...         ...             ...       ...  ...        ...       ...           ...\n",
            "186         2.0             1.0       1.0  ...  -74.06262  40.71176             2\n",
            "175         1.0             1.0       1.0  ...  -74.03658  40.72413             3\n",
            "588         1.0             1.0       1.0  ...  -74.04688  40.72575             3\n",
            "1192        1.0             1.0       2.0  ...  -74.04298  40.75179             6\n",
            "971         2.0             2.0       1.0  ...  -74.07907  40.71373             2\n",
            "\n",
            "[719 rows x 8 columns]>\n",
            "\n",
            "\n",
            "Y Training\n",
            "<bound method NDFrame.head of 717      33.0\n",
            "496      32.0\n",
            "357     223.0\n",
            "794     121.0\n",
            "1223    110.0\n",
            "        ...  \n",
            "186      40.0\n",
            "175     129.0\n",
            "588      80.0\n",
            "1192    148.0\n",
            "971      32.0\n",
            "Name: price, Length: 719, dtype: float64>\n",
            "\n",
            "\n",
            "X Testing\n",
            "<bound method NDFrame.head of      room_type  bathrooms_text  bedrooms  ...  longitude  latitude  accommodates\n",
            "79         1.0             1.0       1.0  ...  -74.03679  40.72284             3\n",
            "180        1.0             2.0       2.0  ...  -74.03658  40.72413             5\n",
            "213        1.0             2.0       2.0  ...  -74.03672  40.72213             5\n",
            "964        1.0             1.0       2.0  ...  -74.04909  40.72623             5\n",
            "187        2.0             1.0       1.0  ...  -74.04854  40.72023             1\n",
            "..         ...             ...       ...  ...        ...       ...           ...\n",
            "598        1.0             2.0       3.0  ...  -74.04388  40.72486             9\n",
            "704        2.0             2.0       1.0  ...  -74.04913  40.75718             4\n",
            "465        1.0             1.0       1.0  ...  -74.06666  40.73714             8\n",
            "723        1.0             2.0       4.0  ...  -74.08192  40.72935             7\n",
            "347        1.0             2.0       4.0  ...  -74.04414  40.72502            10\n",
            "\n",
            "[180 rows x 8 columns]>\n",
            "\n",
            "\n",
            "Y Testing\n",
            "<bound method NDFrame.head of 79     129.0\n",
            "180    159.0\n",
            "213    219.0\n",
            "964    146.0\n",
            "187     55.0\n",
            "       ...  \n",
            "598    175.0\n",
            "704     25.0\n",
            "465    100.0\n",
            "723     86.0\n",
            "347    999.0\n",
            "Name: price, Length: 180, dtype: float64>\n",
            "\n",
            "\n",
            "X Train Scaled:\n",
            "[[ 1.24877841 -0.54617452 -0.55404558 ... -1.11780498 -1.0935932\n",
            "  -0.70370382]\n",
            " [ 1.24877841 -0.54617452 -0.55404558 ... -2.02880848 -2.32506263\n",
            "  -0.70370382]\n",
            " [-0.7734551   1.0765506   0.54639301 ...  1.1974715  -0.15323529\n",
            "   0.32607172]\n",
            " ...\n",
            " [-0.7734551  -0.54617452 -0.55404558 ...  0.57258719 -0.02866664\n",
            "  -0.36044531]\n",
            " [-0.7734551  -0.54617452  0.54639301 ...  0.79576016  1.73425056\n",
            "   0.66933023]\n",
            " [ 1.24877841  1.0765506  -0.55404558 ... -1.26944815 -0.84242489\n",
            "  -0.70370382]]\n",
            "X Test Scaled:\n",
            "[[-0.7734551  -0.54617452 -0.55404558 ...  1.14997571 -0.22567467\n",
            "  -0.36044531]\n",
            " [-0.7734551   1.0765506   0.54639301 ...  1.16199272 -0.13834121\n",
            "   0.32607172]\n",
            " [-0.7734551   1.0765506   0.54639301 ...  1.15398138 -0.27374192\n",
            "   0.32607172]\n",
            " ...\n",
            " [-0.7734551  -0.54617452 -0.55404558 ... -0.55930032  0.74244038\n",
            "   1.35584725]\n",
            " [-0.7734551   1.0765506   2.7472702  ... -1.43253609  0.21505463\n",
            "   1.01258874]\n",
            " [-0.7734551   1.0765506   2.7472702  ...  0.7293805  -0.0780879\n",
            "   2.04236428]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNzEQ6tKVPgv",
        "outputId": "2730015f-3bf9-4e51-f58b-e767f9d0f58c"
      },
      "source": [
        "layer1 = (60,30)\n",
        "alpha = 0.0001\n",
        "max_iter = 7500\n",
        "\n",
        "# Give params for neural network\n",
        "clf = MLPRegressor(hidden_layer_sizes=layer1, activation='logistic', solver='adam', verbose=True, alpha=alpha, max_iter=max_iter)\n",
        "\n",
        "# Fit neural network\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "print(clf.score(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 2502, loss = 2435.25623199\n",
            "Iteration 2503, loss = 2434.41188648\n",
            "Iteration 2504, loss = 2433.55332662\n",
            "Iteration 2505, loss = 2432.90590535\n",
            "Iteration 2506, loss = 2432.13400488\n",
            "Iteration 2507, loss = 2431.30285326\n",
            "Iteration 2508, loss = 2430.38831018\n",
            "Iteration 2509, loss = 2429.46840122\n",
            "Iteration 2510, loss = 2428.71550122\n",
            "Iteration 2511, loss = 2428.20255825\n",
            "Iteration 2512, loss = 2427.07126723\n",
            "Iteration 2513, loss = 2426.99139261\n",
            "Iteration 2514, loss = 2425.74509763\n",
            "Iteration 2515, loss = 2424.91404501\n",
            "Iteration 2516, loss = 2424.00089128\n",
            "Iteration 2517, loss = 2423.27342401\n",
            "Iteration 2518, loss = 2422.35234874\n",
            "Iteration 2519, loss = 2421.50223262\n",
            "Iteration 2520, loss = 2420.70098009\n",
            "Iteration 2521, loss = 2419.97898108\n",
            "Iteration 2522, loss = 2419.16714769\n",
            "Iteration 2523, loss = 2418.44542206\n",
            "Iteration 2524, loss = 2417.43246690\n",
            "Iteration 2525, loss = 2416.58996210\n",
            "Iteration 2526, loss = 2415.86332048\n",
            "Iteration 2527, loss = 2415.29478745\n",
            "Iteration 2528, loss = 2414.22836582\n",
            "Iteration 2529, loss = 2413.76707614\n",
            "Iteration 2530, loss = 2412.62033802\n",
            "Iteration 2531, loss = 2411.92016013\n",
            "Iteration 2532, loss = 2410.97428328\n",
            "Iteration 2533, loss = 2410.26769537\n",
            "Iteration 2534, loss = 2409.45789477\n",
            "Iteration 2535, loss = 2408.62448137\n",
            "Iteration 2536, loss = 2407.66704349\n",
            "Iteration 2537, loss = 2407.11871664\n",
            "Iteration 2538, loss = 2406.22626392\n",
            "Iteration 2539, loss = 2405.35030803\n",
            "Iteration 2540, loss = 2404.61433783\n",
            "Iteration 2541, loss = 2403.80277186\n",
            "Iteration 2542, loss = 2402.97202476\n",
            "Iteration 2543, loss = 2402.39084521\n",
            "Iteration 2544, loss = 2401.59560579\n",
            "Iteration 2545, loss = 2400.82407169\n",
            "Iteration 2546, loss = 2399.74878063\n",
            "Iteration 2547, loss = 2399.29261648\n",
            "Iteration 2548, loss = 2398.40198720\n",
            "Iteration 2549, loss = 2397.53452652\n",
            "Iteration 2550, loss = 2396.84179016\n",
            "Iteration 2551, loss = 2395.94989580\n",
            "Iteration 2552, loss = 2395.28710812\n",
            "Iteration 2553, loss = 2394.50963847\n",
            "Iteration 2554, loss = 2393.48368860\n",
            "Iteration 2555, loss = 2392.94845299\n",
            "Iteration 2556, loss = 2391.99781550\n",
            "Iteration 2557, loss = 2391.23014311\n",
            "Iteration 2558, loss = 2390.60553636\n",
            "Iteration 2559, loss = 2389.57371497\n",
            "Iteration 2560, loss = 2389.25471822\n",
            "Iteration 2561, loss = 2388.21685739\n",
            "Iteration 2562, loss = 2387.28210013\n",
            "Iteration 2563, loss = 2386.55686941\n",
            "Iteration 2564, loss = 2385.88739896\n",
            "Iteration 2565, loss = 2384.97337495\n",
            "Iteration 2566, loss = 2384.22632588\n",
            "Iteration 2567, loss = 2383.79524611\n",
            "Iteration 2568, loss = 2382.54348226\n",
            "Iteration 2569, loss = 2381.76476867\n",
            "Iteration 2570, loss = 2381.10637552\n",
            "Iteration 2571, loss = 2381.21173817\n",
            "Iteration 2572, loss = 2379.74890855\n",
            "Iteration 2573, loss = 2378.69932186\n",
            "Iteration 2574, loss = 2377.93384550\n",
            "Iteration 2575, loss = 2377.18248043\n",
            "Iteration 2576, loss = 2376.37780957\n",
            "Iteration 2577, loss = 2375.95416310\n",
            "Iteration 2578, loss = 2374.85368591\n",
            "Iteration 2579, loss = 2373.93490548\n",
            "Iteration 2580, loss = 2373.34723534\n",
            "Iteration 2581, loss = 2372.45204417\n",
            "Iteration 2582, loss = 2371.80816877\n",
            "Iteration 2583, loss = 2370.88601632\n",
            "Iteration 2584, loss = 2370.12885411\n",
            "Iteration 2585, loss = 2369.49227887\n",
            "Iteration 2586, loss = 2368.54981278\n",
            "Iteration 2587, loss = 2368.23932551\n",
            "Iteration 2588, loss = 2367.06850947\n",
            "Iteration 2589, loss = 2366.38528066\n",
            "Iteration 2590, loss = 2365.62738059\n",
            "Iteration 2591, loss = 2364.81318362\n",
            "Iteration 2592, loss = 2363.96470058\n",
            "Iteration 2593, loss = 2363.27099304\n",
            "Iteration 2594, loss = 2362.96000290\n",
            "Iteration 2595, loss = 2361.86928135\n",
            "Iteration 2596, loss = 2360.87046196\n",
            "Iteration 2597, loss = 2360.23367932\n",
            "Iteration 2598, loss = 2359.40242498\n",
            "Iteration 2599, loss = 2358.72280378\n",
            "Iteration 2600, loss = 2357.87009817\n",
            "Iteration 2601, loss = 2357.40285861\n",
            "Iteration 2602, loss = 2356.32095474\n",
            "Iteration 2603, loss = 2355.50438804\n",
            "Iteration 2604, loss = 2354.76395194\n",
            "Iteration 2605, loss = 2354.83536899\n",
            "Iteration 2606, loss = 2353.55976433\n",
            "Iteration 2607, loss = 2352.47962728\n",
            "Iteration 2608, loss = 2351.68771097\n",
            "Iteration 2609, loss = 2351.51108663\n",
            "Iteration 2610, loss = 2350.47942812\n",
            "Iteration 2611, loss = 2349.57556767\n",
            "Iteration 2612, loss = 2348.60376727\n",
            "Iteration 2613, loss = 2348.12948300\n",
            "Iteration 2614, loss = 2347.26646528\n",
            "Iteration 2615, loss = 2346.42732295\n",
            "Iteration 2616, loss = 2345.37044363\n",
            "Iteration 2617, loss = 2344.76485280\n",
            "Iteration 2618, loss = 2344.21994746\n",
            "Iteration 2619, loss = 2343.34092622\n",
            "Iteration 2620, loss = 2342.63401123\n",
            "Iteration 2621, loss = 2341.93777599\n",
            "Iteration 2622, loss = 2341.06536230\n",
            "Iteration 2623, loss = 2340.52241412\n",
            "Iteration 2624, loss = 2339.45601519\n",
            "Iteration 2625, loss = 2339.14479443\n",
            "Iteration 2626, loss = 2337.98684873\n",
            "Iteration 2627, loss = 2337.36025870\n",
            "Iteration 2628, loss = 2337.10918405\n",
            "Iteration 2629, loss = 2335.70105335\n",
            "Iteration 2630, loss = 2334.88678572\n",
            "Iteration 2631, loss = 2334.13028751\n",
            "Iteration 2632, loss = 2333.45955406\n",
            "Iteration 2633, loss = 2332.77063917\n",
            "Iteration 2634, loss = 2331.88172729\n",
            "Iteration 2635, loss = 2331.18703108\n",
            "Iteration 2636, loss = 2330.27145694\n",
            "Iteration 2637, loss = 2329.52917163\n",
            "Iteration 2638, loss = 2328.79773925\n",
            "Iteration 2639, loss = 2328.18382991\n",
            "Iteration 2640, loss = 2327.26564578\n",
            "Iteration 2641, loss = 2326.48401097\n",
            "Iteration 2642, loss = 2325.88448374\n",
            "Iteration 2643, loss = 2325.48509041\n",
            "Iteration 2644, loss = 2324.34782880\n",
            "Iteration 2645, loss = 2323.55171737\n",
            "Iteration 2646, loss = 2322.71391614\n",
            "Iteration 2647, loss = 2321.98130508\n",
            "Iteration 2648, loss = 2321.45131897\n",
            "Iteration 2649, loss = 2321.16703974\n",
            "Iteration 2650, loss = 2319.95612274\n",
            "Iteration 2651, loss = 2319.50660529\n",
            "Iteration 2652, loss = 2318.61138528\n",
            "Iteration 2653, loss = 2317.40196412\n",
            "Iteration 2654, loss = 2316.77476720\n",
            "Iteration 2655, loss = 2316.23832409\n",
            "Iteration 2656, loss = 2315.68142271\n",
            "Iteration 2657, loss = 2314.41790059\n",
            "Iteration 2658, loss = 2313.81950079\n",
            "Iteration 2659, loss = 2313.30663631\n",
            "Iteration 2660, loss = 2312.67060369\n",
            "Iteration 2661, loss = 2311.75129549\n",
            "Iteration 2662, loss = 2310.91526729\n",
            "Iteration 2663, loss = 2309.86470970\n",
            "Iteration 2664, loss = 2309.29487925\n",
            "Iteration 2665, loss = 2308.98328672\n",
            "Iteration 2666, loss = 2307.83167034\n",
            "Iteration 2667, loss = 2307.20833122\n",
            "Iteration 2668, loss = 2306.10087237\n",
            "Iteration 2669, loss = 2305.34502617\n",
            "Iteration 2670, loss = 2304.78843815\n",
            "Iteration 2671, loss = 2304.14636585\n",
            "Iteration 2672, loss = 2303.43742517\n",
            "Iteration 2673, loss = 2302.58485766\n",
            "Iteration 2674, loss = 2302.06172577\n",
            "Iteration 2675, loss = 2301.10108422\n",
            "Iteration 2676, loss = 2300.61551546\n",
            "Iteration 2677, loss = 2299.49543687\n",
            "Iteration 2678, loss = 2299.01154187\n",
            "Iteration 2679, loss = 2298.30476204\n",
            "Iteration 2680, loss = 2297.66064385\n",
            "Iteration 2681, loss = 2296.89274870\n",
            "Iteration 2682, loss = 2296.33493956\n",
            "Iteration 2683, loss = 2295.32725786\n",
            "Iteration 2684, loss = 2294.60213634\n",
            "Iteration 2685, loss = 2293.94312703\n",
            "Iteration 2686, loss = 2292.98554617\n",
            "Iteration 2687, loss = 2292.32593138\n",
            "Iteration 2688, loss = 2291.58306131\n",
            "Iteration 2689, loss = 2290.66158389\n",
            "Iteration 2690, loss = 2290.13724839\n",
            "Iteration 2691, loss = 2290.13675875\n",
            "Iteration 2692, loss = 2288.89221605\n",
            "Iteration 2693, loss = 2287.89080645\n",
            "Iteration 2694, loss = 2287.24400197\n",
            "Iteration 2695, loss = 2286.86410688\n",
            "Iteration 2696, loss = 2286.15227640\n",
            "Iteration 2697, loss = 2285.90735310\n",
            "Iteration 2698, loss = 2284.37908584\n",
            "Iteration 2699, loss = 2283.42971944\n",
            "Iteration 2700, loss = 2282.80185272\n",
            "Iteration 2701, loss = 2282.05589243\n",
            "Iteration 2702, loss = 2281.36949740\n",
            "Iteration 2703, loss = 2280.47154212\n",
            "Iteration 2704, loss = 2279.84570291\n",
            "Iteration 2705, loss = 2279.43522427\n",
            "Iteration 2706, loss = 2278.46310087\n",
            "Iteration 2707, loss = 2277.76112325\n",
            "Iteration 2708, loss = 2277.18880466\n",
            "Iteration 2709, loss = 2277.03311909\n",
            "Iteration 2710, loss = 2275.68951651\n",
            "Iteration 2711, loss = 2275.00261003\n",
            "Iteration 2712, loss = 2274.19583750\n",
            "Iteration 2713, loss = 2273.43390770\n",
            "Iteration 2714, loss = 2272.88972606\n",
            "Iteration 2715, loss = 2271.94963770\n",
            "Iteration 2716, loss = 2271.00378812\n",
            "Iteration 2717, loss = 2270.63943183\n",
            "Iteration 2718, loss = 2269.58142544\n",
            "Iteration 2719, loss = 2268.86357474\n",
            "Iteration 2720, loss = 2268.42331689\n",
            "Iteration 2721, loss = 2267.57452176\n",
            "Iteration 2722, loss = 2266.83097703\n",
            "Iteration 2723, loss = 2266.09458923\n",
            "Iteration 2724, loss = 2265.08948802\n",
            "Iteration 2725, loss = 2264.46134952\n",
            "Iteration 2726, loss = 2264.01150613\n",
            "Iteration 2727, loss = 2263.01224754\n",
            "Iteration 2728, loss = 2262.69953082\n",
            "Iteration 2729, loss = 2261.60315409\n",
            "Iteration 2730, loss = 2260.81483477\n",
            "Iteration 2731, loss = 2260.79797546\n",
            "Iteration 2732, loss = 2259.39790016\n",
            "Iteration 2733, loss = 2258.84661112\n",
            "Iteration 2734, loss = 2258.05311214\n",
            "Iteration 2735, loss = 2257.19251814\n",
            "Iteration 2736, loss = 2256.43313142\n",
            "Iteration 2737, loss = 2255.77953079\n",
            "Iteration 2738, loss = 2255.11010282\n",
            "Iteration 2739, loss = 2254.32649749\n",
            "Iteration 2740, loss = 2253.76627138\n",
            "Iteration 2741, loss = 2253.00280754\n",
            "Iteration 2742, loss = 2252.38974957\n",
            "Iteration 2743, loss = 2252.07624590\n",
            "Iteration 2744, loss = 2250.82972092\n",
            "Iteration 2745, loss = 2250.13805213\n",
            "Iteration 2746, loss = 2249.25541791\n",
            "Iteration 2747, loss = 2248.80996209\n",
            "Iteration 2748, loss = 2247.96875021\n",
            "Iteration 2749, loss = 2247.11079205\n",
            "Iteration 2750, loss = 2246.30972658\n",
            "Iteration 2751, loss = 2245.51195447\n",
            "Iteration 2752, loss = 2244.82733055\n",
            "Iteration 2753, loss = 2244.25876831\n",
            "Iteration 2754, loss = 2244.15283063\n",
            "Iteration 2755, loss = 2242.95225633\n",
            "Iteration 2756, loss = 2241.92557958\n",
            "Iteration 2757, loss = 2241.19591686\n",
            "Iteration 2758, loss = 2240.56159612\n",
            "Iteration 2759, loss = 2239.81243499\n",
            "Iteration 2760, loss = 2239.05746776\n",
            "Iteration 2761, loss = 2238.31836257\n",
            "Iteration 2762, loss = 2237.83581651\n",
            "Iteration 2763, loss = 2236.86332222\n",
            "Iteration 2764, loss = 2236.29929291\n",
            "Iteration 2765, loss = 2235.48535772\n",
            "Iteration 2766, loss = 2234.72464670\n",
            "Iteration 2767, loss = 2234.21626565\n",
            "Iteration 2768, loss = 2233.37323490\n",
            "Iteration 2769, loss = 2232.68867255\n",
            "Iteration 2770, loss = 2231.75789018\n",
            "Iteration 2771, loss = 2230.98817488\n",
            "Iteration 2772, loss = 2230.21461888\n",
            "Iteration 2773, loss = 2229.31260869\n",
            "Iteration 2774, loss = 2228.92283440\n",
            "Iteration 2775, loss = 2228.16011033\n",
            "Iteration 2776, loss = 2227.06033226\n",
            "Iteration 2777, loss = 2226.42265831\n",
            "Iteration 2778, loss = 2225.79057947\n",
            "Iteration 2779, loss = 2224.88960570\n",
            "Iteration 2780, loss = 2223.87389843\n",
            "Iteration 2781, loss = 2223.42342210\n",
            "Iteration 2782, loss = 2222.63071525\n",
            "Iteration 2783, loss = 2222.40422725\n",
            "Iteration 2784, loss = 2221.77229210\n",
            "Iteration 2785, loss = 2220.88108537\n",
            "Iteration 2786, loss = 2220.63251145\n",
            "Iteration 2787, loss = 2218.97856342\n",
            "Iteration 2788, loss = 2218.25019154\n",
            "Iteration 2789, loss = 2217.46009541\n",
            "Iteration 2790, loss = 2216.77192400\n",
            "Iteration 2791, loss = 2216.12901077\n",
            "Iteration 2792, loss = 2215.27568040\n",
            "Iteration 2793, loss = 2214.62968625\n",
            "Iteration 2794, loss = 2214.00078347\n",
            "Iteration 2795, loss = 2213.08383231\n",
            "Iteration 2796, loss = 2212.31802070\n",
            "Iteration 2797, loss = 2211.57665757\n",
            "Iteration 2798, loss = 2210.86539105\n",
            "Iteration 2799, loss = 2210.19576472\n",
            "Iteration 2800, loss = 2209.75148935\n",
            "Iteration 2801, loss = 2208.65146686\n",
            "Iteration 2802, loss = 2207.82182011\n",
            "Iteration 2803, loss = 2207.11136229\n",
            "Iteration 2804, loss = 2206.39862505\n",
            "Iteration 2805, loss = 2205.45932844\n",
            "Iteration 2806, loss = 2204.63343500\n",
            "Iteration 2807, loss = 2204.18858517\n",
            "Iteration 2808, loss = 2204.08222400\n",
            "Iteration 2809, loss = 2202.74996749\n",
            "Iteration 2810, loss = 2201.88677181\n",
            "Iteration 2811, loss = 2200.64685559\n",
            "Iteration 2812, loss = 2200.45713323\n",
            "Iteration 2813, loss = 2200.94390013\n",
            "Iteration 2814, loss = 2199.60176731\n",
            "Iteration 2815, loss = 2198.26855925\n",
            "Iteration 2816, loss = 2197.57453981\n",
            "Iteration 2817, loss = 2196.38374627\n",
            "Iteration 2818, loss = 2196.28900641\n",
            "Iteration 2819, loss = 2195.28889497\n",
            "Iteration 2820, loss = 2194.88499861\n",
            "Iteration 2821, loss = 2193.84691641\n",
            "Iteration 2822, loss = 2192.79254772\n",
            "Iteration 2823, loss = 2192.24954797\n",
            "Iteration 2824, loss = 2191.48092555\n",
            "Iteration 2825, loss = 2190.54337618\n",
            "Iteration 2826, loss = 2189.63894437\n",
            "Iteration 2827, loss = 2188.93249069\n",
            "Iteration 2828, loss = 2188.38518978\n",
            "Iteration 2829, loss = 2187.63958927\n",
            "Iteration 2830, loss = 2186.76845040\n",
            "Iteration 2831, loss = 2186.14763595\n",
            "Iteration 2832, loss = 2185.42597294\n",
            "Iteration 2833, loss = 2185.19206899\n",
            "Iteration 2834, loss = 2183.85159595\n",
            "Iteration 2835, loss = 2183.23170914\n",
            "Iteration 2836, loss = 2182.39202965\n",
            "Iteration 2837, loss = 2181.74343673\n",
            "Iteration 2838, loss = 2181.25944163\n",
            "Iteration 2839, loss = 2180.28525208\n",
            "Iteration 2840, loss = 2179.46349797\n",
            "Iteration 2841, loss = 2178.75654161\n",
            "Iteration 2842, loss = 2177.94552910\n",
            "Iteration 2843, loss = 2177.20221399\n",
            "Iteration 2844, loss = 2176.58603119\n",
            "Iteration 2845, loss = 2175.70114908\n",
            "Iteration 2846, loss = 2176.17691085\n",
            "Iteration 2847, loss = 2174.26275736\n",
            "Iteration 2848, loss = 2173.50670186\n",
            "Iteration 2849, loss = 2172.88642250\n",
            "Iteration 2850, loss = 2172.17277784\n",
            "Iteration 2851, loss = 2171.24007798\n",
            "Iteration 2852, loss = 2170.64483131\n",
            "Iteration 2853, loss = 2169.80191130\n",
            "Iteration 2854, loss = 2168.97250784\n",
            "Iteration 2855, loss = 2168.75080315\n",
            "Iteration 2856, loss = 2167.86099249\n",
            "Iteration 2857, loss = 2166.73333020\n",
            "Iteration 2858, loss = 2166.52118072\n",
            "Iteration 2859, loss = 2165.44854684\n",
            "Iteration 2860, loss = 2164.77255325\n",
            "Iteration 2861, loss = 2163.90844196\n",
            "Iteration 2862, loss = 2163.12902164\n",
            "Iteration 2863, loss = 2162.51722312\n",
            "Iteration 2864, loss = 2161.83470986\n",
            "Iteration 2865, loss = 2161.05994804\n",
            "Iteration 2866, loss = 2160.49194911\n",
            "Iteration 2867, loss = 2159.56701507\n",
            "Iteration 2868, loss = 2158.96143892\n",
            "Iteration 2869, loss = 2158.16054315\n",
            "Iteration 2870, loss = 2157.42339097\n",
            "Iteration 2871, loss = 2156.61504521\n",
            "Iteration 2872, loss = 2155.83474858\n",
            "Iteration 2873, loss = 2155.22001851\n",
            "Iteration 2874, loss = 2154.68021918\n",
            "Iteration 2875, loss = 2153.96684040\n",
            "Iteration 2876, loss = 2153.13968799\n",
            "Iteration 2877, loss = 2152.35561657\n",
            "Iteration 2878, loss = 2152.58673860\n",
            "Iteration 2879, loss = 2151.21982962\n",
            "Iteration 2880, loss = 2150.16336208\n",
            "Iteration 2881, loss = 2149.53112052\n",
            "Iteration 2882, loss = 2148.82013602\n",
            "Iteration 2883, loss = 2148.39753875\n",
            "Iteration 2884, loss = 2147.49531556\n",
            "Iteration 2885, loss = 2146.62561594\n",
            "Iteration 2886, loss = 2145.86806700\n",
            "Iteration 2887, loss = 2145.16512366\n",
            "Iteration 2888, loss = 2144.74037393\n",
            "Iteration 2889, loss = 2143.68181361\n",
            "Iteration 2890, loss = 2143.18434411\n",
            "Iteration 2891, loss = 2142.20753455\n",
            "Iteration 2892, loss = 2141.67988324\n",
            "Iteration 2893, loss = 2141.39357913\n",
            "Iteration 2894, loss = 2140.30640237\n",
            "Iteration 2895, loss = 2139.35336530\n",
            "Iteration 2896, loss = 2138.67489736\n",
            "Iteration 2897, loss = 2138.05453623\n",
            "Iteration 2898, loss = 2137.35445796\n",
            "Iteration 2899, loss = 2136.57342807\n",
            "Iteration 2900, loss = 2135.96445239\n",
            "Iteration 2901, loss = 2135.01053877\n",
            "Iteration 2902, loss = 2134.56310297\n",
            "Iteration 2903, loss = 2133.67252563\n",
            "Iteration 2904, loss = 2133.27089600\n",
            "Iteration 2905, loss = 2132.50875039\n",
            "Iteration 2906, loss = 2131.81516991\n",
            "Iteration 2907, loss = 2131.01774925\n",
            "Iteration 2908, loss = 2130.28847481\n",
            "Iteration 2909, loss = 2129.47321600\n",
            "Iteration 2910, loss = 2128.85235781\n",
            "Iteration 2911, loss = 2128.54757042\n",
            "Iteration 2912, loss = 2127.58468185\n",
            "Iteration 2913, loss = 2126.85487487\n",
            "Iteration 2914, loss = 2126.20704170\n",
            "Iteration 2915, loss = 2125.55749380\n",
            "Iteration 2916, loss = 2124.73295792\n",
            "Iteration 2917, loss = 2124.53844555\n",
            "Iteration 2918, loss = 2123.47929306\n",
            "Iteration 2919, loss = 2122.71761566\n",
            "Iteration 2920, loss = 2122.05094954\n",
            "Iteration 2921, loss = 2121.27603672\n",
            "Iteration 2922, loss = 2120.49787817\n",
            "Iteration 2923, loss = 2119.86316394\n",
            "Iteration 2924, loss = 2119.23997852\n",
            "Iteration 2925, loss = 2118.64352803\n",
            "Iteration 2926, loss = 2118.16936271\n",
            "Iteration 2927, loss = 2117.49001891\n",
            "Iteration 2928, loss = 2116.70199356\n",
            "Iteration 2929, loss = 2115.98009778\n",
            "Iteration 2930, loss = 2115.00306754\n",
            "Iteration 2931, loss = 2114.91400379\n",
            "Iteration 2932, loss = 2113.71466656\n",
            "Iteration 2933, loss = 2112.90407835\n",
            "Iteration 2934, loss = 2112.26197694\n",
            "Iteration 2935, loss = 2111.53328854\n",
            "Iteration 2936, loss = 2111.35575740\n",
            "Iteration 2937, loss = 2110.30097197\n",
            "Iteration 2938, loss = 2109.88249748\n",
            "Iteration 2939, loss = 2108.90091407\n",
            "Iteration 2940, loss = 2108.16615834\n",
            "Iteration 2941, loss = 2107.55218470\n",
            "Iteration 2942, loss = 2107.01058924\n",
            "Iteration 2943, loss = 2106.41416706\n",
            "Iteration 2944, loss = 2105.72907437\n",
            "Iteration 2945, loss = 2105.32689342\n",
            "Iteration 2946, loss = 2104.09992418\n",
            "Iteration 2947, loss = 2103.32939673\n",
            "Iteration 2948, loss = 2102.85048756\n",
            "Iteration 2949, loss = 2102.41049720\n",
            "Iteration 2950, loss = 2101.54582137\n",
            "Iteration 2951, loss = 2101.06123797\n",
            "Iteration 2952, loss = 2099.98238303\n",
            "Iteration 2953, loss = 2099.43248005\n",
            "Iteration 2954, loss = 2098.43488085\n",
            "Iteration 2955, loss = 2098.00599637\n",
            "Iteration 2956, loss = 2097.29493215\n",
            "Iteration 2957, loss = 2096.79001020\n",
            "Iteration 2958, loss = 2096.20379977\n",
            "Iteration 2959, loss = 2095.19922582\n",
            "Iteration 2960, loss = 2095.13435601\n",
            "Iteration 2961, loss = 2093.81471789\n",
            "Iteration 2962, loss = 2092.98559461\n",
            "Iteration 2963, loss = 2092.37869043\n",
            "Iteration 2964, loss = 2091.57479139\n",
            "Iteration 2965, loss = 2090.88702373\n",
            "Iteration 2966, loss = 2090.01793031\n",
            "Iteration 2967, loss = 2089.64831846\n",
            "Iteration 2968, loss = 2088.89569745\n",
            "Iteration 2969, loss = 2088.13748298\n",
            "Iteration 2970, loss = 2087.54575497\n",
            "Iteration 2971, loss = 2086.90170860\n",
            "Iteration 2972, loss = 2086.26705658\n",
            "Iteration 2973, loss = 2085.36286000\n",
            "Iteration 2974, loss = 2084.77452622\n",
            "Iteration 2975, loss = 2084.13183616\n",
            "Iteration 2976, loss = 2083.40950349\n",
            "Iteration 2977, loss = 2082.74135822\n",
            "Iteration 2978, loss = 2082.14424883\n",
            "Iteration 2979, loss = 2081.31682181\n",
            "Iteration 2980, loss = 2080.83248688\n",
            "Iteration 2981, loss = 2080.33089845\n",
            "Iteration 2982, loss = 2079.44250188\n",
            "Iteration 2983, loss = 2078.91540835\n",
            "Iteration 2984, loss = 2078.18600301\n",
            "Iteration 2985, loss = 2077.59840369\n",
            "Iteration 2986, loss = 2076.79227012\n",
            "Iteration 2987, loss = 2076.18002899\n",
            "Iteration 2988, loss = 2075.81366259\n",
            "Iteration 2989, loss = 2075.03402565\n",
            "Iteration 2990, loss = 2074.25609494\n",
            "Iteration 2991, loss = 2073.48502925\n",
            "Iteration 2992, loss = 2072.90464432\n",
            "Iteration 2993, loss = 2072.75733961\n",
            "Iteration 2994, loss = 2071.75160152\n",
            "Iteration 2995, loss = 2071.09321814\n",
            "Iteration 2996, loss = 2070.46029297\n",
            "Iteration 2997, loss = 2069.65327233\n",
            "Iteration 2998, loss = 2069.05998115\n",
            "Iteration 2999, loss = 2068.94745356\n",
            "Iteration 3000, loss = 2067.53694287\n",
            "Iteration 3001, loss = 2066.92217649\n",
            "Iteration 3002, loss = 2066.32160337\n",
            "Iteration 3003, loss = 2065.86246131\n",
            "Iteration 3004, loss = 2065.04677592\n",
            "Iteration 3005, loss = 2064.34307947\n",
            "Iteration 3006, loss = 2063.75466639\n",
            "Iteration 3007, loss = 2063.12040134\n",
            "Iteration 3008, loss = 2062.35202711\n",
            "Iteration 3009, loss = 2061.61953835\n",
            "Iteration 3010, loss = 2060.86991171\n",
            "Iteration 3011, loss = 2060.26257497\n",
            "Iteration 3012, loss = 2059.65953423\n",
            "Iteration 3013, loss = 2058.93323579\n",
            "Iteration 3014, loss = 2058.37487569\n",
            "Iteration 3015, loss = 2057.25705573\n",
            "Iteration 3016, loss = 2057.16737157\n",
            "Iteration 3017, loss = 2056.29444735\n",
            "Iteration 3018, loss = 2055.57183389\n",
            "Iteration 3019, loss = 2055.02814711\n",
            "Iteration 3020, loss = 2054.14288668\n",
            "Iteration 3021, loss = 2053.46693753\n",
            "Iteration 3022, loss = 2053.33171787\n",
            "Iteration 3023, loss = 2052.84846352\n",
            "Iteration 3024, loss = 2051.70584564\n",
            "Iteration 3025, loss = 2050.87683113\n",
            "Iteration 3026, loss = 2050.21440045\n",
            "Iteration 3027, loss = 2050.05242988\n",
            "Iteration 3028, loss = 2048.73296053\n",
            "Iteration 3029, loss = 2048.34124437\n",
            "Iteration 3030, loss = 2047.40754609\n",
            "Iteration 3031, loss = 2046.70585761\n",
            "Iteration 3032, loss = 2046.04886068\n",
            "Iteration 3033, loss = 2045.40466141\n",
            "Iteration 3034, loss = 2045.17790694\n",
            "Iteration 3035, loss = 2044.14937649\n",
            "Iteration 3036, loss = 2043.31216694\n",
            "Iteration 3037, loss = 2043.00483316\n",
            "Iteration 3038, loss = 2042.27502383\n",
            "Iteration 3039, loss = 2041.26977589\n",
            "Iteration 3040, loss = 2040.67931785\n",
            "Iteration 3041, loss = 2040.16811967\n",
            "Iteration 3042, loss = 2039.58231325\n",
            "Iteration 3043, loss = 2038.98065431\n",
            "Iteration 3044, loss = 2038.64204299\n",
            "Iteration 3045, loss = 2037.96281853\n",
            "Iteration 3046, loss = 2037.09733974\n",
            "Iteration 3047, loss = 2036.10289391\n",
            "Iteration 3048, loss = 2035.90654890\n",
            "Iteration 3049, loss = 2035.18689545\n",
            "Iteration 3050, loss = 2034.09520153\n",
            "Iteration 3051, loss = 2033.40088978\n",
            "Iteration 3052, loss = 2032.78917968\n",
            "Iteration 3053, loss = 2032.39467434\n",
            "Iteration 3054, loss = 2031.46768890\n",
            "Iteration 3055, loss = 2030.68981979\n",
            "Iteration 3056, loss = 2030.14291851\n",
            "Iteration 3057, loss = 2029.44922264\n",
            "Iteration 3058, loss = 2028.91476523\n",
            "Iteration 3059, loss = 2028.12620818\n",
            "Iteration 3060, loss = 2027.60131874\n",
            "Iteration 3061, loss = 2026.83961891\n",
            "Iteration 3062, loss = 2026.14131528\n",
            "Iteration 3063, loss = 2025.68015049\n",
            "Iteration 3064, loss = 2024.93211194\n",
            "Iteration 3065, loss = 2024.23773055\n",
            "Iteration 3066, loss = 2023.45247002\n",
            "Iteration 3067, loss = 2022.75235339\n",
            "Iteration 3068, loss = 2022.33105353\n",
            "Iteration 3069, loss = 2021.51873548\n",
            "Iteration 3070, loss = 2020.70754457\n",
            "Iteration 3071, loss = 2020.05908088\n",
            "Iteration 3072, loss = 2019.45953985\n",
            "Iteration 3073, loss = 2018.90214636\n",
            "Iteration 3074, loss = 2018.09494453\n",
            "Iteration 3075, loss = 2017.48627124\n",
            "Iteration 3076, loss = 2016.71812358\n",
            "Iteration 3077, loss = 2016.44792472\n",
            "Iteration 3078, loss = 2015.58189465\n",
            "Iteration 3079, loss = 2015.28465504\n",
            "Iteration 3080, loss = 2014.45504951\n",
            "Iteration 3081, loss = 2013.44651931\n",
            "Iteration 3082, loss = 2012.64375122\n",
            "Iteration 3083, loss = 2012.14940990\n",
            "Iteration 3084, loss = 2011.75781166\n",
            "Iteration 3085, loss = 2011.07372899\n",
            "Iteration 3086, loss = 2010.52688910\n",
            "Iteration 3087, loss = 2009.76756190\n",
            "Iteration 3088, loss = 2008.80793366\n",
            "Iteration 3089, loss = 2008.84405682\n",
            "Iteration 3090, loss = 2007.72018428\n",
            "Iteration 3091, loss = 2007.13393414\n",
            "Iteration 3092, loss = 2005.98686941\n",
            "Iteration 3093, loss = 2005.48212928\n",
            "Iteration 3094, loss = 2004.68361383\n",
            "Iteration 3095, loss = 2004.01533399\n",
            "Iteration 3096, loss = 2003.48752134\n",
            "Iteration 3097, loss = 2002.68981342\n",
            "Iteration 3098, loss = 2002.07522646\n",
            "Iteration 3099, loss = 2001.31444594\n",
            "Iteration 3100, loss = 2000.69171250\n",
            "Iteration 3101, loss = 1999.97998618\n",
            "Iteration 3102, loss = 1999.65671658\n",
            "Iteration 3103, loss = 1998.86512484\n",
            "Iteration 3104, loss = 1998.16437295\n",
            "Iteration 3105, loss = 1997.42035336\n",
            "Iteration 3106, loss = 1996.82087452\n",
            "Iteration 3107, loss = 1996.08364839\n",
            "Iteration 3108, loss = 1995.48362368\n",
            "Iteration 3109, loss = 1994.83250911\n",
            "Iteration 3110, loss = 1993.86139776\n",
            "Iteration 3111, loss = 1994.10068045\n",
            "Iteration 3112, loss = 1993.10773058\n",
            "Iteration 3113, loss = 1992.18324667\n",
            "Iteration 3114, loss = 1991.33517083\n",
            "Iteration 3115, loss = 1990.83437060\n",
            "Iteration 3116, loss = 1990.03776875\n",
            "Iteration 3117, loss = 1989.24625467\n",
            "Iteration 3118, loss = 1988.88738808\n",
            "Iteration 3119, loss = 1988.49771758\n",
            "Iteration 3120, loss = 1987.44573574\n",
            "Iteration 3121, loss = 1986.89035946\n",
            "Iteration 3122, loss = 1986.08240829\n",
            "Iteration 3123, loss = 1985.91515669\n",
            "Iteration 3124, loss = 1984.57877594\n",
            "Iteration 3125, loss = 1984.41036746\n",
            "Iteration 3126, loss = 1983.34497975\n",
            "Iteration 3127, loss = 1982.71689493\n",
            "Iteration 3128, loss = 1982.18950973\n",
            "Iteration 3129, loss = 1981.26867451\n",
            "Iteration 3130, loss = 1981.21119854\n",
            "Iteration 3131, loss = 1980.51971631\n",
            "Iteration 3132, loss = 1979.32901107\n",
            "Iteration 3133, loss = 1978.60884046\n",
            "Iteration 3134, loss = 1977.90831984\n",
            "Iteration 3135, loss = 1977.59639734\n",
            "Iteration 3136, loss = 1976.71125155\n",
            "Iteration 3137, loss = 1976.82500550\n",
            "Iteration 3138, loss = 1975.55827070\n",
            "Iteration 3139, loss = 1974.96548855\n",
            "Iteration 3140, loss = 1973.79810316\n",
            "Iteration 3141, loss = 1973.61492331\n",
            "Iteration 3142, loss = 1972.76566202\n",
            "Iteration 3143, loss = 1971.83558093\n",
            "Iteration 3144, loss = 1971.26655833\n",
            "Iteration 3145, loss = 1970.71109869\n",
            "Iteration 3146, loss = 1969.76900481\n",
            "Iteration 3147, loss = 1969.14282584\n",
            "Iteration 3148, loss = 1968.64761555\n",
            "Iteration 3149, loss = 1967.58807529\n",
            "Iteration 3150, loss = 1967.13921356\n",
            "Iteration 3151, loss = 1966.65626368\n",
            "Iteration 3152, loss = 1966.00525901\n",
            "Iteration 3153, loss = 1965.19697041\n",
            "Iteration 3154, loss = 1964.46619280\n",
            "Iteration 3155, loss = 1963.79748824\n",
            "Iteration 3156, loss = 1962.84236749\n",
            "Iteration 3157, loss = 1962.37066843\n",
            "Iteration 3158, loss = 1961.89117871\n",
            "Iteration 3159, loss = 1961.08409120\n",
            "Iteration 3160, loss = 1960.53532730\n",
            "Iteration 3161, loss = 1960.03928511\n",
            "Iteration 3162, loss = 1958.96569561\n",
            "Iteration 3163, loss = 1958.57895128\n",
            "Iteration 3164, loss = 1957.62194637\n",
            "Iteration 3165, loss = 1956.94216293\n",
            "Iteration 3166, loss = 1956.44928805\n",
            "Iteration 3167, loss = 1955.72297430\n",
            "Iteration 3168, loss = 1954.73872251\n",
            "Iteration 3169, loss = 1954.17095116\n",
            "Iteration 3170, loss = 1953.70530616\n",
            "Iteration 3171, loss = 1952.86852056\n",
            "Iteration 3172, loss = 1952.59091705\n",
            "Iteration 3173, loss = 1951.79836447\n",
            "Iteration 3174, loss = 1951.12428063\n",
            "Iteration 3175, loss = 1950.28863155\n",
            "Iteration 3176, loss = 1949.52069503\n",
            "Iteration 3177, loss = 1948.95249145\n",
            "Iteration 3178, loss = 1948.62683915\n",
            "Iteration 3179, loss = 1947.62725018\n",
            "Iteration 3180, loss = 1947.06370299\n",
            "Iteration 3181, loss = 1946.33605029\n",
            "Iteration 3182, loss = 1946.02583747\n",
            "Iteration 3183, loss = 1944.87347551\n",
            "Iteration 3184, loss = 1944.12619109\n",
            "Iteration 3185, loss = 1943.65953166\n",
            "Iteration 3186, loss = 1942.99211871\n",
            "Iteration 3187, loss = 1942.43559062\n",
            "Iteration 3188, loss = 1941.73484283\n",
            "Iteration 3189, loss = 1941.17808275\n",
            "Iteration 3190, loss = 1940.33453741\n",
            "Iteration 3191, loss = 1939.60898886\n",
            "Iteration 3192, loss = 1939.07731196\n",
            "Iteration 3193, loss = 1938.20892122\n",
            "Iteration 3194, loss = 1937.72267039\n",
            "Iteration 3195, loss = 1937.11829844\n",
            "Iteration 3196, loss = 1936.82475730\n",
            "Iteration 3197, loss = 1935.84435822\n",
            "Iteration 3198, loss = 1935.06198448\n",
            "Iteration 3199, loss = 1934.32703647\n",
            "Iteration 3200, loss = 1933.59679528\n",
            "Iteration 3201, loss = 1932.99774257\n",
            "Iteration 3202, loss = 1932.15879515\n",
            "Iteration 3203, loss = 1931.64983920\n",
            "Iteration 3204, loss = 1930.98049994\n",
            "Iteration 3205, loss = 1930.53053881\n",
            "Iteration 3206, loss = 1929.61859171\n",
            "Iteration 3207, loss = 1929.74352259\n",
            "Iteration 3208, loss = 1928.64378734\n",
            "Iteration 3209, loss = 1927.60721230\n",
            "Iteration 3210, loss = 1927.20092236\n",
            "Iteration 3211, loss = 1926.57414184\n",
            "Iteration 3212, loss = 1925.76284704\n",
            "Iteration 3213, loss = 1925.03118364\n",
            "Iteration 3214, loss = 1924.59265305\n",
            "Iteration 3215, loss = 1923.74432582\n",
            "Iteration 3216, loss = 1923.07085806\n",
            "Iteration 3217, loss = 1922.29531009\n",
            "Iteration 3218, loss = 1921.52197032\n",
            "Iteration 3219, loss = 1920.84896596\n",
            "Iteration 3220, loss = 1920.29560590\n",
            "Iteration 3221, loss = 1919.67979368\n",
            "Iteration 3222, loss = 1919.26333927\n",
            "Iteration 3223, loss = 1918.44595905\n",
            "Iteration 3224, loss = 1917.74397246\n",
            "Iteration 3225, loss = 1916.82205508\n",
            "Iteration 3226, loss = 1916.79027331\n",
            "Iteration 3227, loss = 1916.44661461\n",
            "Iteration 3228, loss = 1915.12663192\n",
            "Iteration 3229, loss = 1914.31245495\n",
            "Iteration 3230, loss = 1913.87915883\n",
            "Iteration 3231, loss = 1912.64729187\n",
            "Iteration 3232, loss = 1912.13074740\n",
            "Iteration 3233, loss = 1911.48110998\n",
            "Iteration 3234, loss = 1910.90434579\n",
            "Iteration 3235, loss = 1910.14905147\n",
            "Iteration 3236, loss = 1909.57134478\n",
            "Iteration 3237, loss = 1908.72278341\n",
            "Iteration 3238, loss = 1908.39180812\n",
            "Iteration 3239, loss = 1907.19934140\n",
            "Iteration 3240, loss = 1906.78638148\n",
            "Iteration 3241, loss = 1905.99141448\n",
            "Iteration 3242, loss = 1905.67426428\n",
            "Iteration 3243, loss = 1904.64215551\n",
            "Iteration 3244, loss = 1904.27411616\n",
            "Iteration 3245, loss = 1903.29190622\n",
            "Iteration 3246, loss = 1902.66545551\n",
            "Iteration 3247, loss = 1902.02752078\n",
            "Iteration 3248, loss = 1901.10640934\n",
            "Iteration 3249, loss = 1900.46758662\n",
            "Iteration 3250, loss = 1900.21454624\n",
            "Iteration 3251, loss = 1899.15057810\n",
            "Iteration 3252, loss = 1898.60679168\n",
            "Iteration 3253, loss = 1897.69932573\n",
            "Iteration 3254, loss = 1897.05953493\n",
            "Iteration 3255, loss = 1896.62534880\n",
            "Iteration 3256, loss = 1895.90344852\n",
            "Iteration 3257, loss = 1895.05076473\n",
            "Iteration 3258, loss = 1894.56315781\n",
            "Iteration 3259, loss = 1894.03428560\n",
            "Iteration 3260, loss = 1893.40090903\n",
            "Iteration 3261, loss = 1892.68517611\n",
            "Iteration 3262, loss = 1892.00184732\n",
            "Iteration 3263, loss = 1891.40985673\n",
            "Iteration 3264, loss = 1890.97642594\n",
            "Iteration 3265, loss = 1890.07351735\n",
            "Iteration 3266, loss = 1890.07819024\n",
            "Iteration 3267, loss = 1888.61558677\n",
            "Iteration 3268, loss = 1888.13617031\n",
            "Iteration 3269, loss = 1887.62030171\n",
            "Iteration 3270, loss = 1887.06211536\n",
            "Iteration 3271, loss = 1886.21418116\n",
            "Iteration 3272, loss = 1885.41966208\n",
            "Iteration 3273, loss = 1885.35718678\n",
            "Iteration 3274, loss = 1884.14628239\n",
            "Iteration 3275, loss = 1883.29450680\n",
            "Iteration 3276, loss = 1882.73663431\n",
            "Iteration 3277, loss = 1882.07919788\n",
            "Iteration 3278, loss = 1881.58019607\n",
            "Iteration 3279, loss = 1880.68702146\n",
            "Iteration 3280, loss = 1880.03276358\n",
            "Iteration 3281, loss = 1879.51077199\n",
            "Iteration 3282, loss = 1878.63970408\n",
            "Iteration 3283, loss = 1878.24849259\n",
            "Iteration 3284, loss = 1877.54806908\n",
            "Iteration 3285, loss = 1876.81419248\n",
            "Iteration 3286, loss = 1876.13689470\n",
            "Iteration 3287, loss = 1875.87987266\n",
            "Iteration 3288, loss = 1875.05726573\n",
            "Iteration 3289, loss = 1874.73962885\n",
            "Iteration 3290, loss = 1873.58457293\n",
            "Iteration 3291, loss = 1872.88192072\n",
            "Iteration 3292, loss = 1872.24298276\n",
            "Iteration 3293, loss = 1872.03930479\n",
            "Iteration 3294, loss = 1870.78112427\n",
            "Iteration 3295, loss = 1871.08872700\n",
            "Iteration 3296, loss = 1870.09096890\n",
            "Iteration 3297, loss = 1869.31200910\n",
            "Iteration 3298, loss = 1868.18455802\n",
            "Iteration 3299, loss = 1867.47611978\n",
            "Iteration 3300, loss = 1867.53420111\n",
            "Iteration 3301, loss = 1866.97550511\n",
            "Iteration 3302, loss = 1866.21828580\n",
            "Iteration 3303, loss = 1865.47035763\n",
            "Iteration 3304, loss = 1864.46177269\n",
            "Iteration 3305, loss = 1863.63174377\n",
            "Iteration 3306, loss = 1863.61712451\n",
            "Iteration 3307, loss = 1863.02617889\n",
            "Iteration 3308, loss = 1861.85124434\n",
            "Iteration 3309, loss = 1861.11861535\n",
            "Iteration 3310, loss = 1861.04743468\n",
            "Iteration 3311, loss = 1860.75977248\n",
            "Iteration 3312, loss = 1859.13305533\n",
            "Iteration 3313, loss = 1858.80042838\n",
            "Iteration 3314, loss = 1858.13173867\n",
            "Iteration 3315, loss = 1856.96716426\n",
            "Iteration 3316, loss = 1856.36041175\n",
            "Iteration 3317, loss = 1855.95269179\n",
            "Iteration 3318, loss = 1855.45657674\n",
            "Iteration 3319, loss = 1854.47994012\n",
            "Iteration 3320, loss = 1853.86497401\n",
            "Iteration 3321, loss = 1853.23778262\n",
            "Iteration 3322, loss = 1852.82412113\n",
            "Iteration 3323, loss = 1851.78285363\n",
            "Iteration 3324, loss = 1851.44678087\n",
            "Iteration 3325, loss = 1850.41177572\n",
            "Iteration 3326, loss = 1850.25485661\n",
            "Iteration 3327, loss = 1849.37307175\n",
            "Iteration 3328, loss = 1848.79586525\n",
            "Iteration 3329, loss = 1848.20576424\n",
            "Iteration 3330, loss = 1847.45169278\n",
            "Iteration 3331, loss = 1846.77602861\n",
            "Iteration 3332, loss = 1846.29349744\n",
            "Iteration 3333, loss = 1845.53936619\n",
            "Iteration 3334, loss = 1845.21090281\n",
            "Iteration 3335, loss = 1844.36059594\n",
            "Iteration 3336, loss = 1843.81769208\n",
            "Iteration 3337, loss = 1843.01147493\n",
            "Iteration 3338, loss = 1842.36728340\n",
            "Iteration 3339, loss = 1841.83628047\n",
            "Iteration 3340, loss = 1841.26389159\n",
            "Iteration 3341, loss = 1840.51747242\n",
            "Iteration 3342, loss = 1839.73058086\n",
            "Iteration 3343, loss = 1838.93777455\n",
            "Iteration 3344, loss = 1838.17772414\n",
            "Iteration 3345, loss = 1837.28739085\n",
            "Iteration 3346, loss = 1837.77942213\n",
            "Iteration 3347, loss = 1836.44533014\n",
            "Iteration 3348, loss = 1835.50210847\n",
            "Iteration 3349, loss = 1834.91928176\n",
            "Iteration 3350, loss = 1834.41915430\n",
            "Iteration 3351, loss = 1833.73257038\n",
            "Iteration 3352, loss = 1833.32755066\n",
            "Iteration 3353, loss = 1832.40839010\n",
            "Iteration 3354, loss = 1831.75448882\n",
            "Iteration 3355, loss = 1831.27574065\n",
            "Iteration 3356, loss = 1831.05028509\n",
            "Iteration 3357, loss = 1830.46783933\n",
            "Iteration 3358, loss = 1829.04945994\n",
            "Iteration 3359, loss = 1828.44243179\n",
            "Iteration 3360, loss = 1827.82865419\n",
            "Iteration 3361, loss = 1827.30271559\n",
            "Iteration 3362, loss = 1826.57111452\n",
            "Iteration 3363, loss = 1825.85991085\n",
            "Iteration 3364, loss = 1825.25375714\n",
            "Iteration 3365, loss = 1824.41931751\n",
            "Iteration 3366, loss = 1823.91335204\n",
            "Iteration 3367, loss = 1823.52709209\n",
            "Iteration 3368, loss = 1822.75502504\n",
            "Iteration 3369, loss = 1822.06266250\n",
            "Iteration 3370, loss = 1821.42535356\n",
            "Iteration 3371, loss = 1820.65142818\n",
            "Iteration 3372, loss = 1819.94532903\n",
            "Iteration 3373, loss = 1819.72239800\n",
            "Iteration 3374, loss = 1818.85478952\n",
            "Iteration 3375, loss = 1818.27252142\n",
            "Iteration 3376, loss = 1817.51062673\n",
            "Iteration 3377, loss = 1817.01597912\n",
            "Iteration 3378, loss = 1816.29811563\n",
            "Iteration 3379, loss = 1815.85658962\n",
            "Iteration 3380, loss = 1815.35135285\n",
            "Iteration 3381, loss = 1814.72725141\n",
            "Iteration 3382, loss = 1813.67517108\n",
            "Iteration 3383, loss = 1813.38227019\n",
            "Iteration 3384, loss = 1812.63397704\n",
            "Iteration 3385, loss = 1811.69368423\n",
            "Iteration 3386, loss = 1811.04095690\n",
            "Iteration 3387, loss = 1810.58923973\n",
            "Iteration 3388, loss = 1809.92767659\n",
            "Iteration 3389, loss = 1808.97658158\n",
            "Iteration 3390, loss = 1808.47304802\n",
            "Iteration 3391, loss = 1807.80871956\n",
            "Iteration 3392, loss = 1807.08934125\n",
            "Iteration 3393, loss = 1806.54954212\n",
            "Iteration 3394, loss = 1805.72440960\n",
            "Iteration 3395, loss = 1805.26144824\n",
            "Iteration 3396, loss = 1804.85652209\n",
            "Iteration 3397, loss = 1804.78225165\n",
            "Iteration 3398, loss = 1803.52558468\n",
            "Iteration 3399, loss = 1802.86765705\n",
            "Iteration 3400, loss = 1802.44755929\n",
            "Iteration 3401, loss = 1801.75734100\n",
            "Iteration 3402, loss = 1800.64603458\n",
            "Iteration 3403, loss = 1799.97558839\n",
            "Iteration 3404, loss = 1799.77379473\n",
            "Iteration 3405, loss = 1798.72822016\n",
            "Iteration 3406, loss = 1798.06322709\n",
            "Iteration 3407, loss = 1797.51440107\n",
            "Iteration 3408, loss = 1797.65718090\n",
            "Iteration 3409, loss = 1796.39085561\n",
            "Iteration 3410, loss = 1795.40551370\n",
            "Iteration 3411, loss = 1795.05533192\n",
            "Iteration 3412, loss = 1794.32779078\n",
            "Iteration 3413, loss = 1793.71496157\n",
            "Iteration 3414, loss = 1793.15711895\n",
            "Iteration 3415, loss = 1792.24000125\n",
            "Iteration 3416, loss = 1792.32260588\n",
            "Iteration 3417, loss = 1791.17524851\n",
            "Iteration 3418, loss = 1790.42975045\n",
            "Iteration 3419, loss = 1790.03718118\n",
            "Iteration 3420, loss = 1789.76328919\n",
            "Iteration 3421, loss = 1788.60056260\n",
            "Iteration 3422, loss = 1787.83022376\n",
            "Iteration 3423, loss = 1787.08332243\n",
            "Iteration 3424, loss = 1786.73282300\n",
            "Iteration 3425, loss = 1786.20420223\n",
            "Iteration 3426, loss = 1785.30269866\n",
            "Iteration 3427, loss = 1785.02796466\n",
            "Iteration 3428, loss = 1784.12406576\n",
            "Iteration 3429, loss = 1783.60143062\n",
            "Iteration 3430, loss = 1782.84791448\n",
            "Iteration 3431, loss = 1782.01037703\n",
            "Iteration 3432, loss = 1782.25749873\n",
            "Iteration 3433, loss = 1781.13420395\n",
            "Iteration 3434, loss = 1780.38767925\n",
            "Iteration 3435, loss = 1779.75036804\n",
            "Iteration 3436, loss = 1778.95103453\n",
            "Iteration 3437, loss = 1778.29102383\n",
            "Iteration 3438, loss = 1777.54905474\n",
            "Iteration 3439, loss = 1777.22659409\n",
            "Iteration 3440, loss = 1777.19899748\n",
            "Iteration 3441, loss = 1775.95968656\n",
            "Iteration 3442, loss = 1775.40904301\n",
            "Iteration 3443, loss = 1774.27323820\n",
            "Iteration 3444, loss = 1773.87587386\n",
            "Iteration 3445, loss = 1773.43081903\n",
            "Iteration 3446, loss = 1772.69593425\n",
            "Iteration 3447, loss = 1771.63992329\n",
            "Iteration 3448, loss = 1771.23937853\n",
            "Iteration 3449, loss = 1770.59861934\n",
            "Iteration 3450, loss = 1770.03569281\n",
            "Iteration 3451, loss = 1769.30241088\n",
            "Iteration 3452, loss = 1769.56763073\n",
            "Iteration 3453, loss = 1768.34552257\n",
            "Iteration 3454, loss = 1767.30973588\n",
            "Iteration 3455, loss = 1766.71723893\n",
            "Iteration 3456, loss = 1766.25234496\n",
            "Iteration 3457, loss = 1765.60825141\n",
            "Iteration 3458, loss = 1764.65545632\n",
            "Iteration 3459, loss = 1764.37331738\n",
            "Iteration 3460, loss = 1763.23088710\n",
            "Iteration 3461, loss = 1762.83646751\n",
            "Iteration 3462, loss = 1762.03091433\n",
            "Iteration 3463, loss = 1761.33604952\n",
            "Iteration 3464, loss = 1760.67133747\n",
            "Iteration 3465, loss = 1759.97490964\n",
            "Iteration 3466, loss = 1759.73727724\n",
            "Iteration 3467, loss = 1758.82484124\n",
            "Iteration 3468, loss = 1758.24210322\n",
            "Iteration 3469, loss = 1757.41472345\n",
            "Iteration 3470, loss = 1756.79068565\n",
            "Iteration 3471, loss = 1756.64773004\n",
            "Iteration 3472, loss = 1755.57651346\n",
            "Iteration 3473, loss = 1754.91630840\n",
            "Iteration 3474, loss = 1754.08314070\n",
            "Iteration 3475, loss = 1753.67650130\n",
            "Iteration 3476, loss = 1752.99428101\n",
            "Iteration 3477, loss = 1752.13995535\n",
            "Iteration 3478, loss = 1751.63155260\n",
            "Iteration 3479, loss = 1751.27809506\n",
            "Iteration 3480, loss = 1750.35424084\n",
            "Iteration 3481, loss = 1749.73012763\n",
            "Iteration 3482, loss = 1749.38966097\n",
            "Iteration 3483, loss = 1748.40941045\n",
            "Iteration 3484, loss = 1747.61110865\n",
            "Iteration 3485, loss = 1746.58954108\n",
            "Iteration 3486, loss = 1746.24926936\n",
            "Iteration 3487, loss = 1745.99431096\n",
            "Iteration 3488, loss = 1745.44068794\n",
            "Iteration 3489, loss = 1744.19831280\n",
            "Iteration 3490, loss = 1743.43322661\n",
            "Iteration 3491, loss = 1742.95656801\n",
            "Iteration 3492, loss = 1743.02550505\n",
            "Iteration 3493, loss = 1742.02549176\n",
            "Iteration 3494, loss = 1741.07338733\n",
            "Iteration 3495, loss = 1740.37555146\n",
            "Iteration 3496, loss = 1739.98897055\n",
            "Iteration 3497, loss = 1739.22559326\n",
            "Iteration 3498, loss = 1738.49617770\n",
            "Iteration 3499, loss = 1737.68168175\n",
            "Iteration 3500, loss = 1737.26333590\n",
            "Iteration 3501, loss = 1736.50329007\n",
            "Iteration 3502, loss = 1735.91428955\n",
            "Iteration 3503, loss = 1735.45839863\n",
            "Iteration 3504, loss = 1734.73682458\n",
            "Iteration 3505, loss = 1733.88798994\n",
            "Iteration 3506, loss = 1733.39640927\n",
            "Iteration 3507, loss = 1732.65968851\n",
            "Iteration 3508, loss = 1732.18613396\n",
            "Iteration 3509, loss = 1731.37420164\n",
            "Iteration 3510, loss = 1730.86828019\n",
            "Iteration 3511, loss = 1730.05178740\n",
            "Iteration 3512, loss = 1729.31021465\n",
            "Iteration 3513, loss = 1729.06168055\n",
            "Iteration 3514, loss = 1728.22831264\n",
            "Iteration 3515, loss = 1727.47562660\n",
            "Iteration 3516, loss = 1726.97084334\n",
            "Iteration 3517, loss = 1726.11994538\n",
            "Iteration 3518, loss = 1725.48314559\n",
            "Iteration 3519, loss = 1725.05500706\n",
            "Iteration 3520, loss = 1724.32097693\n",
            "Iteration 3521, loss = 1724.00498385\n",
            "Iteration 3522, loss = 1723.33361089\n",
            "Iteration 3523, loss = 1722.26646942\n",
            "Iteration 3524, loss = 1721.48186366\n",
            "Iteration 3525, loss = 1720.73609278\n",
            "Iteration 3526, loss = 1721.99004353\n",
            "Iteration 3527, loss = 1720.14523508\n",
            "Iteration 3528, loss = 1718.96971224\n",
            "Iteration 3529, loss = 1718.61151560\n",
            "Iteration 3530, loss = 1717.90715167\n",
            "Iteration 3531, loss = 1717.85741176\n",
            "Iteration 3532, loss = 1716.64703049\n",
            "Iteration 3533, loss = 1716.00109358\n",
            "Iteration 3534, loss = 1715.76455165\n",
            "Iteration 3535, loss = 1714.76551880\n",
            "Iteration 3536, loss = 1714.19013060\n",
            "Iteration 3537, loss = 1713.43436906\n",
            "Iteration 3538, loss = 1712.72213046\n",
            "Iteration 3539, loss = 1712.20932519\n",
            "Iteration 3540, loss = 1711.97074054\n",
            "Iteration 3541, loss = 1711.17687663\n",
            "Iteration 3542, loss = 1710.55268878\n",
            "Iteration 3543, loss = 1710.11324046\n",
            "Iteration 3544, loss = 1709.39178014\n",
            "Iteration 3545, loss = 1708.64557134\n",
            "Iteration 3546, loss = 1707.79604293\n",
            "Iteration 3547, loss = 1707.54359225\n",
            "Iteration 3548, loss = 1706.58518572\n",
            "Iteration 3549, loss = 1705.99646697\n",
            "Iteration 3550, loss = 1705.27029068\n",
            "Iteration 3551, loss = 1704.54461139\n",
            "Iteration 3552, loss = 1704.06423438\n",
            "Iteration 3553, loss = 1703.31742204\n",
            "Iteration 3554, loss = 1702.90149380\n",
            "Iteration 3555, loss = 1702.15146729\n",
            "Iteration 3556, loss = 1701.58462415\n",
            "Iteration 3557, loss = 1700.87471206\n",
            "Iteration 3558, loss = 1700.54731397\n",
            "Iteration 3559, loss = 1699.90774942\n",
            "Iteration 3560, loss = 1699.29273465\n",
            "Iteration 3561, loss = 1698.63240566\n",
            "Iteration 3562, loss = 1697.99931336\n",
            "Iteration 3563, loss = 1697.67347455\n",
            "Iteration 3564, loss = 1696.82568984\n",
            "Iteration 3565, loss = 1696.14201810\n",
            "Iteration 3566, loss = 1695.45433350\n",
            "Iteration 3567, loss = 1694.65483490\n",
            "Iteration 3568, loss = 1694.59254806\n",
            "Iteration 3569, loss = 1693.33142540\n",
            "Iteration 3570, loss = 1693.59727536\n",
            "Iteration 3571, loss = 1693.28768120\n",
            "Iteration 3572, loss = 1691.93407737\n",
            "Iteration 3573, loss = 1691.09486207\n",
            "Iteration 3574, loss = 1690.50981473\n",
            "Iteration 3575, loss = 1690.03095655\n",
            "Iteration 3576, loss = 1689.27562181\n",
            "Iteration 3577, loss = 1688.49419144\n",
            "Iteration 3578, loss = 1687.88845910\n",
            "Iteration 3579, loss = 1687.46026331\n",
            "Iteration 3580, loss = 1686.85078425\n",
            "Iteration 3581, loss = 1686.26676702\n",
            "Iteration 3582, loss = 1685.42647483\n",
            "Iteration 3583, loss = 1684.85176994\n",
            "Iteration 3584, loss = 1684.10467503\n",
            "Iteration 3585, loss = 1683.92111745\n",
            "Iteration 3586, loss = 1683.06534522\n",
            "Iteration 3587, loss = 1682.58793586\n",
            "Iteration 3588, loss = 1682.39460976\n",
            "Iteration 3589, loss = 1681.26206872\n",
            "Iteration 3590, loss = 1681.51802869\n",
            "Iteration 3591, loss = 1679.90022539\n",
            "Iteration 3592, loss = 1679.14251110\n",
            "Iteration 3593, loss = 1678.54982191\n",
            "Iteration 3594, loss = 1678.19363470\n",
            "Iteration 3595, loss = 1677.68105574\n",
            "Iteration 3596, loss = 1677.20018831\n",
            "Iteration 3597, loss = 1676.31563374\n",
            "Iteration 3598, loss = 1675.53571548\n",
            "Iteration 3599, loss = 1675.08947354\n",
            "Iteration 3600, loss = 1674.35558255\n",
            "Iteration 3601, loss = 1673.82271164\n",
            "Iteration 3602, loss = 1673.01699571\n",
            "Iteration 3603, loss = 1672.31035614\n",
            "Iteration 3604, loss = 1671.71228568\n",
            "Iteration 3605, loss = 1672.10776738\n",
            "Iteration 3606, loss = 1670.68364684\n",
            "Iteration 3607, loss = 1670.13512601\n",
            "Iteration 3608, loss = 1669.35580773\n",
            "Iteration 3609, loss = 1668.68341094\n",
            "Iteration 3610, loss = 1668.03710606\n",
            "Iteration 3611, loss = 1667.67126523\n",
            "Iteration 3612, loss = 1666.67124245\n",
            "Iteration 3613, loss = 1666.25014997\n",
            "Iteration 3614, loss = 1666.23933763\n",
            "Iteration 3615, loss = 1664.99295324\n",
            "Iteration 3616, loss = 1664.21108250\n",
            "Iteration 3617, loss = 1663.57738434\n",
            "Iteration 3618, loss = 1663.71403418\n",
            "Iteration 3619, loss = 1662.98741736\n",
            "Iteration 3620, loss = 1662.06032515\n",
            "Iteration 3621, loss = 1661.73603400\n",
            "Iteration 3622, loss = 1660.80141110\n",
            "Iteration 3623, loss = 1660.20083032\n",
            "Iteration 3624, loss = 1659.46575440\n",
            "Iteration 3625, loss = 1659.16991431\n",
            "Iteration 3626, loss = 1658.33786825\n",
            "Iteration 3627, loss = 1657.78289726\n",
            "Iteration 3628, loss = 1656.89256874\n",
            "Iteration 3629, loss = 1656.58260833\n",
            "Iteration 3630, loss = 1656.27220771\n",
            "Iteration 3631, loss = 1655.34242165\n",
            "Iteration 3632, loss = 1654.76849774\n",
            "Iteration 3633, loss = 1653.93380550\n",
            "Iteration 3634, loss = 1653.82614781\n",
            "Iteration 3635, loss = 1652.73267014\n",
            "Iteration 3636, loss = 1652.32610085\n",
            "Iteration 3637, loss = 1651.41122698\n",
            "Iteration 3638, loss = 1651.09239114\n",
            "Iteration 3639, loss = 1650.21952281\n",
            "Iteration 3640, loss = 1649.61038541\n",
            "Iteration 3641, loss = 1649.16963589\n",
            "Iteration 3642, loss = 1648.60274508\n",
            "Iteration 3643, loss = 1647.78154689\n",
            "Iteration 3644, loss = 1647.25886411\n",
            "Iteration 3645, loss = 1646.84436035\n",
            "Iteration 3646, loss = 1646.09652266\n",
            "Iteration 3647, loss = 1645.79617441\n",
            "Iteration 3648, loss = 1644.97091663\n",
            "Iteration 3649, loss = 1644.49585832\n",
            "Iteration 3650, loss = 1643.75073057\n",
            "Iteration 3651, loss = 1643.44601231\n",
            "Iteration 3652, loss = 1642.33422666\n",
            "Iteration 3653, loss = 1642.10469918\n",
            "Iteration 3654, loss = 1641.23288327\n",
            "Iteration 3655, loss = 1640.83445098\n",
            "Iteration 3656, loss = 1640.30985836\n",
            "Iteration 3657, loss = 1639.45674549\n",
            "Iteration 3658, loss = 1638.88932445\n",
            "Iteration 3659, loss = 1638.22628475\n",
            "Iteration 3660, loss = 1637.82815577\n",
            "Iteration 3661, loss = 1637.64432399\n",
            "Iteration 3662, loss = 1636.48195149\n",
            "Iteration 3663, loss = 1636.28804780\n",
            "Iteration 3664, loss = 1635.70025277\n",
            "Iteration 3665, loss = 1635.05735795\n",
            "Iteration 3666, loss = 1634.08605873\n",
            "Iteration 3667, loss = 1633.47215685\n",
            "Iteration 3668, loss = 1633.28585990\n",
            "Iteration 3669, loss = 1632.43527814\n",
            "Iteration 3670, loss = 1632.03733587\n",
            "Iteration 3671, loss = 1631.35201506\n",
            "Iteration 3672, loss = 1630.68707262\n",
            "Iteration 3673, loss = 1630.08499606\n",
            "Iteration 3674, loss = 1629.53430603\n",
            "Iteration 3675, loss = 1629.09014672\n",
            "Iteration 3676, loss = 1628.51188049\n",
            "Iteration 3677, loss = 1628.06302124\n",
            "Iteration 3678, loss = 1627.30504563\n",
            "Iteration 3679, loss = 1627.82180739\n",
            "Iteration 3680, loss = 1626.75398719\n",
            "Iteration 3681, loss = 1626.09698622\n",
            "Iteration 3682, loss = 1625.03647072\n",
            "Iteration 3683, loss = 1624.63605266\n",
            "Iteration 3684, loss = 1623.74942150\n",
            "Iteration 3685, loss = 1623.31801693\n",
            "Iteration 3686, loss = 1622.79656536\n",
            "Iteration 3687, loss = 1622.35979916\n",
            "Iteration 3688, loss = 1621.45808812\n",
            "Iteration 3689, loss = 1621.62931908\n",
            "Iteration 3690, loss = 1620.29252629\n",
            "Iteration 3691, loss = 1620.16010286\n",
            "Iteration 3692, loss = 1619.15619152\n",
            "Iteration 3693, loss = 1618.69098781\n",
            "Iteration 3694, loss = 1618.05708163\n",
            "Iteration 3695, loss = 1617.33578767\n",
            "Iteration 3696, loss = 1616.99805186\n",
            "Iteration 3697, loss = 1616.14540208\n",
            "Iteration 3698, loss = 1615.75233459\n",
            "Iteration 3699, loss = 1615.03263354\n",
            "Iteration 3700, loss = 1614.36679432\n",
            "Iteration 3701, loss = 1613.94789995\n",
            "Iteration 3702, loss = 1615.15477773\n",
            "Iteration 3703, loss = 1612.81292949\n",
            "Iteration 3704, loss = 1612.62933737\n",
            "Iteration 3705, loss = 1611.75209843\n",
            "Iteration 3706, loss = 1610.97966915\n",
            "Iteration 3707, loss = 1610.43712587\n",
            "Iteration 3708, loss = 1610.12334068\n",
            "Iteration 3709, loss = 1609.88082297\n",
            "Iteration 3710, loss = 1608.79435483\n",
            "Iteration 3711, loss = 1607.99186087\n",
            "Iteration 3712, loss = 1608.21089379\n",
            "Iteration 3713, loss = 1607.18083525\n",
            "Iteration 3714, loss = 1606.51572976\n",
            "Iteration 3715, loss = 1606.42605447\n",
            "Iteration 3716, loss = 1605.64501962\n",
            "Iteration 3717, loss = 1604.50438585\n",
            "Iteration 3718, loss = 1604.56745600\n",
            "Iteration 3719, loss = 1603.90279439\n",
            "Iteration 3720, loss = 1603.35769616\n",
            "Iteration 3721, loss = 1602.48168403\n",
            "Iteration 3722, loss = 1601.92975908\n",
            "Iteration 3723, loss = 1601.78064128\n",
            "Iteration 3724, loss = 1600.81046265\n",
            "Iteration 3725, loss = 1600.15390800\n",
            "Iteration 3726, loss = 1599.77010370\n",
            "Iteration 3727, loss = 1599.04393839\n",
            "Iteration 3728, loss = 1598.78700145\n",
            "Iteration 3729, loss = 1597.90117298\n",
            "Iteration 3730, loss = 1597.38898114\n",
            "Iteration 3731, loss = 1596.45049705\n",
            "Iteration 3732, loss = 1596.57169341\n",
            "Iteration 3733, loss = 1595.85276688\n",
            "Iteration 3734, loss = 1595.28452616\n",
            "Iteration 3735, loss = 1594.44065272\n",
            "Iteration 3736, loss = 1593.95690015\n",
            "Iteration 3737, loss = 1593.53632349\n",
            "Iteration 3738, loss = 1592.64310189\n",
            "Iteration 3739, loss = 1592.08659767\n",
            "Iteration 3740, loss = 1591.57756033\n",
            "Iteration 3741, loss = 1590.85159815\n",
            "Iteration 3742, loss = 1590.21797858\n",
            "Iteration 3743, loss = 1590.21554428\n",
            "Iteration 3744, loss = 1589.92346378\n",
            "Iteration 3745, loss = 1589.08479971\n",
            "Iteration 3746, loss = 1588.29309642\n",
            "Iteration 3747, loss = 1587.70768341\n",
            "Iteration 3748, loss = 1587.13055219\n",
            "Iteration 3749, loss = 1586.56238954\n",
            "Iteration 3750, loss = 1586.51994244\n",
            "Iteration 3751, loss = 1585.76270525\n",
            "Iteration 3752, loss = 1585.00622546\n",
            "Iteration 3753, loss = 1584.45251783\n",
            "Iteration 3754, loss = 1584.05522554\n",
            "Iteration 3755, loss = 1583.29831972\n",
            "Iteration 3756, loss = 1582.88761605\n",
            "Iteration 3757, loss = 1582.68774312\n",
            "Iteration 3758, loss = 1581.78510406\n",
            "Iteration 3759, loss = 1581.16870115\n",
            "Iteration 3760, loss = 1580.29556929\n",
            "Iteration 3761, loss = 1579.86972093\n",
            "Iteration 3762, loss = 1579.06345793\n",
            "Iteration 3763, loss = 1578.99665674\n",
            "Iteration 3764, loss = 1578.01267620\n",
            "Iteration 3765, loss = 1577.86634693\n",
            "Iteration 3766, loss = 1577.24562531\n",
            "Iteration 3767, loss = 1576.39911048\n",
            "Iteration 3768, loss = 1575.75019582\n",
            "Iteration 3769, loss = 1575.13639524\n",
            "Iteration 3770, loss = 1575.46099415\n",
            "Iteration 3771, loss = 1574.38788672\n",
            "Iteration 3772, loss = 1574.71819520\n",
            "Iteration 3773, loss = 1573.06304392\n",
            "Iteration 3774, loss = 1572.55412095\n",
            "Iteration 3775, loss = 1572.66015564\n",
            "Iteration 3776, loss = 1571.56726873\n",
            "Iteration 3777, loss = 1570.79736182\n",
            "Iteration 3778, loss = 1570.37543188\n",
            "Iteration 3779, loss = 1569.94976778\n",
            "Iteration 3780, loss = 1569.17508407\n",
            "Iteration 3781, loss = 1568.59173853\n",
            "Iteration 3782, loss = 1568.31064928\n",
            "Iteration 3783, loss = 1567.63632756\n",
            "Iteration 3784, loss = 1567.09725216\n",
            "Iteration 3785, loss = 1566.74913740\n",
            "Iteration 3786, loss = 1566.28386615\n",
            "Iteration 3787, loss = 1565.44248605\n",
            "Iteration 3788, loss = 1564.93777868\n",
            "Iteration 3789, loss = 1564.20606539\n",
            "Iteration 3790, loss = 1563.59250327\n",
            "Iteration 3791, loss = 1563.18355277\n",
            "Iteration 3792, loss = 1562.80661125\n",
            "Iteration 3793, loss = 1562.23176889\n",
            "Iteration 3794, loss = 1562.53629103\n",
            "Iteration 3795, loss = 1560.75875924\n",
            "Iteration 3796, loss = 1560.46221415\n",
            "Iteration 3797, loss = 1559.78932810\n",
            "Iteration 3798, loss = 1559.31862262\n",
            "Iteration 3799, loss = 1558.69207763\n",
            "Iteration 3800, loss = 1558.33291988\n",
            "Iteration 3801, loss = 1557.82923383\n",
            "Iteration 3802, loss = 1557.00717104\n",
            "Iteration 3803, loss = 1556.63688635\n",
            "Iteration 3804, loss = 1556.04557632\n",
            "Iteration 3805, loss = 1555.24844328\n",
            "Iteration 3806, loss = 1554.96647372\n",
            "Iteration 3807, loss = 1554.11056282\n",
            "Iteration 3808, loss = 1554.14260366\n",
            "Iteration 3809, loss = 1553.23655906\n",
            "Iteration 3810, loss = 1552.65823925\n",
            "Iteration 3811, loss = 1552.15304099\n",
            "Iteration 3812, loss = 1551.71526003\n",
            "Iteration 3813, loss = 1550.91481312\n",
            "Iteration 3814, loss = 1550.54875285\n",
            "Iteration 3815, loss = 1549.72534641\n",
            "Iteration 3816, loss = 1549.15689190\n",
            "Iteration 3817, loss = 1549.72595890\n",
            "Iteration 3818, loss = 1549.28578069\n",
            "Iteration 3819, loss = 1547.39335889\n",
            "Iteration 3820, loss = 1546.93023554\n",
            "Iteration 3821, loss = 1546.19140809\n",
            "Iteration 3822, loss = 1546.16756355\n",
            "Iteration 3823, loss = 1545.16153850\n",
            "Iteration 3824, loss = 1545.08264355\n",
            "Iteration 3825, loss = 1544.37894775\n",
            "Iteration 3826, loss = 1543.87759976\n",
            "Iteration 3827, loss = 1543.16048371\n",
            "Iteration 3828, loss = 1542.54905114\n",
            "Iteration 3829, loss = 1541.98397166\n",
            "Iteration 3830, loss = 1541.39969587\n",
            "Iteration 3831, loss = 1541.01377101\n",
            "Iteration 3832, loss = 1540.27861512\n",
            "Iteration 3833, loss = 1539.81840799\n",
            "Iteration 3834, loss = 1539.04251745\n",
            "Iteration 3835, loss = 1538.82976877\n",
            "Iteration 3836, loss = 1538.28209721\n",
            "Iteration 3837, loss = 1537.57004398\n",
            "Iteration 3838, loss = 1537.64273527\n",
            "Iteration 3839, loss = 1536.31932604\n",
            "Iteration 3840, loss = 1536.84034445\n",
            "Iteration 3841, loss = 1536.26842494\n",
            "Iteration 3842, loss = 1535.12150113\n",
            "Iteration 3843, loss = 1533.97681525\n",
            "Iteration 3844, loss = 1534.11500697\n",
            "Iteration 3845, loss = 1533.81364965\n",
            "Iteration 3846, loss = 1533.47411754\n",
            "Iteration 3847, loss = 1532.24705397\n",
            "Iteration 3848, loss = 1531.56920867\n",
            "Iteration 3849, loss = 1531.71961303\n",
            "Iteration 3850, loss = 1530.32251001\n",
            "Iteration 3851, loss = 1530.09497887\n",
            "Iteration 3852, loss = 1529.30482894\n",
            "Iteration 3853, loss = 1529.02311858\n",
            "Iteration 3854, loss = 1528.64305549\n",
            "Iteration 3855, loss = 1527.81886172\n",
            "Iteration 3856, loss = 1527.61152671\n",
            "Iteration 3857, loss = 1526.68170002\n",
            "Iteration 3858, loss = 1526.39419045\n",
            "Iteration 3859, loss = 1525.58615196\n",
            "Iteration 3860, loss = 1525.05840408\n",
            "Iteration 3861, loss = 1524.63311709\n",
            "Iteration 3862, loss = 1524.15915435\n",
            "Iteration 3863, loss = 1523.44445607\n",
            "Iteration 3864, loss = 1522.65199758\n",
            "Iteration 3865, loss = 1522.11316099\n",
            "Iteration 3866, loss = 1522.08733040\n",
            "Iteration 3867, loss = 1521.37323036\n",
            "Iteration 3868, loss = 1520.42962889\n",
            "Iteration 3869, loss = 1519.88425360\n",
            "Iteration 3870, loss = 1519.44749755\n",
            "Iteration 3871, loss = 1518.77682797\n",
            "Iteration 3872, loss = 1518.48360681\n",
            "Iteration 3873, loss = 1518.27747811\n",
            "Iteration 3874, loss = 1517.62070605\n",
            "Iteration 3875, loss = 1517.20052292\n",
            "Iteration 3876, loss = 1516.32009354\n",
            "Iteration 3877, loss = 1515.88364205\n",
            "Iteration 3878, loss = 1515.17649545\n",
            "Iteration 3879, loss = 1514.38249365\n",
            "Iteration 3880, loss = 1514.48828326\n",
            "Iteration 3881, loss = 1514.04012715\n",
            "Iteration 3882, loss = 1513.73224507\n",
            "Iteration 3883, loss = 1512.52495983\n",
            "Iteration 3884, loss = 1512.01444031\n",
            "Iteration 3885, loss = 1511.47351334\n",
            "Iteration 3886, loss = 1511.32708076\n",
            "Iteration 3887, loss = 1510.94851626\n",
            "Iteration 3888, loss = 1509.95054344\n",
            "Iteration 3889, loss = 1509.31015778\n",
            "Iteration 3890, loss = 1508.89708702\n",
            "Iteration 3891, loss = 1508.50301853\n",
            "Iteration 3892, loss = 1507.86193140\n",
            "Iteration 3893, loss = 1507.99496760\n",
            "Iteration 3894, loss = 1506.60723323\n",
            "Iteration 3895, loss = 1506.46986432\n",
            "Iteration 3896, loss = 1506.19360215\n",
            "Iteration 3897, loss = 1506.07836830\n",
            "Iteration 3898, loss = 1506.57602086\n",
            "Iteration 3899, loss = 1504.38404255\n",
            "Iteration 3900, loss = 1504.45263373\n",
            "Iteration 3901, loss = 1503.30836284\n",
            "Iteration 3902, loss = 1502.72825301\n",
            "Iteration 3903, loss = 1502.33931519\n",
            "Iteration 3904, loss = 1502.40725678\n",
            "Iteration 3905, loss = 1501.62148927\n",
            "Iteration 3906, loss = 1500.67724717\n",
            "Iteration 3907, loss = 1499.95180571\n",
            "Iteration 3908, loss = 1500.94512659\n",
            "Iteration 3909, loss = 1499.53377064\n",
            "Iteration 3910, loss = 1498.62655099\n",
            "Iteration 3911, loss = 1499.26964045\n",
            "Iteration 3912, loss = 1497.81381653\n",
            "Iteration 3913, loss = 1497.36513328\n",
            "Iteration 3914, loss = 1496.54842390\n",
            "Iteration 3915, loss = 1496.23710387\n",
            "Iteration 3916, loss = 1495.33439411\n",
            "Iteration 3917, loss = 1495.51300849\n",
            "Iteration 3918, loss = 1494.77791475\n",
            "Iteration 3919, loss = 1494.14658882\n",
            "Iteration 3920, loss = 1493.41771205\n",
            "Iteration 3921, loss = 1492.91956793\n",
            "Iteration 3922, loss = 1492.39363268\n",
            "Iteration 3923, loss = 1491.99704511\n",
            "Iteration 3924, loss = 1491.42586161\n",
            "Iteration 3925, loss = 1490.92901898\n",
            "Iteration 3926, loss = 1490.17223287\n",
            "Iteration 3927, loss = 1490.01505220\n",
            "Iteration 3928, loss = 1491.63659138\n",
            "Iteration 3929, loss = 1489.22031642\n",
            "Iteration 3930, loss = 1487.90435032\n",
            "Iteration 3931, loss = 1487.54253624\n",
            "Iteration 3932, loss = 1487.64549181\n",
            "Iteration 3933, loss = 1487.34449921\n",
            "Iteration 3934, loss = 1487.45482103\n",
            "Iteration 3935, loss = 1486.18422821\n",
            "Iteration 3936, loss = 1485.78728409\n",
            "Iteration 3937, loss = 1484.61348033\n",
            "Iteration 3938, loss = 1484.17051123\n",
            "Iteration 3939, loss = 1483.79850442\n",
            "Iteration 3940, loss = 1483.51801670\n",
            "Iteration 3941, loss = 1482.57609111\n",
            "Iteration 3942, loss = 1482.07413330\n",
            "Iteration 3943, loss = 1481.68782435\n",
            "Iteration 3944, loss = 1481.78436178\n",
            "Iteration 3945, loss = 1480.51563203\n",
            "Iteration 3946, loss = 1480.61658542\n",
            "Iteration 3947, loss = 1479.61958654\n",
            "Iteration 3948, loss = 1478.93692239\n",
            "Iteration 3949, loss = 1479.50254289\n",
            "Iteration 3950, loss = 1478.24458493\n",
            "Iteration 3951, loss = 1477.48375574\n",
            "Iteration 3952, loss = 1476.96258175\n",
            "Iteration 3953, loss = 1476.38336699\n",
            "Iteration 3954, loss = 1475.76545531\n",
            "Iteration 3955, loss = 1476.24862731\n",
            "Iteration 3956, loss = 1475.35506950\n",
            "Iteration 3957, loss = 1474.97248370\n",
            "Iteration 3958, loss = 1474.20848196\n",
            "Iteration 3959, loss = 1473.69007513\n",
            "Iteration 3960, loss = 1472.78117825\n",
            "Iteration 3961, loss = 1472.56175252\n",
            "Iteration 3962, loss = 1471.86982888\n",
            "Iteration 3963, loss = 1471.24710838\n",
            "Iteration 3964, loss = 1470.69075271\n",
            "Iteration 3965, loss = 1470.21055357\n",
            "Iteration 3966, loss = 1469.98991434\n",
            "Iteration 3967, loss = 1469.65287739\n",
            "Iteration 3968, loss = 1469.73813806\n",
            "Iteration 3969, loss = 1468.36397711\n",
            "Iteration 3970, loss = 1467.57363283\n",
            "Iteration 3971, loss = 1467.32030571\n",
            "Iteration 3972, loss = 1466.70850410\n",
            "Iteration 3973, loss = 1467.88046661\n",
            "Iteration 3974, loss = 1465.78431975\n",
            "Iteration 3975, loss = 1465.52679657\n",
            "Iteration 3976, loss = 1464.93476980\n",
            "Iteration 3977, loss = 1464.46227129\n",
            "Iteration 3978, loss = 1463.96533017\n",
            "Iteration 3979, loss = 1464.48898711\n",
            "Iteration 3980, loss = 1463.04762782\n",
            "Iteration 3981, loss = 1463.05239209\n",
            "Iteration 3982, loss = 1462.30352622\n",
            "Iteration 3983, loss = 1462.09149124\n",
            "Iteration 3984, loss = 1461.06596097\n",
            "Iteration 3985, loss = 1460.55488885\n",
            "Iteration 3986, loss = 1459.99809540\n",
            "Iteration 3987, loss = 1459.86632608\n",
            "Iteration 3988, loss = 1458.97465406\n",
            "Iteration 3989, loss = 1458.56394508\n",
            "Iteration 3990, loss = 1457.99042994\n",
            "Iteration 3991, loss = 1457.38204059\n",
            "Iteration 3992, loss = 1457.74787782\n",
            "Iteration 3993, loss = 1456.85638400\n",
            "Iteration 3994, loss = 1456.47356622\n",
            "Iteration 3995, loss = 1455.72552679\n",
            "Iteration 3996, loss = 1455.35728635\n",
            "Iteration 3997, loss = 1455.39113931\n",
            "Iteration 3998, loss = 1454.26490680\n",
            "Iteration 3999, loss = 1453.54653796\n",
            "Iteration 4000, loss = 1452.93733416\n",
            "Iteration 4001, loss = 1453.54523448\n",
            "Iteration 4002, loss = 1453.20143237\n",
            "Iteration 4003, loss = 1451.52810226\n",
            "Iteration 4004, loss = 1451.23487041\n",
            "Iteration 4005, loss = 1451.04789986\n",
            "Iteration 4006, loss = 1449.99863688\n",
            "Iteration 4007, loss = 1449.51126110\n",
            "Iteration 4008, loss = 1449.35302087\n",
            "Iteration 4009, loss = 1448.92967229\n",
            "Iteration 4010, loss = 1448.26154679\n",
            "Iteration 4011, loss = 1447.73494576\n",
            "Iteration 4012, loss = 1447.40119135\n",
            "Iteration 4013, loss = 1447.14808720\n",
            "Iteration 4014, loss = 1446.46716264\n",
            "Iteration 4015, loss = 1445.79656199\n",
            "Iteration 4016, loss = 1446.39039214\n",
            "Iteration 4017, loss = 1444.85930864\n",
            "Iteration 4018, loss = 1444.25453156\n",
            "Iteration 4019, loss = 1444.27585846\n",
            "Iteration 4020, loss = 1443.52650919\n",
            "Iteration 4021, loss = 1442.74622867\n",
            "Iteration 4022, loss = 1442.65202629\n",
            "Iteration 4023, loss = 1441.69176866\n",
            "Iteration 4024, loss = 1441.32809081\n",
            "Iteration 4025, loss = 1441.02953143\n",
            "Iteration 4026, loss = 1440.26330381\n",
            "Iteration 4027, loss = 1439.81406297\n",
            "Iteration 4028, loss = 1439.13199491\n",
            "Iteration 4029, loss = 1438.81352128\n",
            "Iteration 4030, loss = 1438.20752167\n",
            "Iteration 4031, loss = 1438.46797679\n",
            "Iteration 4032, loss = 1437.72148856\n",
            "Iteration 4033, loss = 1436.74745816\n",
            "Iteration 4034, loss = 1436.51398860\n",
            "Iteration 4035, loss = 1436.22964908\n",
            "Iteration 4036, loss = 1435.47044748\n",
            "Iteration 4037, loss = 1435.05141418\n",
            "Iteration 4038, loss = 1434.60825355\n",
            "Iteration 4039, loss = 1433.79988809\n",
            "Iteration 4040, loss = 1433.53118039\n",
            "Iteration 4041, loss = 1433.66769886\n",
            "Iteration 4042, loss = 1432.90319880\n",
            "Iteration 4043, loss = 1431.94884152\n",
            "Iteration 4044, loss = 1431.97678450\n",
            "Iteration 4045, loss = 1431.08204030\n",
            "Iteration 4046, loss = 1430.81972537\n",
            "Iteration 4047, loss = 1430.02673161\n",
            "Iteration 4048, loss = 1429.52506455\n",
            "Iteration 4049, loss = 1428.84672069\n",
            "Iteration 4050, loss = 1428.35881252\n",
            "Iteration 4051, loss = 1428.14762683\n",
            "Iteration 4052, loss = 1427.87629324\n",
            "Iteration 4053, loss = 1427.03284549\n",
            "Iteration 4054, loss = 1426.76456499\n",
            "Iteration 4055, loss = 1426.32203767\n",
            "Iteration 4056, loss = 1425.59698351\n",
            "Iteration 4057, loss = 1425.11328814\n",
            "Iteration 4058, loss = 1424.69122138\n",
            "Iteration 4059, loss = 1424.24714230\n",
            "Iteration 4060, loss = 1423.53053939\n",
            "Iteration 4061, loss = 1423.05427607\n",
            "Iteration 4062, loss = 1422.64264090\n",
            "Iteration 4063, loss = 1422.26081825\n",
            "Iteration 4064, loss = 1421.81951075\n",
            "Iteration 4065, loss = 1421.68960738\n",
            "Iteration 4066, loss = 1420.97994836\n",
            "Iteration 4067, loss = 1421.03108279\n",
            "Iteration 4068, loss = 1420.20545512\n",
            "Iteration 4069, loss = 1419.24947052\n",
            "Iteration 4070, loss = 1419.15454977\n",
            "Iteration 4071, loss = 1418.84233144\n",
            "Iteration 4072, loss = 1418.78550282\n",
            "Iteration 4073, loss = 1417.50209080\n",
            "Iteration 4074, loss = 1417.45137606\n",
            "Iteration 4075, loss = 1416.66853508\n",
            "Iteration 4076, loss = 1416.00530113\n",
            "Iteration 4077, loss = 1415.57872127\n",
            "Iteration 4078, loss = 1415.25411879\n",
            "Iteration 4079, loss = 1414.80476769\n",
            "Iteration 4080, loss = 1414.17711432\n",
            "Iteration 4081, loss = 1413.66584311\n",
            "Iteration 4082, loss = 1413.24852990\n",
            "Iteration 4083, loss = 1412.69628905\n",
            "Iteration 4084, loss = 1412.36417915\n",
            "Iteration 4085, loss = 1412.10308080\n",
            "Iteration 4086, loss = 1412.36160105\n",
            "Iteration 4087, loss = 1411.19770982\n",
            "Iteration 4088, loss = 1410.43080008\n",
            "Iteration 4089, loss = 1409.99890661\n",
            "Iteration 4090, loss = 1409.59169997\n",
            "Iteration 4091, loss = 1409.06655360\n",
            "Iteration 4092, loss = 1408.76722573\n",
            "Iteration 4093, loss = 1408.17187277\n",
            "Iteration 4094, loss = 1407.61386511\n",
            "Iteration 4095, loss = 1407.11506768\n",
            "Iteration 4096, loss = 1406.60343462\n",
            "Iteration 4097, loss = 1406.63229108\n",
            "Iteration 4098, loss = 1406.07522044\n",
            "Iteration 4099, loss = 1405.87037665\n",
            "Iteration 4100, loss = 1404.94057588\n",
            "Iteration 4101, loss = 1404.28301641\n",
            "Iteration 4102, loss = 1403.82261753\n",
            "Iteration 4103, loss = 1403.64610409\n",
            "Iteration 4104, loss = 1402.93923850\n",
            "Iteration 4105, loss = 1402.45950155\n",
            "Iteration 4106, loss = 1401.83521387\n",
            "Iteration 4107, loss = 1401.40391656\n",
            "Iteration 4108, loss = 1401.38954817\n",
            "Iteration 4109, loss = 1400.74094982\n",
            "Iteration 4110, loss = 1400.25301328\n",
            "Iteration 4111, loss = 1400.06573395\n",
            "Iteration 4112, loss = 1399.73606416\n",
            "Iteration 4113, loss = 1398.79931004\n",
            "Iteration 4114, loss = 1398.21328438\n",
            "Iteration 4115, loss = 1397.70984100\n",
            "Iteration 4116, loss = 1398.15646357\n",
            "Iteration 4117, loss = 1398.04151354\n",
            "Iteration 4118, loss = 1396.25648502\n",
            "Iteration 4119, loss = 1396.09900602\n",
            "Iteration 4120, loss = 1395.42185554\n",
            "Iteration 4121, loss = 1394.91889580\n",
            "Iteration 4122, loss = 1394.69392086\n",
            "Iteration 4123, loss = 1394.72569611\n",
            "Iteration 4124, loss = 1393.90446467\n",
            "Iteration 4125, loss = 1393.27264124\n",
            "Iteration 4126, loss = 1392.67410701\n",
            "Iteration 4127, loss = 1392.44239641\n",
            "Iteration 4128, loss = 1391.89341314\n",
            "Iteration 4129, loss = 1391.45333846\n",
            "Iteration 4130, loss = 1390.89619283\n",
            "Iteration 4131, loss = 1390.13495938\n",
            "Iteration 4132, loss = 1389.97680929\n",
            "Iteration 4133, loss = 1389.35792887\n",
            "Iteration 4134, loss = 1389.66576457\n",
            "Iteration 4135, loss = 1388.51374968\n",
            "Iteration 4136, loss = 1387.78262892\n",
            "Iteration 4137, loss = 1387.34393926\n",
            "Iteration 4138, loss = 1387.31866592\n",
            "Iteration 4139, loss = 1386.35907792\n",
            "Iteration 4140, loss = 1386.05608527\n",
            "Iteration 4141, loss = 1385.54262362\n",
            "Iteration 4142, loss = 1385.14494643\n",
            "Iteration 4143, loss = 1384.93588135\n",
            "Iteration 4144, loss = 1384.47378816\n",
            "Iteration 4145, loss = 1383.83040883\n",
            "Iteration 4146, loss = 1383.76025000\n",
            "Iteration 4147, loss = 1382.86365622\n",
            "Iteration 4148, loss = 1382.71386768\n",
            "Iteration 4149, loss = 1381.85851319\n",
            "Iteration 4150, loss = 1381.68528120\n",
            "Iteration 4151, loss = 1381.23278414\n",
            "Iteration 4152, loss = 1380.91569789\n",
            "Iteration 4153, loss = 1380.75821562\n",
            "Iteration 4154, loss = 1379.75222617\n",
            "Iteration 4155, loss = 1379.23821062\n",
            "Iteration 4156, loss = 1379.05198643\n",
            "Iteration 4157, loss = 1378.77692692\n",
            "Iteration 4158, loss = 1378.52285566\n",
            "Iteration 4159, loss = 1377.52285253\n",
            "Iteration 4160, loss = 1377.45792287\n",
            "Iteration 4161, loss = 1376.82697132\n",
            "Iteration 4162, loss = 1376.53810258\n",
            "Iteration 4163, loss = 1375.83097448\n",
            "Iteration 4164, loss = 1375.26663891\n",
            "Iteration 4165, loss = 1374.74348608\n",
            "Iteration 4166, loss = 1374.93929740\n",
            "Iteration 4167, loss = 1373.94392194\n",
            "Iteration 4168, loss = 1373.72770123\n",
            "Iteration 4169, loss = 1373.46989438\n",
            "Iteration 4170, loss = 1373.78882904\n",
            "Iteration 4171, loss = 1372.14832965\n",
            "Iteration 4172, loss = 1371.58673977\n",
            "Iteration 4173, loss = 1371.39330277\n",
            "Iteration 4174, loss = 1370.56461043\n",
            "Iteration 4175, loss = 1370.54509116\n",
            "Iteration 4176, loss = 1369.63548082\n",
            "Iteration 4177, loss = 1369.34896123\n",
            "Iteration 4178, loss = 1369.07541917\n",
            "Iteration 4179, loss = 1368.87010808\n",
            "Iteration 4180, loss = 1368.47684373\n",
            "Iteration 4181, loss = 1367.57280572\n",
            "Iteration 4182, loss = 1367.04756683\n",
            "Iteration 4183, loss = 1366.55359522\n",
            "Iteration 4184, loss = 1366.50511510\n",
            "Iteration 4185, loss = 1365.92365111\n",
            "Iteration 4186, loss = 1365.20890460\n",
            "Iteration 4187, loss = 1364.91150020\n",
            "Iteration 4188, loss = 1364.64408767\n",
            "Iteration 4189, loss = 1364.11566515\n",
            "Iteration 4190, loss = 1363.80171742\n",
            "Iteration 4191, loss = 1363.79089999\n",
            "Iteration 4192, loss = 1362.78334643\n",
            "Iteration 4193, loss = 1362.59671105\n",
            "Iteration 4194, loss = 1362.14404853\n",
            "Iteration 4195, loss = 1362.24564828\n",
            "Iteration 4196, loss = 1361.18321373\n",
            "Iteration 4197, loss = 1360.45332979\n",
            "Iteration 4198, loss = 1360.62264671\n",
            "Iteration 4199, loss = 1359.42264859\n",
            "Iteration 4200, loss = 1358.79625578\n",
            "Iteration 4201, loss = 1358.61583298\n",
            "Iteration 4202, loss = 1358.24224189\n",
            "Iteration 4203, loss = 1357.69483239\n",
            "Iteration 4204, loss = 1357.52248943\n",
            "Iteration 4205, loss = 1356.59902486\n",
            "Iteration 4206, loss = 1356.47354969\n",
            "Iteration 4207, loss = 1355.92825741\n",
            "Iteration 4208, loss = 1355.51187244\n",
            "Iteration 4209, loss = 1355.06211512\n",
            "Iteration 4210, loss = 1355.25605194\n",
            "Iteration 4211, loss = 1354.57701045\n",
            "Iteration 4212, loss = 1353.70196489\n",
            "Iteration 4213, loss = 1354.08825621\n",
            "Iteration 4214, loss = 1353.03040097\n",
            "Iteration 4215, loss = 1353.19005682\n",
            "Iteration 4216, loss = 1352.14396274\n",
            "Iteration 4217, loss = 1351.35235196\n",
            "Iteration 4218, loss = 1350.94279715\n",
            "Iteration 4219, loss = 1350.85115647\n",
            "Iteration 4220, loss = 1350.29172236\n",
            "Iteration 4221, loss = 1349.84590603\n",
            "Iteration 4222, loss = 1349.34094320\n",
            "Iteration 4223, loss = 1349.37337375\n",
            "Iteration 4224, loss = 1349.05492011\n",
            "Iteration 4225, loss = 1348.17781296\n",
            "Iteration 4226, loss = 1348.43738022\n",
            "Iteration 4227, loss = 1347.35439317\n",
            "Iteration 4228, loss = 1346.77418496\n",
            "Iteration 4229, loss = 1346.35121742\n",
            "Iteration 4230, loss = 1345.80216540\n",
            "Iteration 4231, loss = 1346.18112722\n",
            "Iteration 4232, loss = 1345.16884634\n",
            "Iteration 4233, loss = 1345.07513930\n",
            "Iteration 4234, loss = 1344.56789734\n",
            "Iteration 4235, loss = 1343.83633255\n",
            "Iteration 4236, loss = 1343.76619482\n",
            "Iteration 4237, loss = 1343.16381472\n",
            "Iteration 4238, loss = 1342.48894984\n",
            "Iteration 4239, loss = 1341.95202319\n",
            "Iteration 4240, loss = 1341.65198371\n",
            "Iteration 4241, loss = 1341.91454993\n",
            "Iteration 4242, loss = 1340.62694752\n",
            "Iteration 4243, loss = 1340.54345104\n",
            "Iteration 4244, loss = 1340.19081291\n",
            "Iteration 4245, loss = 1339.67330594\n",
            "Iteration 4246, loss = 1339.45465872\n",
            "Iteration 4247, loss = 1338.60660273\n",
            "Iteration 4248, loss = 1338.19859608\n",
            "Iteration 4249, loss = 1337.94691653\n",
            "Iteration 4250, loss = 1337.48203274\n",
            "Iteration 4251, loss = 1336.94927845\n",
            "Iteration 4252, loss = 1336.60890617\n",
            "Iteration 4253, loss = 1336.20680632\n",
            "Iteration 4254, loss = 1335.63432602\n",
            "Iteration 4255, loss = 1335.37273608\n",
            "Iteration 4256, loss = 1335.08958554\n",
            "Iteration 4257, loss = 1334.53138112\n",
            "Iteration 4258, loss = 1334.54675257\n",
            "Iteration 4259, loss = 1333.42150654\n",
            "Iteration 4260, loss = 1333.01010911\n",
            "Iteration 4261, loss = 1332.78849153\n",
            "Iteration 4262, loss = 1332.18040326\n",
            "Iteration 4263, loss = 1332.77568956\n",
            "Iteration 4264, loss = 1331.03249099\n",
            "Iteration 4265, loss = 1330.61902252\n",
            "Iteration 4266, loss = 1330.68553108\n",
            "Iteration 4267, loss = 1330.78428725\n",
            "Iteration 4268, loss = 1329.53660217\n",
            "Iteration 4269, loss = 1329.02103481\n",
            "Iteration 4270, loss = 1328.48832742\n",
            "Iteration 4271, loss = 1328.52452372\n",
            "Iteration 4272, loss = 1327.95060515\n",
            "Iteration 4273, loss = 1327.91854169\n",
            "Iteration 4274, loss = 1327.15571630\n",
            "Iteration 4275, loss = 1326.69812554\n",
            "Iteration 4276, loss = 1326.14637535\n",
            "Iteration 4277, loss = 1325.65730209\n",
            "Iteration 4278, loss = 1325.47384691\n",
            "Iteration 4279, loss = 1325.05097798\n",
            "Iteration 4280, loss = 1324.66604840\n",
            "Iteration 4281, loss = 1323.97774666\n",
            "Iteration 4282, loss = 1323.78625847\n",
            "Iteration 4283, loss = 1323.46387439\n",
            "Iteration 4284, loss = 1323.43201816\n",
            "Iteration 4285, loss = 1322.45310844\n",
            "Iteration 4286, loss = 1322.31986552\n",
            "Iteration 4287, loss = 1321.82406087\n",
            "Iteration 4288, loss = 1321.31682578\n",
            "Iteration 4289, loss = 1320.75296498\n",
            "Iteration 4290, loss = 1320.39605284\n",
            "Iteration 4291, loss = 1319.92019129\n",
            "Iteration 4292, loss = 1319.51102833\n",
            "Iteration 4293, loss = 1319.06545368\n",
            "Iteration 4294, loss = 1318.65362424\n",
            "Iteration 4295, loss = 1318.41664058\n",
            "Iteration 4296, loss = 1318.21194706\n",
            "Iteration 4297, loss = 1317.27439104\n",
            "Iteration 4298, loss = 1317.32557096\n",
            "Iteration 4299, loss = 1316.71421223\n",
            "Iteration 4300, loss = 1315.96460208\n",
            "Iteration 4301, loss = 1315.98097597\n",
            "Iteration 4302, loss = 1315.80304695\n",
            "Iteration 4303, loss = 1315.00884537\n",
            "Iteration 4304, loss = 1314.60605691\n",
            "Iteration 4305, loss = 1314.08739806\n",
            "Iteration 4306, loss = 1313.86014054\n",
            "Iteration 4307, loss = 1313.49939169\n",
            "Iteration 4308, loss = 1312.97894600\n",
            "Iteration 4309, loss = 1312.35071831\n",
            "Iteration 4310, loss = 1312.58753569\n",
            "Iteration 4311, loss = 1311.74728791\n",
            "Iteration 4312, loss = 1311.54972826\n",
            "Iteration 4313, loss = 1310.80300012\n",
            "Iteration 4314, loss = 1310.65459121\n",
            "Iteration 4315, loss = 1310.13652089\n",
            "Iteration 4316, loss = 1309.65509550\n",
            "Iteration 4317, loss = 1309.36251488\n",
            "Iteration 4318, loss = 1308.80622727\n",
            "Iteration 4319, loss = 1308.21838231\n",
            "Iteration 4320, loss = 1308.06500300\n",
            "Iteration 4321, loss = 1308.11082279\n",
            "Iteration 4322, loss = 1307.59730768\n",
            "Iteration 4323, loss = 1307.33865213\n",
            "Iteration 4324, loss = 1306.72722709\n",
            "Iteration 4325, loss = 1306.13581043\n",
            "Iteration 4326, loss = 1305.61073697\n",
            "Iteration 4327, loss = 1305.63170368\n",
            "Iteration 4328, loss = 1304.51603945\n",
            "Iteration 4329, loss = 1304.74595997\n",
            "Iteration 4330, loss = 1304.05196653\n",
            "Iteration 4331, loss = 1303.99782912\n",
            "Iteration 4332, loss = 1303.48067793\n",
            "Iteration 4333, loss = 1302.95018691\n",
            "Iteration 4334, loss = 1302.59564430\n",
            "Iteration 4335, loss = 1302.30536160\n",
            "Iteration 4336, loss = 1301.50835500\n",
            "Iteration 4337, loss = 1301.11201371\n",
            "Iteration 4338, loss = 1300.71934723\n",
            "Iteration 4339, loss = 1300.55524060\n",
            "Iteration 4340, loss = 1300.21298746\n",
            "Iteration 4341, loss = 1299.91425193\n",
            "Iteration 4342, loss = 1299.97637858\n",
            "Iteration 4343, loss = 1299.02738622\n",
            "Iteration 4344, loss = 1298.28210908\n",
            "Iteration 4345, loss = 1298.17601651\n",
            "Iteration 4346, loss = 1297.46987861\n",
            "Iteration 4347, loss = 1297.18035382\n",
            "Iteration 4348, loss = 1296.64339904\n",
            "Iteration 4349, loss = 1298.32380318\n",
            "Iteration 4350, loss = 1295.74736248\n",
            "Iteration 4351, loss = 1295.71974720\n",
            "Iteration 4352, loss = 1295.33269638\n",
            "Iteration 4353, loss = 1294.82317341\n",
            "Iteration 4354, loss = 1294.11010755\n",
            "Iteration 4355, loss = 1294.82228037\n",
            "Iteration 4356, loss = 1293.52382899\n",
            "Iteration 4357, loss = 1292.84054158\n",
            "Iteration 4358, loss = 1292.89870966\n",
            "Iteration 4359, loss = 1292.42328813\n",
            "Iteration 4360, loss = 1291.93504178\n",
            "Iteration 4361, loss = 1291.60727048\n",
            "Iteration 4362, loss = 1290.79363065\n",
            "Iteration 4363, loss = 1290.82286472\n",
            "Iteration 4364, loss = 1290.48077136\n",
            "Iteration 4365, loss = 1289.85378379\n",
            "Iteration 4366, loss = 1289.37478998\n",
            "Iteration 4367, loss = 1288.83706253\n",
            "Iteration 4368, loss = 1288.60793921\n",
            "Iteration 4369, loss = 1287.86815463\n",
            "Iteration 4370, loss = 1287.50529320\n",
            "Iteration 4371, loss = 1287.11356701\n",
            "Iteration 4372, loss = 1286.92837903\n",
            "Iteration 4373, loss = 1286.52434302\n",
            "Iteration 4374, loss = 1286.39744756\n",
            "Iteration 4375, loss = 1285.78182636\n",
            "Iteration 4376, loss = 1285.31588363\n",
            "Iteration 4377, loss = 1285.45387271\n",
            "Iteration 4378, loss = 1284.49935464\n",
            "Iteration 4379, loss = 1284.20966246\n",
            "Iteration 4380, loss = 1283.67667611\n",
            "Iteration 4381, loss = 1283.17836760\n",
            "Iteration 4382, loss = 1283.23161379\n",
            "Iteration 4383, loss = 1282.67696632\n",
            "Iteration 4384, loss = 1282.56565596\n",
            "Iteration 4385, loss = 1281.81218291\n",
            "Iteration 4386, loss = 1281.60431438\n",
            "Iteration 4387, loss = 1281.22658020\n",
            "Iteration 4388, loss = 1280.51785670\n",
            "Iteration 4389, loss = 1280.11040681\n",
            "Iteration 4390, loss = 1279.86341754\n",
            "Iteration 4391, loss = 1279.24066352\n",
            "Iteration 4392, loss = 1279.04509196\n",
            "Iteration 4393, loss = 1279.14205791\n",
            "Iteration 4394, loss = 1278.68491529\n",
            "Iteration 4395, loss = 1278.41037537\n",
            "Iteration 4396, loss = 1277.32037922\n",
            "Iteration 4397, loss = 1276.65254393\n",
            "Iteration 4398, loss = 1276.26834867\n",
            "Iteration 4399, loss = 1275.92081310\n",
            "Iteration 4400, loss = 1276.31398422\n",
            "Iteration 4401, loss = 1275.18148541\n",
            "Iteration 4402, loss = 1275.15853472\n",
            "Iteration 4403, loss = 1274.56686120\n",
            "Iteration 4404, loss = 1273.88340975\n",
            "Iteration 4405, loss = 1274.05849247\n",
            "Iteration 4406, loss = 1272.93840612\n",
            "Iteration 4407, loss = 1272.98225876\n",
            "Iteration 4408, loss = 1273.10024243\n",
            "Iteration 4409, loss = 1272.46869519\n",
            "Iteration 4410, loss = 1272.07935830\n",
            "Iteration 4411, loss = 1271.66426420\n",
            "Iteration 4412, loss = 1271.55929772\n",
            "Iteration 4413, loss = 1270.51334470\n",
            "Iteration 4414, loss = 1270.11670278\n",
            "Iteration 4415, loss = 1269.80909124\n",
            "Iteration 4416, loss = 1269.61904362\n",
            "Iteration 4417, loss = 1268.93557298\n",
            "Iteration 4418, loss = 1269.26283961\n",
            "Iteration 4419, loss = 1268.56356156\n",
            "Iteration 4420, loss = 1267.96179121\n",
            "Iteration 4421, loss = 1268.49092306\n",
            "Iteration 4422, loss = 1267.06931903\n",
            "Iteration 4423, loss = 1267.07960507\n",
            "Iteration 4424, loss = 1266.66434574\n",
            "Iteration 4425, loss = 1266.09893710\n",
            "Iteration 4426, loss = 1265.69658569\n",
            "Iteration 4427, loss = 1265.84628878\n",
            "Iteration 4428, loss = 1265.08121441\n",
            "Iteration 4429, loss = 1264.60557566\n",
            "Iteration 4430, loss = 1264.66826898\n",
            "Iteration 4431, loss = 1263.85850645\n",
            "Iteration 4432, loss = 1263.12638176\n",
            "Iteration 4433, loss = 1262.64928906\n",
            "Iteration 4434, loss = 1262.44603118\n",
            "Iteration 4435, loss = 1263.28838369\n",
            "Iteration 4436, loss = 1262.33609649\n",
            "Iteration 4437, loss = 1261.74027889\n",
            "Iteration 4438, loss = 1261.82084837\n",
            "Iteration 4439, loss = 1260.77728198\n",
            "Iteration 4440, loss = 1260.48068705\n",
            "Iteration 4441, loss = 1259.75414473\n",
            "Iteration 4442, loss = 1259.52673967\n",
            "Iteration 4443, loss = 1258.84822524\n",
            "Iteration 4444, loss = 1258.64189674\n",
            "Iteration 4445, loss = 1258.53761831\n",
            "Iteration 4446, loss = 1258.09807093\n",
            "Iteration 4447, loss = 1257.54287781\n",
            "Iteration 4448, loss = 1257.60670512\n",
            "Iteration 4449, loss = 1257.34070799\n",
            "Iteration 4450, loss = 1256.38332434\n",
            "Iteration 4451, loss = 1255.75377042\n",
            "Iteration 4452, loss = 1255.54102747\n",
            "Iteration 4453, loss = 1256.00678298\n",
            "Iteration 4454, loss = 1255.86365097\n",
            "Iteration 4455, loss = 1254.76189855\n",
            "Iteration 4456, loss = 1254.19696302\n",
            "Iteration 4457, loss = 1253.69566993\n",
            "Iteration 4458, loss = 1253.35773348\n",
            "Iteration 4459, loss = 1252.97805545\n",
            "Iteration 4460, loss = 1253.38492208\n",
            "Iteration 4461, loss = 1252.54296410\n",
            "Iteration 4462, loss = 1251.74983820\n",
            "Iteration 4463, loss = 1251.46748441\n",
            "Iteration 4464, loss = 1251.10257186\n",
            "Iteration 4465, loss = 1250.60035541\n",
            "Iteration 4466, loss = 1250.21474185\n",
            "Iteration 4467, loss = 1250.36630226\n",
            "Iteration 4468, loss = 1249.43000189\n",
            "Iteration 4469, loss = 1249.27088067\n",
            "Iteration 4470, loss = 1248.87083779\n",
            "Iteration 4471, loss = 1248.98430863\n",
            "Iteration 4472, loss = 1248.50190644\n",
            "Iteration 4473, loss = 1247.63058035\n",
            "Iteration 4474, loss = 1247.38187693\n",
            "Iteration 4475, loss = 1246.71653087\n",
            "Iteration 4476, loss = 1246.97609683\n",
            "Iteration 4477, loss = 1246.55171601\n",
            "Iteration 4478, loss = 1245.76535760\n",
            "Iteration 4479, loss = 1245.47934442\n",
            "Iteration 4480, loss = 1245.09681963\n",
            "Iteration 4481, loss = 1245.19632279\n",
            "Iteration 4482, loss = 1244.52653355\n",
            "Iteration 4483, loss = 1243.76881422\n",
            "Iteration 4484, loss = 1243.76388167\n",
            "Iteration 4485, loss = 1242.91796252\n",
            "Iteration 4486, loss = 1242.86878616\n",
            "Iteration 4487, loss = 1242.68297877\n",
            "Iteration 4488, loss = 1241.86516635\n",
            "Iteration 4489, loss = 1241.54747530\n",
            "Iteration 4490, loss = 1241.38193611\n",
            "Iteration 4491, loss = 1240.96061685\n",
            "Iteration 4492, loss = 1240.57578590\n",
            "Iteration 4493, loss = 1240.73336286\n",
            "Iteration 4494, loss = 1239.76148355\n",
            "Iteration 4495, loss = 1239.41966782\n",
            "Iteration 4496, loss = 1239.38841250\n",
            "Iteration 4497, loss = 1238.92744410\n",
            "Iteration 4498, loss = 1238.51931732\n",
            "Iteration 4499, loss = 1237.73004307\n",
            "Iteration 4500, loss = 1237.33162958\n",
            "Iteration 4501, loss = 1237.07674993\n",
            "Iteration 4502, loss = 1236.78938982\n",
            "Iteration 4503, loss = 1236.41765924\n",
            "Iteration 4504, loss = 1235.96476372\n",
            "Iteration 4505, loss = 1235.57740381\n",
            "Iteration 4506, loss = 1235.43933677\n",
            "Iteration 4507, loss = 1234.93509812\n",
            "Iteration 4508, loss = 1234.73652453\n",
            "Iteration 4509, loss = 1234.13645556\n",
            "Iteration 4510, loss = 1233.54955559\n",
            "Iteration 4511, loss = 1233.70831642\n",
            "Iteration 4512, loss = 1233.22837426\n",
            "Iteration 4513, loss = 1233.57696622\n",
            "Iteration 4514, loss = 1232.45711485\n",
            "Iteration 4515, loss = 1232.34059383\n",
            "Iteration 4516, loss = 1231.63788352\n",
            "Iteration 4517, loss = 1231.17989211\n",
            "Iteration 4518, loss = 1230.47264400\n",
            "Iteration 4519, loss = 1230.07917684\n",
            "Iteration 4520, loss = 1230.67836146\n",
            "Iteration 4521, loss = 1230.65291113\n",
            "Iteration 4522, loss = 1229.95243580\n",
            "Iteration 4523, loss = 1229.41586839\n",
            "Iteration 4524, loss = 1228.66196807\n",
            "Iteration 4525, loss = 1228.43256514\n",
            "Iteration 4526, loss = 1228.17994683\n",
            "Iteration 4527, loss = 1227.42191997\n",
            "Iteration 4528, loss = 1227.03163253\n",
            "Iteration 4529, loss = 1227.72715629\n",
            "Iteration 4530, loss = 1227.07411761\n",
            "Iteration 4531, loss = 1226.12794973\n",
            "Iteration 4532, loss = 1225.78573887\n",
            "Iteration 4533, loss = 1225.67571848\n",
            "Iteration 4534, loss = 1225.39520113\n",
            "Iteration 4535, loss = 1224.66617024\n",
            "Iteration 4536, loss = 1224.42458141\n",
            "Iteration 4537, loss = 1223.97792428\n",
            "Iteration 4538, loss = 1223.73547267\n",
            "Iteration 4539, loss = 1223.18065849\n",
            "Iteration 4540, loss = 1222.76262522\n",
            "Iteration 4541, loss = 1222.43868344\n",
            "Iteration 4542, loss = 1222.51678709\n",
            "Iteration 4543, loss = 1221.77057201\n",
            "Iteration 4544, loss = 1221.52782827\n",
            "Iteration 4545, loss = 1221.29285886\n",
            "Iteration 4546, loss = 1220.79858985\n",
            "Iteration 4547, loss = 1220.58411419\n",
            "Iteration 4548, loss = 1220.11140915\n",
            "Iteration 4549, loss = 1219.94063660\n",
            "Iteration 4550, loss = 1219.58671982\n",
            "Iteration 4551, loss = 1219.57681614\n",
            "Iteration 4552, loss = 1218.72353562\n",
            "Iteration 4553, loss = 1218.23820356\n",
            "Iteration 4554, loss = 1217.87643619\n",
            "Iteration 4555, loss = 1217.43137531\n",
            "Iteration 4556, loss = 1217.15699743\n",
            "Iteration 4557, loss = 1216.86496250\n",
            "Iteration 4558, loss = 1216.36669893\n",
            "Iteration 4559, loss = 1216.11844007\n",
            "Iteration 4560, loss = 1216.02839617\n",
            "Iteration 4561, loss = 1215.33933383\n",
            "Iteration 4562, loss = 1215.36421455\n",
            "Iteration 4563, loss = 1215.22513732\n",
            "Iteration 4564, loss = 1214.71682929\n",
            "Iteration 4565, loss = 1214.53960565\n",
            "Iteration 4566, loss = 1215.03272613\n",
            "Iteration 4567, loss = 1212.95040396\n",
            "Iteration 4568, loss = 1213.19667293\n",
            "Iteration 4569, loss = 1213.00964751\n",
            "Iteration 4570, loss = 1212.20668555\n",
            "Iteration 4571, loss = 1212.71233829\n",
            "Iteration 4572, loss = 1211.70750546\n",
            "Iteration 4573, loss = 1211.04717851\n",
            "Iteration 4574, loss = 1210.78905622\n",
            "Iteration 4575, loss = 1210.33618246\n",
            "Iteration 4576, loss = 1209.82094158\n",
            "Iteration 4577, loss = 1209.26972253\n",
            "Iteration 4578, loss = 1209.03670395\n",
            "Iteration 4579, loss = 1208.78416670\n",
            "Iteration 4580, loss = 1208.69595364\n",
            "Iteration 4581, loss = 1208.25348189\n",
            "Iteration 4582, loss = 1207.94814111\n",
            "Iteration 4583, loss = 1207.22318980\n",
            "Iteration 4584, loss = 1207.85667261\n",
            "Iteration 4585, loss = 1206.84128657\n",
            "Iteration 4586, loss = 1206.10747866\n",
            "Iteration 4587, loss = 1206.31704845\n",
            "Iteration 4588, loss = 1205.48301791\n",
            "Iteration 4589, loss = 1205.02216350\n",
            "Iteration 4590, loss = 1204.65336149\n",
            "Iteration 4591, loss = 1204.63949909\n",
            "Iteration 4592, loss = 1204.56075602\n",
            "Iteration 4593, loss = 1203.67374993\n",
            "Iteration 4594, loss = 1203.44437724\n",
            "Iteration 4595, loss = 1203.25958743\n",
            "Iteration 4596, loss = 1202.48197110\n",
            "Iteration 4597, loss = 1202.78086028\n",
            "Iteration 4598, loss = 1201.94807592\n",
            "Iteration 4599, loss = 1201.57540125\n",
            "Iteration 4600, loss = 1200.96975896\n",
            "Iteration 4601, loss = 1200.87030651\n",
            "Iteration 4602, loss = 1200.62167274\n",
            "Iteration 4603, loss = 1199.96008484\n",
            "Iteration 4604, loss = 1199.57124020\n",
            "Iteration 4605, loss = 1199.35911244\n",
            "Iteration 4606, loss = 1198.92988853\n",
            "Iteration 4607, loss = 1198.44600166\n",
            "Iteration 4608, loss = 1198.20543498\n",
            "Iteration 4609, loss = 1198.22058240\n",
            "Iteration 4610, loss = 1197.53637466\n",
            "Iteration 4611, loss = 1197.14645972\n",
            "Iteration 4612, loss = 1197.34704017\n",
            "Iteration 4613, loss = 1196.35973677\n",
            "Iteration 4614, loss = 1196.21255426\n",
            "Iteration 4615, loss = 1195.76832745\n",
            "Iteration 4616, loss = 1195.37548079\n",
            "Iteration 4617, loss = 1194.96900469\n",
            "Iteration 4618, loss = 1194.64117453\n",
            "Iteration 4619, loss = 1194.90440835\n",
            "Iteration 4620, loss = 1194.03858960\n",
            "Iteration 4621, loss = 1193.99983282\n",
            "Iteration 4622, loss = 1193.52965893\n",
            "Iteration 4623, loss = 1192.92693834\n",
            "Iteration 4624, loss = 1192.49930118\n",
            "Iteration 4625, loss = 1192.57564706\n",
            "Iteration 4626, loss = 1192.08320289\n",
            "Iteration 4627, loss = 1191.42030959\n",
            "Iteration 4628, loss = 1191.55038032\n",
            "Iteration 4629, loss = 1190.85455080\n",
            "Iteration 4630, loss = 1190.19608139\n",
            "Iteration 4631, loss = 1189.87897132\n",
            "Iteration 4632, loss = 1189.63000758\n",
            "Iteration 4633, loss = 1189.84902033\n",
            "Iteration 4634, loss = 1189.19274689\n",
            "Iteration 4635, loss = 1188.83143386\n",
            "Iteration 4636, loss = 1188.41145678\n",
            "Iteration 4637, loss = 1188.40119402\n",
            "Iteration 4638, loss = 1187.91079273\n",
            "Iteration 4639, loss = 1187.41381453\n",
            "Iteration 4640, loss = 1186.86910994\n",
            "Iteration 4641, loss = 1187.00751487\n",
            "Iteration 4642, loss = 1186.63125562\n",
            "Iteration 4643, loss = 1186.12309676\n",
            "Iteration 4644, loss = 1185.59114953\n",
            "Iteration 4645, loss = 1185.10298635\n",
            "Iteration 4646, loss = 1184.95543375\n",
            "Iteration 4647, loss = 1184.75677868\n",
            "Iteration 4648, loss = 1184.26396292\n",
            "Iteration 4649, loss = 1184.26440269\n",
            "Iteration 4650, loss = 1183.63994616\n",
            "Iteration 4651, loss = 1183.28705177\n",
            "Iteration 4652, loss = 1183.01472540\n",
            "Iteration 4653, loss = 1182.90809411\n",
            "Iteration 4654, loss = 1182.29854161\n",
            "Iteration 4655, loss = 1182.03460631\n",
            "Iteration 4656, loss = 1181.62060442\n",
            "Iteration 4657, loss = 1181.23644558\n",
            "Iteration 4658, loss = 1180.71960844\n",
            "Iteration 4659, loss = 1180.68982260\n",
            "Iteration 4660, loss = 1179.84363343\n",
            "Iteration 4661, loss = 1180.12906691\n",
            "Iteration 4662, loss = 1179.49551165\n",
            "Iteration 4663, loss = 1179.36851665\n",
            "Iteration 4664, loss = 1179.06046176\n",
            "Iteration 4665, loss = 1179.09112665\n",
            "Iteration 4666, loss = 1178.25796307\n",
            "Iteration 4667, loss = 1178.90213344\n",
            "Iteration 4668, loss = 1177.53572108\n",
            "Iteration 4669, loss = 1177.06095079\n",
            "Iteration 4670, loss = 1176.72415116\n",
            "Iteration 4671, loss = 1176.34483723\n",
            "Iteration 4672, loss = 1175.92954737\n",
            "Iteration 4673, loss = 1175.76783073\n",
            "Iteration 4674, loss = 1175.55692114\n",
            "Iteration 4675, loss = 1175.30092219\n",
            "Iteration 4676, loss = 1174.64799480\n",
            "Iteration 4677, loss = 1174.36325674\n",
            "Iteration 4678, loss = 1173.80136720\n",
            "Iteration 4679, loss = 1173.75300973\n",
            "Iteration 4680, loss = 1173.30573273\n",
            "Iteration 4681, loss = 1173.52548692\n",
            "Iteration 4682, loss = 1173.20837214\n",
            "Iteration 4683, loss = 1172.37360688\n",
            "Iteration 4684, loss = 1172.60203221\n",
            "Iteration 4685, loss = 1172.26959730\n",
            "Iteration 4686, loss = 1171.64850337\n",
            "Iteration 4687, loss = 1171.46616732\n",
            "Iteration 4688, loss = 1170.71383890\n",
            "Iteration 4689, loss = 1170.30830280\n",
            "Iteration 4690, loss = 1170.24596515\n",
            "Iteration 4691, loss = 1169.87183909\n",
            "Iteration 4692, loss = 1169.61664307\n",
            "Iteration 4693, loss = 1169.27727520\n",
            "Iteration 4694, loss = 1168.59485866\n",
            "Iteration 4695, loss = 1168.43561234\n",
            "Iteration 4696, loss = 1167.91198218\n",
            "Iteration 4697, loss = 1167.63561900\n",
            "Iteration 4698, loss = 1167.31831402\n",
            "Iteration 4699, loss = 1167.08109563\n",
            "Iteration 4700, loss = 1166.91318823\n",
            "Iteration 4701, loss = 1166.02065086\n",
            "Iteration 4702, loss = 1165.89045868\n",
            "Iteration 4703, loss = 1165.44369810\n",
            "Iteration 4704, loss = 1164.95371667\n",
            "Iteration 4705, loss = 1164.88258555\n",
            "Iteration 4706, loss = 1164.53100749\n",
            "Iteration 4707, loss = 1164.57009411\n",
            "Iteration 4708, loss = 1164.70047153\n",
            "Iteration 4709, loss = 1163.92675132\n",
            "Iteration 4710, loss = 1163.60671582\n",
            "Iteration 4711, loss = 1162.98886344\n",
            "Iteration 4712, loss = 1163.07551102\n",
            "Iteration 4713, loss = 1162.28068810\n",
            "Iteration 4714, loss = 1162.00199484\n",
            "Iteration 4715, loss = 1161.52023805\n",
            "Iteration 4716, loss = 1161.66039378\n",
            "Iteration 4717, loss = 1160.94514557\n",
            "Iteration 4718, loss = 1160.87835732\n",
            "Iteration 4719, loss = 1160.10553557\n",
            "Iteration 4720, loss = 1159.87834052\n",
            "Iteration 4721, loss = 1159.72681732\n",
            "Iteration 4722, loss = 1159.23036208\n",
            "Iteration 4723, loss = 1159.48896598\n",
            "Iteration 4724, loss = 1158.78705162\n",
            "Iteration 4725, loss = 1158.35662345\n",
            "Iteration 4726, loss = 1158.82712164\n",
            "Iteration 4727, loss = 1157.98418206\n",
            "Iteration 4728, loss = 1157.24818204\n",
            "Iteration 4729, loss = 1157.02022367\n",
            "Iteration 4730, loss = 1156.67461096\n",
            "Iteration 4731, loss = 1156.32402177\n",
            "Iteration 4732, loss = 1156.02080971\n",
            "Iteration 4733, loss = 1156.38255916\n",
            "Iteration 4734, loss = 1155.36733279\n",
            "Iteration 4735, loss = 1155.03026790\n",
            "Iteration 4736, loss = 1155.02961389\n",
            "Iteration 4737, loss = 1154.98564173\n",
            "Iteration 4738, loss = 1154.43758203\n",
            "Iteration 4739, loss = 1153.74078963\n",
            "Iteration 4740, loss = 1153.15243401\n",
            "Iteration 4741, loss = 1153.43992567\n",
            "Iteration 4742, loss = 1153.05142830\n",
            "Iteration 4743, loss = 1152.64863718\n",
            "Iteration 4744, loss = 1151.81948880\n",
            "Iteration 4745, loss = 1151.51802276\n",
            "Iteration 4746, loss = 1151.29089099\n",
            "Iteration 4747, loss = 1151.07981806\n",
            "Iteration 4748, loss = 1151.17104622\n",
            "Iteration 4749, loss = 1150.23294779\n",
            "Iteration 4750, loss = 1150.44355785\n",
            "Iteration 4751, loss = 1149.40241997\n",
            "Iteration 4752, loss = 1149.77284441\n",
            "Iteration 4753, loss = 1149.16937059\n",
            "Iteration 4754, loss = 1148.82706728\n",
            "Iteration 4755, loss = 1148.52303926\n",
            "Iteration 4756, loss = 1149.10471286\n",
            "Iteration 4757, loss = 1147.82167756\n",
            "Iteration 4758, loss = 1147.57045138\n",
            "Iteration 4759, loss = 1147.56589398\n",
            "Iteration 4760, loss = 1146.90203765\n",
            "Iteration 4761, loss = 1146.44849281\n",
            "Iteration 4762, loss = 1146.16389162\n",
            "Iteration 4763, loss = 1145.91773733\n",
            "Iteration 4764, loss = 1145.66160450\n",
            "Iteration 4765, loss = 1145.20114531\n",
            "Iteration 4766, loss = 1145.14320153\n",
            "Iteration 4767, loss = 1144.59140479\n",
            "Iteration 4768, loss = 1144.50376262\n",
            "Iteration 4769, loss = 1144.13111409\n",
            "Iteration 4770, loss = 1143.62064878\n",
            "Iteration 4771, loss = 1143.09918922\n",
            "Iteration 4772, loss = 1143.13329660\n",
            "Iteration 4773, loss = 1142.67999090\n",
            "Iteration 4774, loss = 1142.70488244\n",
            "Iteration 4775, loss = 1142.55691990\n",
            "Iteration 4776, loss = 1141.64502684\n",
            "Iteration 4777, loss = 1141.29430405\n",
            "Iteration 4778, loss = 1141.12283228\n",
            "Iteration 4779, loss = 1140.83065363\n",
            "Iteration 4780, loss = 1140.58277575\n",
            "Iteration 4781, loss = 1140.26446461\n",
            "Iteration 4782, loss = 1139.70188857\n",
            "Iteration 4783, loss = 1140.05603699\n",
            "Iteration 4784, loss = 1139.28415245\n",
            "Iteration 4785, loss = 1139.01073803\n",
            "Iteration 4786, loss = 1138.99430667\n",
            "Iteration 4787, loss = 1138.30024598\n",
            "Iteration 4788, loss = 1138.29009142\n",
            "Iteration 4789, loss = 1138.96931349\n",
            "Iteration 4790, loss = 1137.57758292\n",
            "Iteration 4791, loss = 1137.25381300\n",
            "Iteration 4792, loss = 1136.71806738\n",
            "Iteration 4793, loss = 1136.66514068\n",
            "Iteration 4794, loss = 1135.81148248\n",
            "Iteration 4795, loss = 1135.80098387\n",
            "Iteration 4796, loss = 1135.50641252\n",
            "Iteration 4797, loss = 1134.98976332\n",
            "Iteration 4798, loss = 1134.79706080\n",
            "Iteration 4799, loss = 1134.43089241\n",
            "Iteration 4800, loss = 1133.99563961\n",
            "Iteration 4801, loss = 1134.20676072\n",
            "Iteration 4802, loss = 1133.72719555\n",
            "Iteration 4803, loss = 1132.98931207\n",
            "Iteration 4804, loss = 1133.39381407\n",
            "Iteration 4805, loss = 1132.86402277\n",
            "Iteration 4806, loss = 1132.40736074\n",
            "Iteration 4807, loss = 1132.50518048\n",
            "Iteration 4808, loss = 1131.49189550\n",
            "Iteration 4809, loss = 1131.40969952\n",
            "Iteration 4810, loss = 1131.99508449\n",
            "Iteration 4811, loss = 1131.23023623\n",
            "Iteration 4812, loss = 1130.33403086\n",
            "Iteration 4813, loss = 1130.15382087\n",
            "Iteration 4814, loss = 1129.68786943\n",
            "Iteration 4815, loss = 1129.59785282\n",
            "Iteration 4816, loss = 1128.92906321\n",
            "Iteration 4817, loss = 1128.64670326\n",
            "Iteration 4818, loss = 1128.59581520\n",
            "Iteration 4819, loss = 1128.81991210\n",
            "Iteration 4820, loss = 1127.91923556\n",
            "Iteration 4821, loss = 1127.37731418\n",
            "Iteration 4822, loss = 1127.14897776\n",
            "Iteration 4823, loss = 1127.17293202\n",
            "Iteration 4824, loss = 1128.04756504\n",
            "Iteration 4825, loss = 1126.31767990\n",
            "Iteration 4826, loss = 1125.88782374\n",
            "Iteration 4827, loss = 1125.69649268\n",
            "Iteration 4828, loss = 1125.33142499\n",
            "Iteration 4829, loss = 1125.04579465\n",
            "Iteration 4830, loss = 1124.61647533\n",
            "Iteration 4831, loss = 1124.30475619\n",
            "Iteration 4832, loss = 1124.28407900\n",
            "Iteration 4833, loss = 1124.05175139\n",
            "Iteration 4834, loss = 1123.29812541\n",
            "Iteration 4835, loss = 1123.22665877\n",
            "Iteration 4836, loss = 1122.77739681\n",
            "Iteration 4837, loss = 1123.57320313\n",
            "Iteration 4838, loss = 1122.10732416\n",
            "Iteration 4839, loss = 1121.92782264\n",
            "Iteration 4840, loss = 1121.61329898\n",
            "Iteration 4841, loss = 1121.17070828\n",
            "Iteration 4842, loss = 1120.78438813\n",
            "Iteration 4843, loss = 1120.60343235\n",
            "Iteration 4844, loss = 1120.88592754\n",
            "Iteration 4845, loss = 1120.44469534\n",
            "Iteration 4846, loss = 1119.60381092\n",
            "Iteration 4847, loss = 1120.04582193\n",
            "Iteration 4848, loss = 1119.68105531\n",
            "Iteration 4849, loss = 1120.03002292\n",
            "Iteration 4850, loss = 1118.89582309\n",
            "Iteration 4851, loss = 1118.01123624\n",
            "Iteration 4852, loss = 1118.70783889\n",
            "Iteration 4853, loss = 1118.45268367\n",
            "Iteration 4854, loss = 1117.84884951\n",
            "Iteration 4855, loss = 1117.67126388\n",
            "Iteration 4856, loss = 1116.60725963\n",
            "Iteration 4857, loss = 1116.36576439\n",
            "Iteration 4858, loss = 1115.99992326\n",
            "Iteration 4859, loss = 1115.74716805\n",
            "Iteration 4860, loss = 1115.60406583\n",
            "Iteration 4861, loss = 1114.90520699\n",
            "Iteration 4862, loss = 1114.84442137\n",
            "Iteration 4863, loss = 1114.77847432\n",
            "Iteration 4864, loss = 1114.22626064\n",
            "Iteration 4865, loss = 1114.16163685\n",
            "Iteration 4866, loss = 1113.95786634\n",
            "Iteration 4867, loss = 1113.35717669\n",
            "Iteration 4868, loss = 1112.84880701\n",
            "Iteration 4869, loss = 1113.54094630\n",
            "Iteration 4870, loss = 1112.45080654\n",
            "Iteration 4871, loss = 1112.05840185\n",
            "Iteration 4872, loss = 1111.82300260\n",
            "Iteration 4873, loss = 1111.51286871\n",
            "Iteration 4874, loss = 1111.40242495\n",
            "Iteration 4875, loss = 1110.84642360\n",
            "Iteration 4876, loss = 1110.67042116\n",
            "Iteration 4877, loss = 1110.54607616\n",
            "Iteration 4878, loss = 1110.03707044\n",
            "Iteration 4879, loss = 1109.83342259\n",
            "Iteration 4880, loss = 1109.33034730\n",
            "Iteration 4881, loss = 1109.47663685\n",
            "Iteration 4882, loss = 1108.70082089\n",
            "Iteration 4883, loss = 1108.61405706\n",
            "Iteration 4884, loss = 1108.23060748\n",
            "Iteration 4885, loss = 1108.00778158\n",
            "Iteration 4886, loss = 1107.43021962\n",
            "Iteration 4887, loss = 1107.05079586\n",
            "Iteration 4888, loss = 1107.67738342\n",
            "Iteration 4889, loss = 1106.80845441\n",
            "Iteration 4890, loss = 1106.74520034\n",
            "Iteration 4891, loss = 1106.51532217\n",
            "Iteration 4892, loss = 1106.14882778\n",
            "Iteration 4893, loss = 1105.66203262\n",
            "Iteration 4894, loss = 1104.99809495\n",
            "Iteration 4895, loss = 1105.64845723\n",
            "Iteration 4896, loss = 1105.34905438\n",
            "Iteration 4897, loss = 1104.35280315\n",
            "Iteration 4898, loss = 1104.50053345\n",
            "Iteration 4899, loss = 1103.62037233\n",
            "Iteration 4900, loss = 1103.43971503\n",
            "Iteration 4901, loss = 1103.16019020\n",
            "Iteration 4902, loss = 1102.94484370\n",
            "Iteration 4903, loss = 1102.60112921\n",
            "Iteration 4904, loss = 1102.36597705\n",
            "Iteration 4905, loss = 1101.77724780\n",
            "Iteration 4906, loss = 1101.51304413\n",
            "Iteration 4907, loss = 1101.23795160\n",
            "Iteration 4908, loss = 1101.11352166\n",
            "Iteration 4909, loss = 1100.91686140\n",
            "Iteration 4910, loss = 1100.42191553\n",
            "Iteration 4911, loss = 1100.34893461\n",
            "Iteration 4912, loss = 1100.03100993\n",
            "Iteration 4913, loss = 1099.88459374\n",
            "Iteration 4914, loss = 1099.91350859\n",
            "Iteration 4915, loss = 1099.06410824\n",
            "Iteration 4916, loss = 1098.92564832\n",
            "Iteration 4917, loss = 1098.39122047\n",
            "Iteration 4918, loss = 1098.21881527\n",
            "Iteration 4919, loss = 1097.66189541\n",
            "Iteration 4920, loss = 1097.50481597\n",
            "Iteration 4921, loss = 1097.22714038\n",
            "Iteration 4922, loss = 1096.75981020\n",
            "Iteration 4923, loss = 1096.57628972\n",
            "Iteration 4924, loss = 1096.35361052\n",
            "Iteration 4925, loss = 1095.87591307\n",
            "Iteration 4926, loss = 1095.78857055\n",
            "Iteration 4927, loss = 1095.13451776\n",
            "Iteration 4928, loss = 1094.86627967\n",
            "Iteration 4929, loss = 1094.94479492\n",
            "Iteration 4930, loss = 1095.31580197\n",
            "Iteration 4931, loss = 1094.10632442\n",
            "Iteration 4932, loss = 1094.51230837\n",
            "Iteration 4933, loss = 1094.05726527\n",
            "Iteration 4934, loss = 1093.83063517\n",
            "Iteration 4935, loss = 1093.06201670\n",
            "Iteration 4936, loss = 1092.93955313\n",
            "Iteration 4937, loss = 1092.58376153\n",
            "Iteration 4938, loss = 1092.45245538\n",
            "Iteration 4939, loss = 1091.72135221\n",
            "Iteration 4940, loss = 1091.67536024\n",
            "Iteration 4941, loss = 1091.18032933\n",
            "Iteration 4942, loss = 1091.07633547\n",
            "Iteration 4943, loss = 1090.70829034\n",
            "Iteration 4944, loss = 1090.70884674\n",
            "Iteration 4945, loss = 1089.68106910\n",
            "Iteration 4946, loss = 1090.50241483\n",
            "Iteration 4947, loss = 1089.84271805\n",
            "Iteration 4948, loss = 1089.43685170\n",
            "Iteration 4949, loss = 1090.06876184\n",
            "Iteration 4950, loss = 1088.81132788\n",
            "Iteration 4951, loss = 1087.96756250\n",
            "Iteration 4952, loss = 1087.95759369\n",
            "Iteration 4953, loss = 1087.94251417\n",
            "Iteration 4954, loss = 1087.77947940\n",
            "Iteration 4955, loss = 1087.09925056\n",
            "Iteration 4956, loss = 1087.34772180\n",
            "Iteration 4957, loss = 1086.95633919\n",
            "Iteration 4958, loss = 1086.35227189\n",
            "Iteration 4959, loss = 1086.12754753\n",
            "Iteration 4960, loss = 1085.49363112\n",
            "Iteration 4961, loss = 1085.97086296\n",
            "Iteration 4962, loss = 1085.47238991\n",
            "Iteration 4963, loss = 1084.83404172\n",
            "Iteration 4964, loss = 1084.43090114\n",
            "Iteration 4965, loss = 1084.78428534\n",
            "Iteration 4966, loss = 1084.38096852\n",
            "Iteration 4967, loss = 1084.83521556\n",
            "Iteration 4968, loss = 1083.23431512\n",
            "Iteration 4969, loss = 1083.02903499\n",
            "Iteration 4970, loss = 1083.25957026\n",
            "Iteration 4971, loss = 1082.63185221\n",
            "Iteration 4972, loss = 1082.00592614\n",
            "Iteration 4973, loss = 1081.75125644\n",
            "Iteration 4974, loss = 1081.47053389\n",
            "Iteration 4975, loss = 1081.30229474\n",
            "Iteration 4976, loss = 1081.00232901\n",
            "Iteration 4977, loss = 1080.59637699\n",
            "Iteration 4978, loss = 1080.50184256\n",
            "Iteration 4979, loss = 1080.85150220\n",
            "Iteration 4980, loss = 1079.56491375\n",
            "Iteration 4981, loss = 1079.40698450\n",
            "Iteration 4982, loss = 1079.33722837\n",
            "Iteration 4983, loss = 1078.87058528\n",
            "Iteration 4984, loss = 1078.90168310\n",
            "Iteration 4985, loss = 1078.55290596\n",
            "Iteration 4986, loss = 1078.11567119\n",
            "Iteration 4987, loss = 1077.90504739\n",
            "Iteration 4988, loss = 1077.55603187\n",
            "Iteration 4989, loss = 1077.40902845\n",
            "Iteration 4990, loss = 1076.98520523\n",
            "Iteration 4991, loss = 1077.24527823\n",
            "Iteration 4992, loss = 1077.07474459\n",
            "Iteration 4993, loss = 1076.47748644\n",
            "Iteration 4994, loss = 1075.84942053\n",
            "Iteration 4995, loss = 1075.87044010\n",
            "Iteration 4996, loss = 1075.00458545\n",
            "Iteration 4997, loss = 1075.77437465\n",
            "Iteration 4998, loss = 1074.82576078\n",
            "Iteration 4999, loss = 1074.32982052\n",
            "Iteration 5000, loss = 1074.29045120\n",
            "Iteration 5001, loss = 1073.77152491\n",
            "Iteration 5002, loss = 1073.62447492\n",
            "Iteration 5003, loss = 1073.48369874\n",
            "Iteration 5004, loss = 1072.86720621\n",
            "Iteration 5005, loss = 1072.98403729\n",
            "Iteration 5006, loss = 1072.48227772\n",
            "Iteration 5007, loss = 1072.52148173\n",
            "Iteration 5008, loss = 1071.92263065\n",
            "Iteration 5009, loss = 1071.80091307\n",
            "Iteration 5010, loss = 1071.13732942\n",
            "Iteration 5011, loss = 1070.88638213\n",
            "Iteration 5012, loss = 1071.17914272\n",
            "Iteration 5013, loss = 1070.55217133\n",
            "Iteration 5014, loss = 1070.30788465\n",
            "Iteration 5015, loss = 1069.78188028\n",
            "Iteration 5016, loss = 1069.73420212\n",
            "Iteration 5017, loss = 1069.12088969\n",
            "Iteration 5018, loss = 1068.83606510\n",
            "Iteration 5019, loss = 1068.79957775\n",
            "Iteration 5020, loss = 1068.56710795\n",
            "Iteration 5021, loss = 1068.07088114\n",
            "Iteration 5022, loss = 1067.83201610\n",
            "Iteration 5023, loss = 1067.61905385\n",
            "Iteration 5024, loss = 1067.32378110\n",
            "Iteration 5025, loss = 1067.00151463\n",
            "Iteration 5026, loss = 1067.12655640\n",
            "Iteration 5027, loss = 1066.41691573\n",
            "Iteration 5028, loss = 1065.81357566\n",
            "Iteration 5029, loss = 1065.80538396\n",
            "Iteration 5030, loss = 1065.80148225\n",
            "Iteration 5031, loss = 1065.07971147\n",
            "Iteration 5032, loss = 1065.05624162\n",
            "Iteration 5033, loss = 1064.85804535\n",
            "Iteration 5034, loss = 1064.63174004\n",
            "Iteration 5035, loss = 1064.16999242\n",
            "Iteration 5036, loss = 1063.89839011\n",
            "Iteration 5037, loss = 1063.44685053\n",
            "Iteration 5038, loss = 1063.38071409\n",
            "Iteration 5039, loss = 1064.31262110\n",
            "Iteration 5040, loss = 1063.05297527\n",
            "Iteration 5041, loss = 1063.28844570\n",
            "Iteration 5042, loss = 1062.04794406\n",
            "Iteration 5043, loss = 1062.88732758\n",
            "Iteration 5044, loss = 1061.71058917\n",
            "Iteration 5045, loss = 1061.10474939\n",
            "Iteration 5046, loss = 1061.41141467\n",
            "Iteration 5047, loss = 1061.39592372\n",
            "Iteration 5048, loss = 1060.69320985\n",
            "Iteration 5049, loss = 1060.19182862\n",
            "Iteration 5050, loss = 1060.08201544\n",
            "Iteration 5051, loss = 1060.03725004\n",
            "Iteration 5052, loss = 1059.57673264\n",
            "Iteration 5053, loss = 1059.66615833\n",
            "Iteration 5054, loss = 1058.79371555\n",
            "Iteration 5055, loss = 1058.42751331\n",
            "Iteration 5056, loss = 1058.22666026\n",
            "Iteration 5057, loss = 1057.95156574\n",
            "Iteration 5058, loss = 1058.03874881\n",
            "Iteration 5059, loss = 1057.56940619\n",
            "Iteration 5060, loss = 1057.78364347\n",
            "Iteration 5061, loss = 1057.12638923\n",
            "Iteration 5062, loss = 1057.22284296\n",
            "Iteration 5063, loss = 1056.54046584\n",
            "Iteration 5064, loss = 1056.49509292\n",
            "Iteration 5065, loss = 1056.10360760\n",
            "Iteration 5066, loss = 1056.10538163\n",
            "Iteration 5067, loss = 1055.62898652\n",
            "Iteration 5068, loss = 1055.64687276\n",
            "Iteration 5069, loss = 1055.02871809\n",
            "Iteration 5070, loss = 1054.65657146\n",
            "Iteration 5071, loss = 1054.67338196\n",
            "Iteration 5072, loss = 1054.17839820\n",
            "Iteration 5073, loss = 1054.26703673\n",
            "Iteration 5074, loss = 1054.11267694\n",
            "Iteration 5075, loss = 1053.48594124\n",
            "Iteration 5076, loss = 1052.73738844\n",
            "Iteration 5077, loss = 1052.69628039\n",
            "Iteration 5078, loss = 1052.95052316\n",
            "Iteration 5079, loss = 1052.40120797\n",
            "Iteration 5080, loss = 1052.03446656\n",
            "Iteration 5081, loss = 1051.70339098\n",
            "Iteration 5082, loss = 1051.54144101\n",
            "Iteration 5083, loss = 1050.98282416\n",
            "Iteration 5084, loss = 1051.27750627\n",
            "Iteration 5085, loss = 1050.72802519\n",
            "Iteration 5086, loss = 1050.19085819\n",
            "Iteration 5087, loss = 1049.88360257\n",
            "Iteration 5088, loss = 1049.58400805\n",
            "Iteration 5089, loss = 1049.41398124\n",
            "Iteration 5090, loss = 1049.53467247\n",
            "Iteration 5091, loss = 1049.73007613\n",
            "Iteration 5092, loss = 1048.50506677\n",
            "Iteration 5093, loss = 1048.40515874\n",
            "Iteration 5094, loss = 1048.11774301\n",
            "Iteration 5095, loss = 1047.76102715\n",
            "Iteration 5096, loss = 1048.36465075\n",
            "Iteration 5097, loss = 1047.41376465\n",
            "Iteration 5098, loss = 1047.00302095\n",
            "Iteration 5099, loss = 1046.56535881\n",
            "Iteration 5100, loss = 1046.98591370\n",
            "Iteration 5101, loss = 1046.14571630\n",
            "Iteration 5102, loss = 1046.31045321\n",
            "Iteration 5103, loss = 1046.27303525\n",
            "Iteration 5104, loss = 1045.79336526\n",
            "Iteration 5105, loss = 1045.34812170\n",
            "Iteration 5106, loss = 1045.76709210\n",
            "Iteration 5107, loss = 1045.28493550\n",
            "Iteration 5108, loss = 1044.36357142\n",
            "Iteration 5109, loss = 1044.45784845\n",
            "Iteration 5110, loss = 1044.36205004\n",
            "Iteration 5111, loss = 1043.72634998\n",
            "Iteration 5112, loss = 1043.18787105\n",
            "Iteration 5113, loss = 1042.83123625\n",
            "Iteration 5114, loss = 1042.78316976\n",
            "Iteration 5115, loss = 1042.80170799\n",
            "Iteration 5116, loss = 1042.11201715\n",
            "Iteration 5117, loss = 1041.96705624\n",
            "Iteration 5118, loss = 1041.66327618\n",
            "Iteration 5119, loss = 1042.16067666\n",
            "Iteration 5120, loss = 1041.33342743\n",
            "Iteration 5121, loss = 1040.95948028\n",
            "Iteration 5122, loss = 1040.64027999\n",
            "Iteration 5123, loss = 1040.39796780\n",
            "Iteration 5124, loss = 1039.78017963\n",
            "Iteration 5125, loss = 1039.44503540\n",
            "Iteration 5126, loss = 1039.60729512\n",
            "Iteration 5127, loss = 1039.52854138\n",
            "Iteration 5128, loss = 1039.29086337\n",
            "Iteration 5129, loss = 1039.49312725\n",
            "Iteration 5130, loss = 1038.66839537\n",
            "Iteration 5131, loss = 1038.37628517\n",
            "Iteration 5132, loss = 1038.19713277\n",
            "Iteration 5133, loss = 1037.67931562\n",
            "Iteration 5134, loss = 1037.78546653\n",
            "Iteration 5135, loss = 1037.48329667\n",
            "Iteration 5136, loss = 1036.81685258\n",
            "Iteration 5137, loss = 1036.81865411\n",
            "Iteration 5138, loss = 1036.34483417\n",
            "Iteration 5139, loss = 1036.31167752\n",
            "Iteration 5140, loss = 1035.87026515\n",
            "Iteration 5141, loss = 1035.53774090\n",
            "Iteration 5142, loss = 1035.52854019\n",
            "Iteration 5143, loss = 1035.10205807\n",
            "Iteration 5144, loss = 1034.57707022\n",
            "Iteration 5145, loss = 1035.22986698\n",
            "Iteration 5146, loss = 1034.41546616\n",
            "Iteration 5147, loss = 1033.94277151\n",
            "Iteration 5148, loss = 1034.17760640\n",
            "Iteration 5149, loss = 1034.15731380\n",
            "Iteration 5150, loss = 1033.15989542\n",
            "Iteration 5151, loss = 1032.71332822\n",
            "Iteration 5152, loss = 1032.95661104\n",
            "Iteration 5153, loss = 1032.49128444\n",
            "Iteration 5154, loss = 1032.23597004\n",
            "Iteration 5155, loss = 1031.81532373\n",
            "Iteration 5156, loss = 1031.25985198\n",
            "Iteration 5157, loss = 1031.39210498\n",
            "Iteration 5158, loss = 1031.15898482\n",
            "Iteration 5159, loss = 1030.72553241\n",
            "Iteration 5160, loss = 1030.51183393\n",
            "Iteration 5161, loss = 1030.12890479\n",
            "Iteration 5162, loss = 1030.24947872\n",
            "Iteration 5163, loss = 1029.74640808\n",
            "Iteration 5164, loss = 1029.41345745\n",
            "Iteration 5165, loss = 1029.25502104\n",
            "Iteration 5166, loss = 1028.95402077\n",
            "Iteration 5167, loss = 1028.75949526\n",
            "Iteration 5168, loss = 1028.66279283\n",
            "Iteration 5169, loss = 1028.52899170\n",
            "Iteration 5170, loss = 1027.99762961\n",
            "Iteration 5171, loss = 1027.64215630\n",
            "Iteration 5172, loss = 1027.58726199\n",
            "Iteration 5173, loss = 1027.09785708\n",
            "Iteration 5174, loss = 1026.92715885\n",
            "Iteration 5175, loss = 1026.66258050\n",
            "Iteration 5176, loss = 1026.69328247\n",
            "Iteration 5177, loss = 1026.46952035\n",
            "Iteration 5178, loss = 1026.04381150\n",
            "Iteration 5179, loss = 1025.74805580\n",
            "Iteration 5180, loss = 1025.42024432\n",
            "Iteration 5181, loss = 1025.20737689\n",
            "Iteration 5182, loss = 1024.98880756\n",
            "Iteration 5183, loss = 1024.48766005\n",
            "Iteration 5184, loss = 1025.19169367\n",
            "Iteration 5185, loss = 1024.02361195\n",
            "Iteration 5186, loss = 1024.07739125\n",
            "Iteration 5187, loss = 1023.67766615\n",
            "Iteration 5188, loss = 1023.31318260\n",
            "Iteration 5189, loss = 1023.00197946\n",
            "Iteration 5190, loss = 1022.85845599\n",
            "Iteration 5191, loss = 1022.77143729\n",
            "Iteration 5192, loss = 1022.73149392\n",
            "Iteration 5193, loss = 1022.17776346\n",
            "Iteration 5194, loss = 1021.53588760\n",
            "Iteration 5195, loss = 1021.32040595\n",
            "Iteration 5196, loss = 1021.16325075\n",
            "Iteration 5197, loss = 1021.71809796\n",
            "Iteration 5198, loss = 1021.97494534\n",
            "Iteration 5199, loss = 1020.76242570\n",
            "Iteration 5200, loss = 1020.27595842\n",
            "Iteration 5201, loss = 1019.81114499\n",
            "Iteration 5202, loss = 1019.88065604\n",
            "Iteration 5203, loss = 1019.28104096\n",
            "Iteration 5204, loss = 1019.59713764\n",
            "Iteration 5205, loss = 1018.89045203\n",
            "Iteration 5206, loss = 1018.56987412\n",
            "Iteration 5207, loss = 1019.17158131\n",
            "Iteration 5208, loss = 1019.26033058\n",
            "Iteration 5209, loss = 1018.11001005\n",
            "Iteration 5210, loss = 1017.65305880\n",
            "Iteration 5211, loss = 1017.78640865\n",
            "Iteration 5212, loss = 1016.91155174\n",
            "Iteration 5213, loss = 1017.04585382\n",
            "Iteration 5214, loss = 1017.14368246\n",
            "Iteration 5215, loss = 1016.36077505\n",
            "Iteration 5216, loss = 1016.19414128\n",
            "Iteration 5217, loss = 1015.91549536\n",
            "Iteration 5218, loss = 1015.40201524\n",
            "Iteration 5219, loss = 1015.38481756\n",
            "Iteration 5220, loss = 1015.21104487\n",
            "Iteration 5221, loss = 1014.94716416\n",
            "Iteration 5222, loss = 1014.64737990\n",
            "Iteration 5223, loss = 1014.49142293\n",
            "Iteration 5224, loss = 1014.33716353\n",
            "Iteration 5225, loss = 1014.12625934\n",
            "Iteration 5226, loss = 1013.84212818\n",
            "Iteration 5227, loss = 1013.38224027\n",
            "Iteration 5228, loss = 1013.28641053\n",
            "Iteration 5229, loss = 1012.81728767\n",
            "Iteration 5230, loss = 1012.51456068\n",
            "Iteration 5231, loss = 1012.65933134\n",
            "Iteration 5232, loss = 1012.50295254\n",
            "Iteration 5233, loss = 1012.34904253\n",
            "Iteration 5234, loss = 1011.86582397\n",
            "Iteration 5235, loss = 1011.43300950\n",
            "Iteration 5236, loss = 1011.04965918\n",
            "Iteration 5237, loss = 1011.25659702\n",
            "Iteration 5238, loss = 1010.97029531\n",
            "Iteration 5239, loss = 1010.46694774\n",
            "Iteration 5240, loss = 1010.36746250\n",
            "Iteration 5241, loss = 1009.95958836\n",
            "Iteration 5242, loss = 1009.54267623\n",
            "Iteration 5243, loss = 1009.25959590\n",
            "Iteration 5244, loss = 1009.07581101\n",
            "Iteration 5245, loss = 1009.02217907\n",
            "Iteration 5246, loss = 1008.74029869\n",
            "Iteration 5247, loss = 1008.41637612\n",
            "Iteration 5248, loss = 1008.36949023\n",
            "Iteration 5249, loss = 1007.95917348\n",
            "Iteration 5250, loss = 1008.13885997\n",
            "Iteration 5251, loss = 1007.64176885\n",
            "Iteration 5252, loss = 1007.41252674\n",
            "Iteration 5253, loss = 1008.32601668\n",
            "Iteration 5254, loss = 1006.90365946\n",
            "Iteration 5255, loss = 1006.45131336\n",
            "Iteration 5256, loss = 1006.82286999\n",
            "Iteration 5257, loss = 1005.95867494\n",
            "Iteration 5258, loss = 1005.58525270\n",
            "Iteration 5259, loss = 1005.92475319\n",
            "Iteration 5260, loss = 1004.80339970\n",
            "Iteration 5261, loss = 1005.00584792\n",
            "Iteration 5262, loss = 1004.69375089\n",
            "Iteration 5263, loss = 1004.81935994\n",
            "Iteration 5264, loss = 1004.30393387\n",
            "Iteration 5265, loss = 1003.61298948\n",
            "Iteration 5266, loss = 1006.00728112\n",
            "Iteration 5267, loss = 1003.67735157\n",
            "Iteration 5268, loss = 1003.45628854\n",
            "Iteration 5269, loss = 1003.37410961\n",
            "Iteration 5270, loss = 1002.85821989\n",
            "Iteration 5271, loss = 1002.35831959\n",
            "Iteration 5272, loss = 1001.94074044\n",
            "Iteration 5273, loss = 1001.70494783\n",
            "Iteration 5274, loss = 1001.44983801\n",
            "Iteration 5275, loss = 1001.33238123\n",
            "Iteration 5276, loss = 1000.97276820\n",
            "Iteration 5277, loss = 1000.73955419\n",
            "Iteration 5278, loss = 1000.72637983\n",
            "Iteration 5279, loss = 1000.60282178\n",
            "Iteration 5280, loss = 1000.08334733\n",
            "Iteration 5281, loss = 999.97236541\n",
            "Iteration 5282, loss = 999.30516078\n",
            "Iteration 5283, loss = 999.06677532\n",
            "Iteration 5284, loss = 998.99293436\n",
            "Iteration 5285, loss = 998.68613288\n",
            "Iteration 5286, loss = 998.54567646\n",
            "Iteration 5287, loss = 998.27758207\n",
            "Iteration 5288, loss = 998.20647522\n",
            "Iteration 5289, loss = 997.64840206\n",
            "Iteration 5290, loss = 998.18801102\n",
            "Iteration 5291, loss = 997.60015762\n",
            "Iteration 5292, loss = 997.22221980\n",
            "Iteration 5293, loss = 996.94948872\n",
            "Iteration 5294, loss = 996.37412757\n",
            "Iteration 5295, loss = 996.38017118\n",
            "Iteration 5296, loss = 996.30960488\n",
            "Iteration 5297, loss = 996.05785663\n",
            "Iteration 5298, loss = 995.70711185\n",
            "Iteration 5299, loss = 995.37838144\n",
            "Iteration 5300, loss = 995.38344231\n",
            "Iteration 5301, loss = 995.37360834\n",
            "Iteration 5302, loss = 994.81020817\n",
            "Iteration 5303, loss = 994.44064766\n",
            "Iteration 5304, loss = 994.02301477\n",
            "Iteration 5305, loss = 993.88846304\n",
            "Iteration 5306, loss = 993.90603337\n",
            "Iteration 5307, loss = 994.47087556\n",
            "Iteration 5308, loss = 993.30527577\n",
            "Iteration 5309, loss = 993.08015464\n",
            "Iteration 5310, loss = 992.85008973\n",
            "Iteration 5311, loss = 992.95895247\n",
            "Iteration 5312, loss = 992.77222628\n",
            "Iteration 5313, loss = 992.15301502\n",
            "Iteration 5314, loss = 992.13948124\n",
            "Iteration 5315, loss = 991.68190602\n",
            "Iteration 5316, loss = 991.38359712\n",
            "Iteration 5317, loss = 990.95736475\n",
            "Iteration 5318, loss = 991.38928298\n",
            "Iteration 5319, loss = 990.80041888\n",
            "Iteration 5320, loss = 990.46180541\n",
            "Iteration 5321, loss = 990.39649018\n",
            "Iteration 5322, loss = 989.83548420\n",
            "Iteration 5323, loss = 989.82498958\n",
            "Iteration 5324, loss = 989.93820782\n",
            "Iteration 5325, loss = 989.48351860\n",
            "Iteration 5326, loss = 989.19055844\n",
            "Iteration 5327, loss = 988.82008620\n",
            "Iteration 5328, loss = 989.00279532\n",
            "Iteration 5329, loss = 989.31191070\n",
            "Iteration 5330, loss = 988.28982299\n",
            "Iteration 5331, loss = 987.74402421\n",
            "Iteration 5332, loss = 987.94458815\n",
            "Iteration 5333, loss = 987.39406517\n",
            "Iteration 5334, loss = 987.27238330\n",
            "Iteration 5335, loss = 987.65077059\n",
            "Iteration 5336, loss = 986.94962948\n",
            "Iteration 5337, loss = 986.66821100\n",
            "Iteration 5338, loss = 985.99129078\n",
            "Iteration 5339, loss = 986.31702506\n",
            "Iteration 5340, loss = 986.21265008\n",
            "Iteration 5341, loss = 985.51663290\n",
            "Iteration 5342, loss = 985.42240496\n",
            "Iteration 5343, loss = 985.08866988\n",
            "Iteration 5344, loss = 984.49741229\n",
            "Iteration 5345, loss = 984.26159103\n",
            "Iteration 5346, loss = 984.42362406\n",
            "Iteration 5347, loss = 984.50112890\n",
            "Iteration 5348, loss = 983.86033060\n",
            "Iteration 5349, loss = 983.70601686\n",
            "Iteration 5350, loss = 983.48348105\n",
            "Iteration 5351, loss = 983.16826793\n",
            "Iteration 5352, loss = 983.28082548\n",
            "Iteration 5353, loss = 982.61068236\n",
            "Iteration 5354, loss = 982.40062423\n",
            "Iteration 5355, loss = 982.16068794\n",
            "Iteration 5356, loss = 981.79095585\n",
            "Iteration 5357, loss = 981.83136086\n",
            "Iteration 5358, loss = 981.34727323\n",
            "Iteration 5359, loss = 982.06380694\n",
            "Iteration 5360, loss = 980.90230169\n",
            "Iteration 5361, loss = 980.60341353\n",
            "Iteration 5362, loss = 980.39710096\n",
            "Iteration 5363, loss = 980.23237203\n",
            "Iteration 5364, loss = 980.17649723\n",
            "Iteration 5365, loss = 980.22219472\n",
            "Iteration 5366, loss = 979.54736152\n",
            "Iteration 5367, loss = 979.23371471\n",
            "Iteration 5368, loss = 979.08755686\n",
            "Iteration 5369, loss = 978.68307184\n",
            "Iteration 5370, loss = 978.51829262\n",
            "Iteration 5371, loss = 978.48935071\n",
            "Iteration 5372, loss = 979.67883845\n",
            "Iteration 5373, loss = 977.72874972\n",
            "Iteration 5374, loss = 978.30335345\n",
            "Iteration 5375, loss = 978.01454811\n",
            "Iteration 5376, loss = 977.73892571\n",
            "Iteration 5377, loss = 977.42141666\n",
            "Iteration 5378, loss = 977.04764384\n",
            "Iteration 5379, loss = 976.42496181\n",
            "Iteration 5380, loss = 976.37200597\n",
            "Iteration 5381, loss = 975.95264188\n",
            "Iteration 5382, loss = 975.81141896\n",
            "Iteration 5383, loss = 975.69021750\n",
            "Iteration 5384, loss = 975.87979157\n",
            "Iteration 5385, loss = 975.03622617\n",
            "Iteration 5386, loss = 974.72301270\n",
            "Iteration 5387, loss = 974.95156154\n",
            "Iteration 5388, loss = 974.72437860\n",
            "Iteration 5389, loss = 974.07795978\n",
            "Iteration 5390, loss = 974.24152067\n",
            "Iteration 5391, loss = 973.91760662\n",
            "Iteration 5392, loss = 973.79079900\n",
            "Iteration 5393, loss = 973.25842284\n",
            "Iteration 5394, loss = 973.22752268\n",
            "Iteration 5395, loss = 972.72966670\n",
            "Iteration 5396, loss = 972.69325132\n",
            "Iteration 5397, loss = 972.82238989\n",
            "Iteration 5398, loss = 972.31090443\n",
            "Iteration 5399, loss = 972.36763726\n",
            "Iteration 5400, loss = 971.45273343\n",
            "Iteration 5401, loss = 972.68049699\n",
            "Iteration 5402, loss = 971.21398237\n",
            "Iteration 5403, loss = 971.89025922\n",
            "Iteration 5404, loss = 970.86728008\n",
            "Iteration 5405, loss = 970.61268053\n",
            "Iteration 5406, loss = 970.58487507\n",
            "Iteration 5407, loss = 969.96003891\n",
            "Iteration 5408, loss = 969.68309444\n",
            "Iteration 5409, loss = 969.47344260\n",
            "Iteration 5410, loss = 969.30044575\n",
            "Iteration 5411, loss = 968.98892786\n",
            "Iteration 5412, loss = 968.79111418\n",
            "Iteration 5413, loss = 968.73719998\n",
            "Iteration 5414, loss = 967.94005999\n",
            "Iteration 5415, loss = 968.04815267\n",
            "Iteration 5416, loss = 967.86471644\n",
            "Iteration 5417, loss = 967.48614758\n",
            "Iteration 5418, loss = 967.08313692\n",
            "Iteration 5419, loss = 966.92359975\n",
            "Iteration 5420, loss = 966.84362846\n",
            "Iteration 5421, loss = 967.18078068\n",
            "Iteration 5422, loss = 966.21052156\n",
            "Iteration 5423, loss = 965.74156238\n",
            "Iteration 5424, loss = 965.94226577\n",
            "Iteration 5425, loss = 966.28924489\n",
            "Iteration 5426, loss = 965.62227168\n",
            "Iteration 5427, loss = 965.59377809\n",
            "Iteration 5428, loss = 965.00949224\n",
            "Iteration 5429, loss = 964.81903065\n",
            "Iteration 5430, loss = 964.31355919\n",
            "Iteration 5431, loss = 964.23259660\n",
            "Iteration 5432, loss = 964.20603888\n",
            "Iteration 5433, loss = 963.69209210\n",
            "Iteration 5434, loss = 963.43975433\n",
            "Iteration 5435, loss = 963.58935396\n",
            "Iteration 5436, loss = 963.71395882\n",
            "Iteration 5437, loss = 962.85737630\n",
            "Iteration 5438, loss = 962.44372107\n",
            "Iteration 5439, loss = 962.43048570\n",
            "Iteration 5440, loss = 962.02519667\n",
            "Iteration 5441, loss = 962.05640093\n",
            "Iteration 5442, loss = 961.59922695\n",
            "Iteration 5443, loss = 961.53818689\n",
            "Iteration 5444, loss = 961.00239695\n",
            "Iteration 5445, loss = 961.35058931\n",
            "Iteration 5446, loss = 960.65325838\n",
            "Iteration 5447, loss = 960.18761931\n",
            "Iteration 5448, loss = 960.35826370\n",
            "Iteration 5449, loss = 960.21888460\n",
            "Iteration 5450, loss = 960.27093331\n",
            "Iteration 5451, loss = 959.32136018\n",
            "Iteration 5452, loss = 959.66258249\n",
            "Iteration 5453, loss = 959.46250616\n",
            "Iteration 5454, loss = 959.08397265\n",
            "Iteration 5455, loss = 958.90248458\n",
            "Iteration 5456, loss = 958.39247469\n",
            "Iteration 5457, loss = 958.18576135\n",
            "Iteration 5458, loss = 958.38113630\n",
            "Iteration 5459, loss = 957.65121649\n",
            "Iteration 5460, loss = 957.70446062\n",
            "Iteration 5461, loss = 957.22601645\n",
            "Iteration 5462, loss = 957.19754075\n",
            "Iteration 5463, loss = 956.67872259\n",
            "Iteration 5464, loss = 956.70629196\n",
            "Iteration 5465, loss = 956.20499510\n",
            "Iteration 5466, loss = 956.22858616\n",
            "Iteration 5467, loss = 955.88643097\n",
            "Iteration 5468, loss = 955.90674356\n",
            "Iteration 5469, loss = 955.57961650\n",
            "Iteration 5470, loss = 955.40353042\n",
            "Iteration 5471, loss = 954.83687681\n",
            "Iteration 5472, loss = 955.17277418\n",
            "Iteration 5473, loss = 954.63619840\n",
            "Iteration 5474, loss = 954.39526852\n",
            "Iteration 5475, loss = 953.89306133\n",
            "Iteration 5476, loss = 953.71723473\n",
            "Iteration 5477, loss = 953.41111750\n",
            "Iteration 5478, loss = 953.48871835\n",
            "Iteration 5479, loss = 953.13556343\n",
            "Iteration 5480, loss = 953.43051440\n",
            "Iteration 5481, loss = 952.61011905\n",
            "Iteration 5482, loss = 952.51469452\n",
            "Iteration 5483, loss = 952.13994955\n",
            "Iteration 5484, loss = 951.93097366\n",
            "Iteration 5485, loss = 951.90891165\n",
            "Iteration 5486, loss = 951.71799854\n",
            "Iteration 5487, loss = 951.31525375\n",
            "Iteration 5488, loss = 950.94425992\n",
            "Iteration 5489, loss = 950.82913145\n",
            "Iteration 5490, loss = 950.74672741\n",
            "Iteration 5491, loss = 950.27233643\n",
            "Iteration 5492, loss = 950.21925024\n",
            "Iteration 5493, loss = 950.10420480\n",
            "Iteration 5494, loss = 949.78316529\n",
            "Iteration 5495, loss = 949.73978699\n",
            "Iteration 5496, loss = 949.24169754\n",
            "Iteration 5497, loss = 949.94424283\n",
            "Iteration 5498, loss = 948.74660763\n",
            "Iteration 5499, loss = 948.69463056\n",
            "Iteration 5500, loss = 948.59358915\n",
            "Iteration 5501, loss = 948.06223530\n",
            "Iteration 5502, loss = 949.06895616\n",
            "Iteration 5503, loss = 948.16754055\n",
            "Iteration 5504, loss = 947.47682738\n",
            "Iteration 5505, loss = 947.75621058\n",
            "Iteration 5506, loss = 946.95343524\n",
            "Iteration 5507, loss = 946.83874214\n",
            "Iteration 5508, loss = 946.56112531\n",
            "Iteration 5509, loss = 946.32398905\n",
            "Iteration 5510, loss = 946.57025421\n",
            "Iteration 5511, loss = 946.22793962\n",
            "Iteration 5512, loss = 945.48477244\n",
            "Iteration 5513, loss = 945.61716879\n",
            "Iteration 5514, loss = 945.63492944\n",
            "Iteration 5515, loss = 945.31369466\n",
            "Iteration 5516, loss = 944.95856639\n",
            "Iteration 5517, loss = 944.76305978\n",
            "Iteration 5518, loss = 944.40677185\n",
            "Iteration 5519, loss = 944.03419824\n",
            "Iteration 5520, loss = 943.94604600\n",
            "Iteration 5521, loss = 943.79517461\n",
            "Iteration 5522, loss = 943.45674524\n",
            "Iteration 5523, loss = 943.14361347\n",
            "Iteration 5524, loss = 942.64049764\n",
            "Iteration 5525, loss = 942.80981925\n",
            "Iteration 5526, loss = 942.70582811\n",
            "Iteration 5527, loss = 942.81337454\n",
            "Iteration 5528, loss = 941.94383563\n",
            "Iteration 5529, loss = 941.74119375\n",
            "Iteration 5530, loss = 941.69478722\n",
            "Iteration 5531, loss = 941.57204603\n",
            "Iteration 5532, loss = 940.96032992\n",
            "Iteration 5533, loss = 940.55381481\n",
            "Iteration 5534, loss = 941.02304695\n",
            "Iteration 5535, loss = 940.59335268\n",
            "Iteration 5536, loss = 940.46251065\n",
            "Iteration 5537, loss = 940.19270758\n",
            "Iteration 5538, loss = 940.24415068\n",
            "Iteration 5539, loss = 940.23869088\n",
            "Iteration 5540, loss = 939.08061138\n",
            "Iteration 5541, loss = 938.93044877\n",
            "Iteration 5542, loss = 939.59443523\n",
            "Iteration 5543, loss = 939.19168854\n",
            "Iteration 5544, loss = 938.87231107\n",
            "Iteration 5545, loss = 938.06834230\n",
            "Iteration 5546, loss = 937.91378017\n",
            "Iteration 5547, loss = 938.23689759\n",
            "Iteration 5548, loss = 938.38181315\n",
            "Iteration 5549, loss = 937.96272146\n",
            "Iteration 5550, loss = 937.27393108\n",
            "Iteration 5551, loss = 937.87056724\n",
            "Iteration 5552, loss = 936.54209609\n",
            "Iteration 5553, loss = 936.63070354\n",
            "Iteration 5554, loss = 936.51866070\n",
            "Iteration 5555, loss = 936.50820284\n",
            "Iteration 5556, loss = 936.24715968\n",
            "Iteration 5557, loss = 935.56188999\n",
            "Iteration 5558, loss = 935.45951826\n",
            "Iteration 5559, loss = 935.34440751\n",
            "Iteration 5560, loss = 935.29757931\n",
            "Iteration 5561, loss = 935.09436146\n",
            "Iteration 5562, loss = 934.34244901\n",
            "Iteration 5563, loss = 934.16912445\n",
            "Iteration 5564, loss = 934.08441286\n",
            "Iteration 5565, loss = 933.88803149\n",
            "Iteration 5566, loss = 934.43595644\n",
            "Iteration 5567, loss = 933.40660574\n",
            "Iteration 5568, loss = 933.51420381\n",
            "Iteration 5569, loss = 933.43593734\n",
            "Iteration 5570, loss = 932.60132227\n",
            "Iteration 5571, loss = 932.63296020\n",
            "Iteration 5572, loss = 932.52829463\n",
            "Iteration 5573, loss = 931.98804944\n",
            "Iteration 5574, loss = 931.83946692\n",
            "Iteration 5575, loss = 932.14169187\n",
            "Iteration 5576, loss = 932.08970163\n",
            "Iteration 5577, loss = 931.13793737\n",
            "Iteration 5578, loss = 932.14218229\n",
            "Iteration 5579, loss = 930.92109507\n",
            "Iteration 5580, loss = 930.90790408\n",
            "Iteration 5581, loss = 930.79986356\n",
            "Iteration 5582, loss = 929.96191939\n",
            "Iteration 5583, loss = 930.13615020\n",
            "Iteration 5584, loss = 929.52310984\n",
            "Iteration 5585, loss = 929.13846286\n",
            "Iteration 5586, loss = 930.05497370\n",
            "Iteration 5587, loss = 929.62867787\n",
            "Iteration 5588, loss = 929.08687008\n",
            "Iteration 5589, loss = 928.42446023\n",
            "Iteration 5590, loss = 928.60418628\n",
            "Iteration 5591, loss = 927.98379747\n",
            "Iteration 5592, loss = 928.01118674\n",
            "Iteration 5593, loss = 927.46900987\n",
            "Iteration 5594, loss = 927.82025433\n",
            "Iteration 5595, loss = 927.21055277\n",
            "Iteration 5596, loss = 927.08237816\n",
            "Iteration 5597, loss = 926.92612867\n",
            "Iteration 5598, loss = 926.70074418\n",
            "Iteration 5599, loss = 926.48386803\n",
            "Iteration 5600, loss = 926.81370398\n",
            "Iteration 5601, loss = 926.35514034\n",
            "Iteration 5602, loss = 925.67879228\n",
            "Iteration 5603, loss = 925.51276220\n",
            "Iteration 5604, loss = 925.63797000\n",
            "Iteration 5605, loss = 924.87026327\n",
            "Iteration 5606, loss = 924.83314935\n",
            "Iteration 5607, loss = 924.51331846\n",
            "Iteration 5608, loss = 924.66887446\n",
            "Iteration 5609, loss = 923.99509664\n",
            "Iteration 5610, loss = 924.03435103\n",
            "Iteration 5611, loss = 923.81654999\n",
            "Iteration 5612, loss = 923.60067907\n",
            "Iteration 5613, loss = 923.30058247\n",
            "Iteration 5614, loss = 923.14900988\n",
            "Iteration 5615, loss = 922.90319242\n",
            "Iteration 5616, loss = 922.81745717\n",
            "Iteration 5617, loss = 922.44152566\n",
            "Iteration 5618, loss = 924.07586627\n",
            "Iteration 5619, loss = 921.71035372\n",
            "Iteration 5620, loss = 922.01278508\n",
            "Iteration 5621, loss = 921.52882411\n",
            "Iteration 5622, loss = 921.27964971\n",
            "Iteration 5623, loss = 920.94023350\n",
            "Iteration 5624, loss = 921.82813922\n",
            "Iteration 5625, loss = 920.84124488\n",
            "Iteration 5626, loss = 920.37843508\n",
            "Iteration 5627, loss = 920.20208596\n",
            "Iteration 5628, loss = 920.08079216\n",
            "Iteration 5629, loss = 919.58984044\n",
            "Iteration 5630, loss = 919.71203495\n",
            "Iteration 5631, loss = 919.29529268\n",
            "Iteration 5632, loss = 919.23659072\n",
            "Iteration 5633, loss = 919.15306323\n",
            "Iteration 5634, loss = 918.71249877\n",
            "Iteration 5635, loss = 918.37593928\n",
            "Iteration 5636, loss = 918.14453602\n",
            "Iteration 5637, loss = 918.16014256\n",
            "Iteration 5638, loss = 917.86303232\n",
            "Iteration 5639, loss = 918.06183444\n",
            "Iteration 5640, loss = 917.77690124\n",
            "Iteration 5641, loss = 917.27564506\n",
            "Iteration 5642, loss = 917.30840311\n",
            "Iteration 5643, loss = 916.69586629\n",
            "Iteration 5644, loss = 916.33353996\n",
            "Iteration 5645, loss = 916.40202877\n",
            "Iteration 5646, loss = 915.97447341\n",
            "Iteration 5647, loss = 915.90736176\n",
            "Iteration 5648, loss = 915.85317956\n",
            "Iteration 5649, loss = 915.30222145\n",
            "Iteration 5650, loss = 915.40237956\n",
            "Iteration 5651, loss = 914.90822997\n",
            "Iteration 5652, loss = 914.68886875\n",
            "Iteration 5653, loss = 914.95771874\n",
            "Iteration 5654, loss = 914.48627256\n",
            "Iteration 5655, loss = 914.12563784\n",
            "Iteration 5656, loss = 914.00689094\n",
            "Iteration 5657, loss = 913.83750305\n",
            "Iteration 5658, loss = 913.77789447\n",
            "Iteration 5659, loss = 913.15518981\n",
            "Iteration 5660, loss = 912.92337378\n",
            "Iteration 5661, loss = 912.87511826\n",
            "Iteration 5662, loss = 912.95347089\n",
            "Iteration 5663, loss = 913.35992571\n",
            "Iteration 5664, loss = 912.42319130\n",
            "Iteration 5665, loss = 912.78756710\n",
            "Iteration 5666, loss = 912.03742055\n",
            "Iteration 5667, loss = 911.70506996\n",
            "Iteration 5668, loss = 911.54205326\n",
            "Iteration 5669, loss = 911.21520822\n",
            "Iteration 5670, loss = 910.96112651\n",
            "Iteration 5671, loss = 910.97705995\n",
            "Iteration 5672, loss = 910.57093376\n",
            "Iteration 5673, loss = 910.41229397\n",
            "Iteration 5674, loss = 910.19043815\n",
            "Iteration 5675, loss = 910.01148041\n",
            "Iteration 5676, loss = 909.69335509\n",
            "Iteration 5677, loss = 909.82747320\n",
            "Iteration 5678, loss = 909.82561454\n",
            "Iteration 5679, loss = 909.32431664\n",
            "Iteration 5680, loss = 909.06295624\n",
            "Iteration 5681, loss = 909.36100323\n",
            "Iteration 5682, loss = 908.55134233\n",
            "Iteration 5683, loss = 908.43921524\n",
            "Iteration 5684, loss = 907.86896667\n",
            "Iteration 5685, loss = 907.98143421\n",
            "Iteration 5686, loss = 907.96154437\n",
            "Iteration 5687, loss = 907.41338440\n",
            "Iteration 5688, loss = 907.23999340\n",
            "Iteration 5689, loss = 908.17501326\n",
            "Iteration 5690, loss = 906.79995768\n",
            "Iteration 5691, loss = 906.34951813\n",
            "Iteration 5692, loss = 906.95080019\n",
            "Iteration 5693, loss = 907.58735763\n",
            "Iteration 5694, loss = 906.81990976\n",
            "Iteration 5695, loss = 905.62774060\n",
            "Iteration 5696, loss = 905.54149274\n",
            "Iteration 5697, loss = 905.38662413\n",
            "Iteration 5698, loss = 905.08500065\n",
            "Iteration 5699, loss = 904.80172533\n",
            "Iteration 5700, loss = 904.48739016\n",
            "Iteration 5701, loss = 904.46432452\n",
            "Iteration 5702, loss = 904.21715960\n",
            "Iteration 5703, loss = 903.86367927\n",
            "Iteration 5704, loss = 904.15957862\n",
            "Iteration 5705, loss = 903.61616750\n",
            "Iteration 5706, loss = 903.50868159\n",
            "Iteration 5707, loss = 903.27082791\n",
            "Iteration 5708, loss = 902.99944973\n",
            "Iteration 5709, loss = 902.98116797\n",
            "Iteration 5710, loss = 902.83022820\n",
            "Iteration 5711, loss = 902.29476789\n",
            "Iteration 5712, loss = 902.54009923\n",
            "Iteration 5713, loss = 902.20922370\n",
            "Iteration 5714, loss = 901.79815743\n",
            "Iteration 5715, loss = 901.41751076\n",
            "Iteration 5716, loss = 901.31182999\n",
            "Iteration 5717, loss = 901.03405413\n",
            "Iteration 5718, loss = 901.13624782\n",
            "Iteration 5719, loss = 900.54835481\n",
            "Iteration 5720, loss = 900.65150335\n",
            "Iteration 5721, loss = 900.24588853\n",
            "Iteration 5722, loss = 900.23199142\n",
            "Iteration 5723, loss = 900.09114454\n",
            "Iteration 5724, loss = 899.60813534\n",
            "Iteration 5725, loss = 899.40149171\n",
            "Iteration 5726, loss = 899.42651617\n",
            "Iteration 5727, loss = 899.58508809\n",
            "Iteration 5728, loss = 898.51269729\n",
            "Iteration 5729, loss = 898.67040437\n",
            "Iteration 5730, loss = 898.08559688\n",
            "Iteration 5731, loss = 898.42039516\n",
            "Iteration 5732, loss = 897.90034059\n",
            "Iteration 5733, loss = 897.93455225\n",
            "Iteration 5734, loss = 897.35314663\n",
            "Iteration 5735, loss = 897.36326767\n",
            "Iteration 5736, loss = 897.98762749\n",
            "Iteration 5737, loss = 896.85017406\n",
            "Iteration 5738, loss = 896.88887190\n",
            "Iteration 5739, loss = 896.34523845\n",
            "Iteration 5740, loss = 896.47797802\n",
            "Iteration 5741, loss = 896.36026976\n",
            "Iteration 5742, loss = 895.97026187\n",
            "Iteration 5743, loss = 895.75156422\n",
            "Iteration 5744, loss = 895.79996013\n",
            "Iteration 5745, loss = 895.22240366\n",
            "Iteration 5746, loss = 895.52761284\n",
            "Iteration 5747, loss = 895.20385293\n",
            "Iteration 5748, loss = 894.54094139\n",
            "Iteration 5749, loss = 894.24676288\n",
            "Iteration 5750, loss = 894.19709016\n",
            "Iteration 5751, loss = 893.90223945\n",
            "Iteration 5752, loss = 893.84208527\n",
            "Iteration 5753, loss = 893.44538139\n",
            "Iteration 5754, loss = 893.28383695\n",
            "Iteration 5755, loss = 893.32561526\n",
            "Iteration 5756, loss = 892.86876306\n",
            "Iteration 5757, loss = 892.85485805\n",
            "Iteration 5758, loss = 892.65791727\n",
            "Iteration 5759, loss = 892.54914087\n",
            "Iteration 5760, loss = 892.56617592\n",
            "Iteration 5761, loss = 891.52046101\n",
            "Iteration 5762, loss = 891.45894512\n",
            "Iteration 5763, loss = 892.53121245\n",
            "Iteration 5764, loss = 892.32703423\n",
            "Iteration 5765, loss = 891.41397745\n",
            "Iteration 5766, loss = 890.56733936\n",
            "Iteration 5767, loss = 891.31725119\n",
            "Iteration 5768, loss = 890.58392031\n",
            "Iteration 5769, loss = 890.37037169\n",
            "Iteration 5770, loss = 890.53787492\n",
            "Iteration 5771, loss = 889.84219256\n",
            "Iteration 5772, loss = 889.45783964\n",
            "Iteration 5773, loss = 889.53160622\n",
            "Iteration 5774, loss = 889.05067185\n",
            "Iteration 5775, loss = 888.88804052\n",
            "Iteration 5776, loss = 889.09430440\n",
            "Iteration 5777, loss = 888.61028749\n",
            "Iteration 5778, loss = 888.38861985\n",
            "Iteration 5779, loss = 888.51839921\n",
            "Iteration 5780, loss = 888.20809481\n",
            "Iteration 5781, loss = 887.54816110\n",
            "Iteration 5782, loss = 887.46229477\n",
            "Iteration 5783, loss = 887.11851645\n",
            "Iteration 5784, loss = 887.12043735\n",
            "Iteration 5785, loss = 886.67945815\n",
            "Iteration 5786, loss = 886.70774169\n",
            "Iteration 5787, loss = 886.46050194\n",
            "Iteration 5788, loss = 886.04830806\n",
            "Iteration 5789, loss = 885.74179161\n",
            "Iteration 5790, loss = 885.72168399\n",
            "Iteration 5791, loss = 885.95883073\n",
            "Iteration 5792, loss = 886.19553720\n",
            "Iteration 5793, loss = 885.47063711\n",
            "Iteration 5794, loss = 885.70930481\n",
            "Iteration 5795, loss = 884.85559043\n",
            "Iteration 5796, loss = 884.34446886\n",
            "Iteration 5797, loss = 884.54205884\n",
            "Iteration 5798, loss = 884.46808874\n",
            "Iteration 5799, loss = 883.83762708\n",
            "Iteration 5800, loss = 884.67630370\n",
            "Iteration 5801, loss = 883.82761349\n",
            "Iteration 5802, loss = 883.73618996\n",
            "Iteration 5803, loss = 882.98610517\n",
            "Iteration 5804, loss = 882.79551552\n",
            "Iteration 5805, loss = 882.59284285\n",
            "Iteration 5806, loss = 882.41951378\n",
            "Iteration 5807, loss = 882.81461629\n",
            "Iteration 5808, loss = 882.37205066\n",
            "Iteration 5809, loss = 882.14256437\n",
            "Iteration 5810, loss = 881.54780026\n",
            "Iteration 5811, loss = 881.99361593\n",
            "Iteration 5812, loss = 881.42786411\n",
            "Iteration 5813, loss = 880.98713296\n",
            "Iteration 5814, loss = 881.04647227\n",
            "Iteration 5815, loss = 880.47234850\n",
            "Iteration 5816, loss = 880.35831061\n",
            "Iteration 5817, loss = 880.47204003\n",
            "Iteration 5818, loss = 880.12676046\n",
            "Iteration 5819, loss = 879.79535077\n",
            "Iteration 5820, loss = 879.88595163\n",
            "Iteration 5821, loss = 879.47513987\n",
            "Iteration 5822, loss = 879.36673387\n",
            "Iteration 5823, loss = 879.07091922\n",
            "Iteration 5824, loss = 878.70220547\n",
            "Iteration 5825, loss = 878.46772025\n",
            "Iteration 5826, loss = 878.34692584\n",
            "Iteration 5827, loss = 878.12746267\n",
            "Iteration 5828, loss = 877.90298941\n",
            "Iteration 5829, loss = 877.79997719\n",
            "Iteration 5830, loss = 877.38662828\n",
            "Iteration 5831, loss = 877.29017856\n",
            "Iteration 5832, loss = 876.94292446\n",
            "Iteration 5833, loss = 877.35206074\n",
            "Iteration 5834, loss = 876.90526265\n",
            "Iteration 5835, loss = 876.67344238\n",
            "Iteration 5836, loss = 876.47840173\n",
            "Iteration 5837, loss = 876.02247590\n",
            "Iteration 5838, loss = 875.90576043\n",
            "Iteration 5839, loss = 875.51394937\n",
            "Iteration 5840, loss = 875.27196022\n",
            "Iteration 5841, loss = 875.52735348\n",
            "Iteration 5842, loss = 875.02450294\n",
            "Iteration 5843, loss = 874.79916370\n",
            "Iteration 5844, loss = 874.93911225\n",
            "Iteration 5845, loss = 874.37814044\n",
            "Iteration 5846, loss = 874.49022700\n",
            "Iteration 5847, loss = 874.12819469\n",
            "Iteration 5848, loss = 874.69655363\n",
            "Iteration 5849, loss = 873.72415477\n",
            "Iteration 5850, loss = 874.26193143\n",
            "Iteration 5851, loss = 873.41269082\n",
            "Iteration 5852, loss = 872.88446743\n",
            "Iteration 5853, loss = 872.80790205\n",
            "Iteration 5854, loss = 872.68849106\n",
            "Iteration 5855, loss = 872.38054842\n",
            "Iteration 5856, loss = 872.18111769\n",
            "Iteration 5857, loss = 872.04982687\n",
            "Iteration 5858, loss = 871.72247571\n",
            "Iteration 5859, loss = 871.49938491\n",
            "Iteration 5860, loss = 871.49416683\n",
            "Iteration 5861, loss = 871.78111972\n",
            "Iteration 5862, loss = 870.78060913\n",
            "Iteration 5863, loss = 870.66967574\n",
            "Iteration 5864, loss = 870.49674914\n",
            "Iteration 5865, loss = 870.39438791\n",
            "Iteration 5866, loss = 870.07492337\n",
            "Iteration 5867, loss = 870.56403799\n",
            "Iteration 5868, loss = 869.70433550\n",
            "Iteration 5869, loss = 869.74446776\n",
            "Iteration 5870, loss = 869.40293201\n",
            "Iteration 5871, loss = 868.84644481\n",
            "Iteration 5872, loss = 868.85437443\n",
            "Iteration 5873, loss = 868.71487276\n",
            "Iteration 5874, loss = 868.45596598\n",
            "Iteration 5875, loss = 868.30253057\n",
            "Iteration 5876, loss = 868.04294818\n",
            "Iteration 5877, loss = 867.78268532\n",
            "Iteration 5878, loss = 867.67201088\n",
            "Iteration 5879, loss = 867.36314606\n",
            "Iteration 5880, loss = 867.42549566\n",
            "Iteration 5881, loss = 867.33399161\n",
            "Iteration 5882, loss = 867.04129441\n",
            "Iteration 5883, loss = 866.65804300\n",
            "Iteration 5884, loss = 866.71262707\n",
            "Iteration 5885, loss = 866.64839460\n",
            "Iteration 5886, loss = 866.04720395\n",
            "Iteration 5887, loss = 866.03480542\n",
            "Iteration 5888, loss = 865.60054774\n",
            "Iteration 5889, loss = 865.52566952\n",
            "Iteration 5890, loss = 865.17010236\n",
            "Iteration 5891, loss = 865.20938611\n",
            "Iteration 5892, loss = 865.47875186\n",
            "Iteration 5893, loss = 865.09492481\n",
            "Iteration 5894, loss = 864.87457903\n",
            "Iteration 5895, loss = 864.38601274\n",
            "Iteration 5896, loss = 864.29012831\n",
            "Iteration 5897, loss = 863.93065181\n",
            "Iteration 5898, loss = 863.96502214\n",
            "Iteration 5899, loss = 864.92732711\n",
            "Iteration 5900, loss = 863.54629245\n",
            "Iteration 5901, loss = 863.35561170\n",
            "Iteration 5902, loss = 863.03453690\n",
            "Iteration 5903, loss = 862.59097934\n",
            "Iteration 5904, loss = 862.23276982\n",
            "Iteration 5905, loss = 862.38722827\n",
            "Iteration 5906, loss = 862.04454784\n",
            "Iteration 5907, loss = 862.12785838\n",
            "Iteration 5908, loss = 861.95066798\n",
            "Iteration 5909, loss = 861.86356709\n",
            "Iteration 5910, loss = 861.16953650\n",
            "Iteration 5911, loss = 861.32941969\n",
            "Iteration 5912, loss = 861.09066869\n",
            "Iteration 5913, loss = 861.89449062\n",
            "Iteration 5914, loss = 861.12959651\n",
            "Iteration 5915, loss = 860.58255857\n",
            "Iteration 5916, loss = 860.37760370\n",
            "Iteration 5917, loss = 860.18335366\n",
            "Iteration 5918, loss = 859.84407049\n",
            "Iteration 5919, loss = 859.71853746\n",
            "Iteration 5920, loss = 859.49014610\n",
            "Iteration 5921, loss = 859.30038958\n",
            "Iteration 5922, loss = 859.38596064\n",
            "Iteration 5923, loss = 859.03659700\n",
            "Iteration 5924, loss = 858.43552714\n",
            "Iteration 5925, loss = 859.53816531\n",
            "Iteration 5926, loss = 858.64775941\n",
            "Iteration 5927, loss = 857.97611949\n",
            "Iteration 5928, loss = 858.09603008\n",
            "Iteration 5929, loss = 857.43846124\n",
            "Iteration 5930, loss = 857.27937857\n",
            "Iteration 5931, loss = 857.39515806\n",
            "Iteration 5932, loss = 856.83740694\n",
            "Iteration 5933, loss = 856.63032748\n",
            "Iteration 5934, loss = 857.11981610\n",
            "Iteration 5935, loss = 856.24814224\n",
            "Iteration 5936, loss = 856.07196441\n",
            "Iteration 5937, loss = 856.24567743\n",
            "Iteration 5938, loss = 855.86446058\n",
            "Iteration 5939, loss = 855.96052333\n",
            "Iteration 5940, loss = 855.74561307\n",
            "Iteration 5941, loss = 855.19014006\n",
            "Iteration 5942, loss = 854.85964998\n",
            "Iteration 5943, loss = 854.74493727\n",
            "Iteration 5944, loss = 854.64101327\n",
            "Iteration 5945, loss = 854.56951712\n",
            "Iteration 5946, loss = 854.71358112\n",
            "Iteration 5947, loss = 854.30961835\n",
            "Iteration 5948, loss = 853.85810642\n",
            "Iteration 5949, loss = 853.97331299\n",
            "Iteration 5950, loss = 854.09226180\n",
            "Iteration 5951, loss = 853.43183921\n",
            "Iteration 5952, loss = 853.10535109\n",
            "Iteration 5953, loss = 852.93978610\n",
            "Iteration 5954, loss = 852.66332289\n",
            "Iteration 5955, loss = 852.71111285\n",
            "Iteration 5956, loss = 852.21167768\n",
            "Iteration 5957, loss = 851.99477125\n",
            "Iteration 5958, loss = 852.21587911\n",
            "Iteration 5959, loss = 851.68595489\n",
            "Iteration 5960, loss = 851.49609522\n",
            "Iteration 5961, loss = 851.57034315\n",
            "Iteration 5962, loss = 851.25936938\n",
            "Iteration 5963, loss = 851.10251287\n",
            "Iteration 5964, loss = 850.99949612\n",
            "Iteration 5965, loss = 851.01120581\n",
            "Iteration 5966, loss = 850.81104677\n",
            "Iteration 5967, loss = 850.27904493\n",
            "Iteration 5968, loss = 850.11359217\n",
            "Iteration 5969, loss = 849.65055736\n",
            "Iteration 5970, loss = 849.55369681\n",
            "Iteration 5971, loss = 850.09834976\n",
            "Iteration 5972, loss = 848.94762135\n",
            "Iteration 5973, loss = 849.44092082\n",
            "Iteration 5974, loss = 848.98404998\n",
            "Iteration 5975, loss = 848.86590792\n",
            "Iteration 5976, loss = 849.28909711\n",
            "Iteration 5977, loss = 848.27403175\n",
            "Iteration 5978, loss = 848.11171916\n",
            "Iteration 5979, loss = 847.90725289\n",
            "Iteration 5980, loss = 847.32389201\n",
            "Iteration 5981, loss = 847.37544906\n",
            "Iteration 5982, loss = 847.28979148\n",
            "Iteration 5983, loss = 847.13580183\n",
            "Iteration 5984, loss = 846.66589664\n",
            "Iteration 5985, loss = 846.62870083\n",
            "Iteration 5986, loss = 846.31681401\n",
            "Iteration 5987, loss = 846.81817475\n",
            "Iteration 5988, loss = 845.86273096\n",
            "Iteration 5989, loss = 846.49130578\n",
            "Iteration 5990, loss = 846.23554428\n",
            "Iteration 5991, loss = 845.56783042\n",
            "Iteration 5992, loss = 845.11396250\n",
            "Iteration 5993, loss = 844.89205902\n",
            "Iteration 5994, loss = 844.89923804\n",
            "Iteration 5995, loss = 844.96412335\n",
            "Iteration 5996, loss = 844.50761443\n",
            "Iteration 5997, loss = 844.25632388\n",
            "Iteration 5998, loss = 844.03611601\n",
            "Iteration 5999, loss = 844.25091850\n",
            "Iteration 6000, loss = 844.05981547\n",
            "Iteration 6001, loss = 843.79317167\n",
            "Iteration 6002, loss = 843.35788411\n",
            "Iteration 6003, loss = 843.13764591\n",
            "Iteration 6004, loss = 842.90018721\n",
            "Iteration 6005, loss = 843.86692730\n",
            "Iteration 6006, loss = 842.51494993\n",
            "Iteration 6007, loss = 841.55947582\n",
            "Iteration 6008, loss = 841.80469246\n",
            "Iteration 6009, loss = 843.20939412\n",
            "Iteration 6010, loss = 843.14422004\n",
            "Iteration 6011, loss = 841.62903055\n",
            "Iteration 6012, loss = 840.92370751\n",
            "Iteration 6013, loss = 841.74739257\n",
            "Iteration 6014, loss = 841.62715824\n",
            "Iteration 6015, loss = 840.69319112\n",
            "Iteration 6016, loss = 840.25062813\n",
            "Iteration 6017, loss = 840.83215485\n",
            "Iteration 6018, loss = 840.82873035\n",
            "Iteration 6019, loss = 839.88911057\n",
            "Iteration 6020, loss = 839.76406298\n",
            "Iteration 6021, loss = 839.42657562\n",
            "Iteration 6022, loss = 839.40889150\n",
            "Iteration 6023, loss = 839.01986526\n",
            "Iteration 6024, loss = 838.71104198\n",
            "Iteration 6025, loss = 838.79905012\n",
            "Iteration 6026, loss = 838.57385426\n",
            "Iteration 6027, loss = 838.21971646\n",
            "Iteration 6028, loss = 837.89529755\n",
            "Iteration 6029, loss = 837.78464818\n",
            "Iteration 6030, loss = 837.59845782\n",
            "Iteration 6031, loss = 837.54686599\n",
            "Iteration 6032, loss = 837.32236957\n",
            "Iteration 6033, loss = 837.04287623\n",
            "Iteration 6034, loss = 837.41554869\n",
            "Iteration 6035, loss = 837.78739818\n",
            "Iteration 6036, loss = 836.81322965\n",
            "Iteration 6037, loss = 837.06167311\n",
            "Iteration 6038, loss = 836.68411150\n",
            "Iteration 6039, loss = 836.11486627\n",
            "Iteration 6040, loss = 835.96301807\n",
            "Iteration 6041, loss = 835.67541877\n",
            "Iteration 6042, loss = 835.35486762\n",
            "Iteration 6043, loss = 835.24855210\n",
            "Iteration 6044, loss = 835.05967725\n",
            "Iteration 6045, loss = 834.89484637\n",
            "Iteration 6046, loss = 834.62413019\n",
            "Iteration 6047, loss = 834.79153957\n",
            "Iteration 6048, loss = 834.21980047\n",
            "Iteration 6049, loss = 834.41205016\n",
            "Iteration 6050, loss = 833.47769132\n",
            "Iteration 6051, loss = 834.35562402\n",
            "Iteration 6052, loss = 833.92068674\n",
            "Iteration 6053, loss = 833.36734428\n",
            "Iteration 6054, loss = 832.99604722\n",
            "Iteration 6055, loss = 832.80976957\n",
            "Iteration 6056, loss = 832.55013990\n",
            "Iteration 6057, loss = 832.52001029\n",
            "Iteration 6058, loss = 832.20782166\n",
            "Iteration 6059, loss = 831.96511180\n",
            "Iteration 6060, loss = 831.99131834\n",
            "Iteration 6061, loss = 831.34424720\n",
            "Iteration 6062, loss = 831.43794483\n",
            "Iteration 6063, loss = 831.33103369\n",
            "Iteration 6064, loss = 830.90786501\n",
            "Iteration 6065, loss = 831.17098253\n",
            "Iteration 6066, loss = 831.04290706\n",
            "Iteration 6067, loss = 830.66218953\n",
            "Iteration 6068, loss = 830.30917300\n",
            "Iteration 6069, loss = 829.87259112\n",
            "Iteration 6070, loss = 829.85302817\n",
            "Iteration 6071, loss = 829.47618634\n",
            "Iteration 6072, loss = 829.32985321\n",
            "Iteration 6073, loss = 828.99161461\n",
            "Iteration 6074, loss = 829.14671105\n",
            "Iteration 6075, loss = 829.48083866\n",
            "Iteration 6076, loss = 828.63222301\n",
            "Iteration 6077, loss = 828.85382349\n",
            "Iteration 6078, loss = 828.19361023\n",
            "Iteration 6079, loss = 828.09928704\n",
            "Iteration 6080, loss = 827.92772917\n",
            "Iteration 6081, loss = 827.68259226\n",
            "Iteration 6082, loss = 827.33533811\n",
            "Iteration 6083, loss = 827.16857993\n",
            "Iteration 6084, loss = 827.26145996\n",
            "Iteration 6085, loss = 827.01214842\n",
            "Iteration 6086, loss = 826.38709602\n",
            "Iteration 6087, loss = 826.84267721\n",
            "Iteration 6088, loss = 826.84432979\n",
            "Iteration 6089, loss = 826.17073613\n",
            "Iteration 6090, loss = 825.90159615\n",
            "Iteration 6091, loss = 825.41359288\n",
            "Iteration 6092, loss = 825.34128048\n",
            "Iteration 6093, loss = 825.55400018\n",
            "Iteration 6094, loss = 825.54976059\n",
            "Iteration 6095, loss = 824.68405887\n",
            "Iteration 6096, loss = 824.40287029\n",
            "Iteration 6097, loss = 824.55236739\n",
            "Iteration 6098, loss = 824.79907569\n",
            "Iteration 6099, loss = 824.40530588\n",
            "Iteration 6100, loss = 823.90565370\n",
            "Iteration 6101, loss = 823.61198962\n",
            "Iteration 6102, loss = 823.14508676\n",
            "Iteration 6103, loss = 823.14676963\n",
            "Iteration 6104, loss = 822.86655443\n",
            "Iteration 6105, loss = 822.79926975\n",
            "Iteration 6106, loss = 822.79514228\n",
            "Iteration 6107, loss = 822.45121253\n",
            "Iteration 6108, loss = 822.17712707\n",
            "Iteration 6109, loss = 821.97249302\n",
            "Iteration 6110, loss = 821.74355191\n",
            "Iteration 6111, loss = 821.69635216\n",
            "Iteration 6112, loss = 821.25917837\n",
            "Iteration 6113, loss = 820.97561315\n",
            "Iteration 6114, loss = 821.43526556\n",
            "Iteration 6115, loss = 820.97010828\n",
            "Iteration 6116, loss = 820.70867600\n",
            "Iteration 6117, loss = 821.35813053\n",
            "Iteration 6118, loss = 820.21718954\n",
            "Iteration 6119, loss = 820.55253185\n",
            "Iteration 6120, loss = 819.81238916\n",
            "Iteration 6121, loss = 819.55551062\n",
            "Iteration 6122, loss = 819.51883833\n",
            "Iteration 6123, loss = 819.45915563\n",
            "Iteration 6124, loss = 819.20338659\n",
            "Iteration 6125, loss = 818.96177276\n",
            "Iteration 6126, loss = 818.78965147\n",
            "Iteration 6127, loss = 818.49487875\n",
            "Iteration 6128, loss = 818.23837445\n",
            "Iteration 6129, loss = 817.64814300\n",
            "Iteration 6130, loss = 817.68566212\n",
            "Iteration 6131, loss = 817.49948862\n",
            "Iteration 6132, loss = 817.75477994\n",
            "Iteration 6133, loss = 817.50020688\n",
            "Iteration 6134, loss = 816.83025976\n",
            "Iteration 6135, loss = 817.06262793\n",
            "Iteration 6136, loss = 817.12243757\n",
            "Iteration 6137, loss = 817.03134375\n",
            "Iteration 6138, loss = 816.24395341\n",
            "Iteration 6139, loss = 815.95631461\n",
            "Iteration 6140, loss = 815.91863717\n",
            "Iteration 6141, loss = 815.61085390\n",
            "Iteration 6142, loss = 815.40979171\n",
            "Iteration 6143, loss = 815.16622739\n",
            "Iteration 6144, loss = 814.89264284\n",
            "Iteration 6145, loss = 816.01468074\n",
            "Iteration 6146, loss = 814.98665396\n",
            "Iteration 6147, loss = 814.52111441\n",
            "Iteration 6148, loss = 813.82944091\n",
            "Iteration 6149, loss = 814.13388967\n",
            "Iteration 6150, loss = 813.93993396\n",
            "Iteration 6151, loss = 813.41945056\n",
            "Iteration 6152, loss = 813.40776342\n",
            "Iteration 6153, loss = 813.04532100\n",
            "Iteration 6154, loss = 813.18483766\n",
            "Iteration 6155, loss = 812.89187575\n",
            "Iteration 6156, loss = 812.90190876\n",
            "Iteration 6157, loss = 812.41340420\n",
            "Iteration 6158, loss = 812.10527520\n",
            "Iteration 6159, loss = 811.82553475\n",
            "Iteration 6160, loss = 811.63632471\n",
            "Iteration 6161, loss = 811.35425784\n",
            "Iteration 6162, loss = 810.93445408\n",
            "Iteration 6163, loss = 810.91883807\n",
            "Iteration 6164, loss = 811.26840028\n",
            "Iteration 6165, loss = 810.73906528\n",
            "Iteration 6166, loss = 810.43345148\n",
            "Iteration 6167, loss = 810.23218717\n",
            "Iteration 6168, loss = 810.01743094\n",
            "Iteration 6169, loss = 809.52936604\n",
            "Iteration 6170, loss = 810.10555350\n",
            "Iteration 6171, loss = 809.62296724\n",
            "Iteration 6172, loss = 809.37866506\n",
            "Iteration 6173, loss = 809.03947539\n",
            "Iteration 6174, loss = 808.59755280\n",
            "Iteration 6175, loss = 808.43931963\n",
            "Iteration 6176, loss = 808.49450588\n",
            "Iteration 6177, loss = 808.11912194\n",
            "Iteration 6178, loss = 808.01616502\n",
            "Iteration 6179, loss = 807.99082250\n",
            "Iteration 6180, loss = 807.55938719\n",
            "Iteration 6181, loss = 807.43863966\n",
            "Iteration 6182, loss = 807.06411311\n",
            "Iteration 6183, loss = 806.71253668\n",
            "Iteration 6184, loss = 806.51666474\n",
            "Iteration 6185, loss = 806.55964752\n",
            "Iteration 6186, loss = 806.49787917\n",
            "Iteration 6187, loss = 806.11472145\n",
            "Iteration 6188, loss = 805.83888135\n",
            "Iteration 6189, loss = 805.49079719\n",
            "Iteration 6190, loss = 806.42261388\n",
            "Iteration 6191, loss = 805.83700154\n",
            "Iteration 6192, loss = 805.10195316\n",
            "Iteration 6193, loss = 804.71377774\n",
            "Iteration 6194, loss = 805.27460237\n",
            "Iteration 6195, loss = 804.54236400\n",
            "Iteration 6196, loss = 804.28900682\n",
            "Iteration 6197, loss = 803.99511889\n",
            "Iteration 6198, loss = 804.00959693\n",
            "Iteration 6199, loss = 803.60710095\n",
            "Iteration 6200, loss = 803.18687780\n",
            "Iteration 6201, loss = 803.13273687\n",
            "Iteration 6202, loss = 803.24604630\n",
            "Iteration 6203, loss = 803.11541303\n",
            "Iteration 6204, loss = 802.66027102\n",
            "Iteration 6205, loss = 802.47487837\n",
            "Iteration 6206, loss = 802.48638907\n",
            "Iteration 6207, loss = 802.20549590\n",
            "Iteration 6208, loss = 801.66046095\n",
            "Iteration 6209, loss = 801.42868081\n",
            "Iteration 6210, loss = 801.62779176\n",
            "Iteration 6211, loss = 801.55236073\n",
            "Iteration 6212, loss = 800.79759572\n",
            "Iteration 6213, loss = 800.78981979\n",
            "Iteration 6214, loss = 800.39004528\n",
            "Iteration 6215, loss = 800.54000391\n",
            "Iteration 6216, loss = 800.08056020\n",
            "Iteration 6217, loss = 799.87308409\n",
            "Iteration 6218, loss = 799.65219958\n",
            "Iteration 6219, loss = 799.73942418\n",
            "Iteration 6220, loss = 799.43919704\n",
            "Iteration 6221, loss = 799.66602190\n",
            "Iteration 6222, loss = 799.52486554\n",
            "Iteration 6223, loss = 798.77570424\n",
            "Iteration 6224, loss = 798.56619102\n",
            "Iteration 6225, loss = 798.91060133\n",
            "Iteration 6226, loss = 798.04316226\n",
            "Iteration 6227, loss = 797.78696171\n",
            "Iteration 6228, loss = 797.55825081\n",
            "Iteration 6229, loss = 797.30624702\n",
            "Iteration 6230, loss = 797.13747782\n",
            "Iteration 6231, loss = 797.14510540\n",
            "Iteration 6232, loss = 797.21735675\n",
            "Iteration 6233, loss = 796.63497060\n",
            "Iteration 6234, loss = 796.02730980\n",
            "Iteration 6235, loss = 795.87957938\n",
            "Iteration 6236, loss = 795.56135663\n",
            "Iteration 6237, loss = 795.57786583\n",
            "Iteration 6238, loss = 795.48458137\n",
            "Iteration 6239, loss = 794.78021893\n",
            "Iteration 6240, loss = 795.47656657\n",
            "Iteration 6241, loss = 795.15651124\n",
            "Iteration 6242, loss = 794.63210683\n",
            "Iteration 6243, loss = 794.27315886\n",
            "Iteration 6244, loss = 794.19300542\n",
            "Iteration 6245, loss = 794.65308531\n",
            "Iteration 6246, loss = 793.80923688\n",
            "Iteration 6247, loss = 793.46005810\n",
            "Iteration 6248, loss = 794.14834777\n",
            "Iteration 6249, loss = 793.32523505\n",
            "Iteration 6250, loss = 793.15535613\n",
            "Iteration 6251, loss = 792.96461979\n",
            "Iteration 6252, loss = 793.09958669\n",
            "Iteration 6253, loss = 792.29456389\n",
            "Iteration 6254, loss = 792.12298244\n",
            "Iteration 6255, loss = 791.67696438\n",
            "Iteration 6256, loss = 791.79222682\n",
            "Iteration 6257, loss = 791.73572613\n",
            "Iteration 6258, loss = 791.27213097\n",
            "Iteration 6259, loss = 791.05109824\n",
            "Iteration 6260, loss = 791.09777989\n",
            "Iteration 6261, loss = 790.62001870\n",
            "Iteration 6262, loss = 790.88596104\n",
            "Iteration 6263, loss = 790.85593784\n",
            "Iteration 6264, loss = 790.38377535\n",
            "Iteration 6265, loss = 790.35786801\n",
            "Iteration 6266, loss = 789.76928496\n",
            "Iteration 6267, loss = 789.77810895\n",
            "Iteration 6268, loss = 789.24529311\n",
            "Iteration 6269, loss = 789.11313618\n",
            "Iteration 6270, loss = 788.87984895\n",
            "Iteration 6271, loss = 788.35931455\n",
            "Iteration 6272, loss = 788.38567977\n",
            "Iteration 6273, loss = 788.06389482\n",
            "Iteration 6274, loss = 787.76135042\n",
            "Iteration 6275, loss = 787.86520876\n",
            "Iteration 6276, loss = 787.49562771\n",
            "Iteration 6277, loss = 787.29035925\n",
            "Iteration 6278, loss = 786.94941984\n",
            "Iteration 6279, loss = 786.78444217\n",
            "Iteration 6280, loss = 786.75537434\n",
            "Iteration 6281, loss = 786.64819230\n",
            "Iteration 6282, loss = 786.02323217\n",
            "Iteration 6283, loss = 786.24108896\n",
            "Iteration 6284, loss = 786.00793505\n",
            "Iteration 6285, loss = 785.51206415\n",
            "Iteration 6286, loss = 785.36871732\n",
            "Iteration 6287, loss = 785.17393955\n",
            "Iteration 6288, loss = 784.85477838\n",
            "Iteration 6289, loss = 785.22178624\n",
            "Iteration 6290, loss = 784.62101278\n",
            "Iteration 6291, loss = 785.17588411\n",
            "Iteration 6292, loss = 784.63788159\n",
            "Iteration 6293, loss = 784.06216980\n",
            "Iteration 6294, loss = 783.59479814\n",
            "Iteration 6295, loss = 783.42919399\n",
            "Iteration 6296, loss = 783.58919326\n",
            "Iteration 6297, loss = 783.06979210\n",
            "Iteration 6298, loss = 782.98168846\n",
            "Iteration 6299, loss = 783.12871191\n",
            "Iteration 6300, loss = 782.66594370\n",
            "Iteration 6301, loss = 782.25991876\n",
            "Iteration 6302, loss = 783.13517835\n",
            "Iteration 6303, loss = 781.91310151\n",
            "Iteration 6304, loss = 781.88107630\n",
            "Iteration 6305, loss = 781.46480589\n",
            "Iteration 6306, loss = 782.02425659\n",
            "Iteration 6307, loss = 781.69977052\n",
            "Iteration 6308, loss = 780.78736474\n",
            "Iteration 6309, loss = 780.42117823\n",
            "Iteration 6310, loss = 780.40389793\n",
            "Iteration 6311, loss = 780.14896403\n",
            "Iteration 6312, loss = 780.35444108\n",
            "Iteration 6313, loss = 780.62612496\n",
            "Iteration 6314, loss = 779.92737080\n",
            "Iteration 6315, loss = 780.72254617\n",
            "Iteration 6316, loss = 779.37632875\n",
            "Iteration 6317, loss = 779.10172895\n",
            "Iteration 6318, loss = 778.67509477\n",
            "Iteration 6319, loss = 778.83315831\n",
            "Iteration 6320, loss = 778.25963814\n",
            "Iteration 6321, loss = 778.39898101\n",
            "Iteration 6322, loss = 777.95041839\n",
            "Iteration 6323, loss = 777.86703155\n",
            "Iteration 6324, loss = 777.31723247\n",
            "Iteration 6325, loss = 777.81788200\n",
            "Iteration 6326, loss = 777.05798507\n",
            "Iteration 6327, loss = 777.50703642\n",
            "Iteration 6328, loss = 776.95227281\n",
            "Iteration 6329, loss = 776.65165061\n",
            "Iteration 6330, loss = 776.68239174\n",
            "Iteration 6331, loss = 776.14280049\n",
            "Iteration 6332, loss = 776.49729677\n",
            "Iteration 6333, loss = 776.26106114\n",
            "Iteration 6334, loss = 775.66556261\n",
            "Iteration 6335, loss = 775.79986194\n",
            "Iteration 6336, loss = 775.26090813\n",
            "Iteration 6337, loss = 774.97180726\n",
            "Iteration 6338, loss = 774.81550514\n",
            "Iteration 6339, loss = 775.07311108\n",
            "Iteration 6340, loss = 774.77897852\n",
            "Iteration 6341, loss = 774.40324091\n",
            "Iteration 6342, loss = 774.38413316\n",
            "Iteration 6343, loss = 774.53068968\n",
            "Iteration 6344, loss = 774.63341180\n",
            "Iteration 6345, loss = 773.57589641\n",
            "Iteration 6346, loss = 773.07461365\n",
            "Iteration 6347, loss = 773.10880365\n",
            "Iteration 6348, loss = 772.79626946\n",
            "Iteration 6349, loss = 772.56318358\n",
            "Iteration 6350, loss = 772.32087075\n",
            "Iteration 6351, loss = 772.56157764\n",
            "Iteration 6352, loss = 772.36430871\n",
            "Iteration 6353, loss = 771.93729082\n",
            "Iteration 6354, loss = 771.77889412\n",
            "Iteration 6355, loss = 771.83626614\n",
            "Iteration 6356, loss = 771.64575443\n",
            "Iteration 6357, loss = 771.05774610\n",
            "Iteration 6358, loss = 770.87943222\n",
            "Iteration 6359, loss = 770.64323623\n",
            "Iteration 6360, loss = 770.53879949\n",
            "Iteration 6361, loss = 771.00708768\n",
            "Iteration 6362, loss = 770.57408299\n",
            "Iteration 6363, loss = 769.70719544\n",
            "Iteration 6364, loss = 770.00099768\n",
            "Iteration 6365, loss = 769.92443094\n",
            "Iteration 6366, loss = 769.07511120\n",
            "Iteration 6367, loss = 768.95424821\n",
            "Iteration 6368, loss = 768.99115264\n",
            "Iteration 6369, loss = 768.95907253\n",
            "Iteration 6370, loss = 768.52753086\n",
            "Iteration 6371, loss = 768.77728475\n",
            "Iteration 6372, loss = 768.19392967\n",
            "Iteration 6373, loss = 768.24910161\n",
            "Iteration 6374, loss = 767.60910552\n",
            "Iteration 6375, loss = 767.78041441\n",
            "Iteration 6376, loss = 767.70078732\n",
            "Iteration 6377, loss = 767.53397605\n",
            "Iteration 6378, loss = 766.74988129\n",
            "Iteration 6379, loss = 766.94308516\n",
            "Iteration 6380, loss = 767.25761174\n",
            "Iteration 6381, loss = 766.45492514\n",
            "Iteration 6382, loss = 766.00216878\n",
            "Iteration 6383, loss = 766.13343014\n",
            "Iteration 6384, loss = 766.04510448\n",
            "Iteration 6385, loss = 766.90636102\n",
            "Iteration 6386, loss = 765.94199446\n",
            "Iteration 6387, loss = 765.49878483\n",
            "Iteration 6388, loss = 765.88375273\n",
            "Iteration 6389, loss = 764.88904072\n",
            "Iteration 6390, loss = 765.36876659\n",
            "Iteration 6391, loss = 764.52649799\n",
            "Iteration 6392, loss = 764.14211000\n",
            "Iteration 6393, loss = 763.95467731\n",
            "Iteration 6394, loss = 764.39477679\n",
            "Iteration 6395, loss = 763.89060540\n",
            "Iteration 6396, loss = 763.79623292\n",
            "Iteration 6397, loss = 763.63553466\n",
            "Iteration 6398, loss = 763.58251782\n",
            "Iteration 6399, loss = 763.40298398\n",
            "Iteration 6400, loss = 762.64592729\n",
            "Iteration 6401, loss = 762.74793872\n",
            "Iteration 6402, loss = 762.88267659\n",
            "Iteration 6403, loss = 762.06858214\n",
            "Iteration 6404, loss = 761.85679687\n",
            "Iteration 6405, loss = 762.43766464\n",
            "Iteration 6406, loss = 762.53853993\n",
            "Iteration 6407, loss = 761.78338186\n",
            "Iteration 6408, loss = 761.23179861\n",
            "Iteration 6409, loss = 761.52172487\n",
            "Iteration 6410, loss = 760.96222908\n",
            "Iteration 6411, loss = 761.12957585\n",
            "Iteration 6412, loss = 760.58536353\n",
            "Iteration 6413, loss = 760.40209926\n",
            "Iteration 6414, loss = 760.38251061\n",
            "Iteration 6415, loss = 759.99634571\n",
            "Iteration 6416, loss = 760.16821999\n",
            "Iteration 6417, loss = 760.35676335\n",
            "Iteration 6418, loss = 759.91996903\n",
            "Iteration 6419, loss = 759.00416609\n",
            "Iteration 6420, loss = 758.75273059\n",
            "Iteration 6421, loss = 759.01885721\n",
            "Iteration 6422, loss = 758.86879538\n",
            "Iteration 6423, loss = 758.83043531\n",
            "Iteration 6424, loss = 758.58999864\n",
            "Iteration 6425, loss = 758.60013722\n",
            "Iteration 6426, loss = 757.84969213\n",
            "Iteration 6427, loss = 757.57232365\n",
            "Iteration 6428, loss = 757.30855440\n",
            "Iteration 6429, loss = 757.18343413\n",
            "Iteration 6430, loss = 757.43519531\n",
            "Iteration 6431, loss = 756.92663855\n",
            "Iteration 6432, loss = 756.53997091\n",
            "Iteration 6433, loss = 756.37128937\n",
            "Iteration 6434, loss = 756.65354332\n",
            "Iteration 6435, loss = 755.91583714\n",
            "Iteration 6436, loss = 755.77143818\n",
            "Iteration 6437, loss = 755.70730867\n",
            "Iteration 6438, loss = 755.47055899\n",
            "Iteration 6439, loss = 755.41347313\n",
            "Iteration 6440, loss = 755.33133502\n",
            "Iteration 6441, loss = 754.98305360\n",
            "Iteration 6442, loss = 754.66475336\n",
            "Iteration 6443, loss = 755.24710351\n",
            "Iteration 6444, loss = 755.52547558\n",
            "Iteration 6445, loss = 754.01995274\n",
            "Iteration 6446, loss = 753.60126987\n",
            "Iteration 6447, loss = 753.97871984\n",
            "Iteration 6448, loss = 754.72505219\n",
            "Iteration 6449, loss = 754.83242820\n",
            "Iteration 6450, loss = 754.02609834\n",
            "Iteration 6451, loss = 753.08546112\n",
            "Iteration 6452, loss = 753.15227699\n",
            "Iteration 6453, loss = 753.31132949\n",
            "Iteration 6454, loss = 753.17416632\n",
            "Iteration 6455, loss = 752.38871571\n",
            "Iteration 6456, loss = 752.99263749\n",
            "Iteration 6457, loss = 752.06475063\n",
            "Iteration 6458, loss = 751.61268393\n",
            "Iteration 6459, loss = 751.69351646\n",
            "Iteration 6460, loss = 751.52246079\n",
            "Iteration 6461, loss = 751.15878062\n",
            "Iteration 6462, loss = 750.90401198\n",
            "Iteration 6463, loss = 750.86585595\n",
            "Iteration 6464, loss = 750.74464159\n",
            "Iteration 6465, loss = 750.64898043\n",
            "Iteration 6466, loss = 750.04008774\n",
            "Iteration 6467, loss = 750.35759153\n",
            "Iteration 6468, loss = 750.16154116\n",
            "Iteration 6469, loss = 749.68318648\n",
            "Iteration 6470, loss = 749.76715644\n",
            "Iteration 6471, loss = 749.24151123\n",
            "Iteration 6472, loss = 749.96827345\n",
            "Iteration 6473, loss = 749.06232559\n",
            "Iteration 6474, loss = 748.60348755\n",
            "Iteration 6475, loss = 748.15427086\n",
            "Iteration 6476, loss = 748.64658710\n",
            "Iteration 6477, loss = 748.54710479\n",
            "Iteration 6478, loss = 748.15823990\n",
            "Iteration 6479, loss = 748.19488878\n",
            "Iteration 6480, loss = 747.96978839\n",
            "Iteration 6481, loss = 747.90504041\n",
            "Iteration 6482, loss = 747.36420681\n",
            "Iteration 6483, loss = 747.60867843\n",
            "Iteration 6484, loss = 747.03318177\n",
            "Iteration 6485, loss = 746.97218078\n",
            "Iteration 6486, loss = 746.46470785\n",
            "Iteration 6487, loss = 746.40496970\n",
            "Iteration 6488, loss = 746.55009531\n",
            "Iteration 6489, loss = 746.32742829\n",
            "Iteration 6490, loss = 745.95338135\n",
            "Iteration 6491, loss = 745.95150214\n",
            "Iteration 6492, loss = 745.51329606\n",
            "Iteration 6493, loss = 744.92926453\n",
            "Iteration 6494, loss = 744.85638917\n",
            "Iteration 6495, loss = 746.29511578\n",
            "Iteration 6496, loss = 745.96460146\n",
            "Iteration 6497, loss = 744.89922772\n",
            "Iteration 6498, loss = 744.32803904\n",
            "Iteration 6499, loss = 744.93379907\n",
            "Iteration 6500, loss = 744.53042324\n",
            "Iteration 6501, loss = 744.37948836\n",
            "Iteration 6502, loss = 743.97961922\n",
            "Iteration 6503, loss = 744.31375259\n",
            "Iteration 6504, loss = 743.35282128\n",
            "Iteration 6505, loss = 742.91637263\n",
            "Iteration 6506, loss = 742.94499248\n",
            "Iteration 6507, loss = 742.75664213\n",
            "Iteration 6508, loss = 742.67626259\n",
            "Iteration 6509, loss = 742.29670227\n",
            "Iteration 6510, loss = 741.94284094\n",
            "Iteration 6511, loss = 742.27358894\n",
            "Iteration 6512, loss = 742.00718562\n",
            "Iteration 6513, loss = 741.35699351\n",
            "Iteration 6514, loss = 741.89699771\n",
            "Iteration 6515, loss = 740.99559343\n",
            "Iteration 6516, loss = 740.88199898\n",
            "Iteration 6517, loss = 740.57374744\n",
            "Iteration 6518, loss = 740.87658938\n",
            "Iteration 6519, loss = 741.02291843\n",
            "Iteration 6520, loss = 740.12839043\n",
            "Iteration 6521, loss = 740.12454616\n",
            "Iteration 6522, loss = 739.87697129\n",
            "Iteration 6523, loss = 739.71506340\n",
            "Iteration 6524, loss = 739.35596020\n",
            "Iteration 6525, loss = 739.65600839\n",
            "Iteration 6526, loss = 739.07222074\n",
            "Iteration 6527, loss = 738.81322655\n",
            "Iteration 6528, loss = 739.06295972\n",
            "Iteration 6529, loss = 738.65929155\n",
            "Iteration 6530, loss = 738.87945740\n",
            "Iteration 6531, loss = 738.00971862\n",
            "Iteration 6532, loss = 738.15446703\n",
            "Iteration 6533, loss = 738.02939023\n",
            "Iteration 6534, loss = 737.77487133\n",
            "Iteration 6535, loss = 737.80677494\n",
            "Iteration 6536, loss = 737.59202499\n",
            "Iteration 6537, loss = 736.98769052\n",
            "Iteration 6538, loss = 736.75373588\n",
            "Iteration 6539, loss = 736.65106235\n",
            "Iteration 6540, loss = 736.66048990\n",
            "Iteration 6541, loss = 736.51407848\n",
            "Iteration 6542, loss = 736.12961989\n",
            "Iteration 6543, loss = 736.29791826\n",
            "Iteration 6544, loss = 735.79353825\n",
            "Iteration 6545, loss = 736.48802757\n",
            "Iteration 6546, loss = 736.92037102\n",
            "Iteration 6547, loss = 735.57529177\n",
            "Iteration 6548, loss = 735.26858484\n",
            "Iteration 6549, loss = 734.75722754\n",
            "Iteration 6550, loss = 734.69560038\n",
            "Iteration 6551, loss = 734.39639913\n",
            "Iteration 6552, loss = 734.28672430\n",
            "Iteration 6553, loss = 734.26592754\n",
            "Iteration 6554, loss = 734.57226864\n",
            "Iteration 6555, loss = 733.86866618\n",
            "Iteration 6556, loss = 733.80929824\n",
            "Iteration 6557, loss = 733.52897486\n",
            "Iteration 6558, loss = 733.30879246\n",
            "Iteration 6559, loss = 733.02102669\n",
            "Iteration 6560, loss = 732.82428458\n",
            "Iteration 6561, loss = 732.69842271\n",
            "Iteration 6562, loss = 732.49569738\n",
            "Iteration 6563, loss = 732.27460259\n",
            "Iteration 6564, loss = 731.86500596\n",
            "Iteration 6565, loss = 732.12779737\n",
            "Iteration 6566, loss = 732.40988557\n",
            "Iteration 6567, loss = 731.24481948\n",
            "Iteration 6568, loss = 732.12430592\n",
            "Iteration 6569, loss = 731.97900921\n",
            "Iteration 6570, loss = 731.25209333\n",
            "Iteration 6571, loss = 730.60531508\n",
            "Iteration 6572, loss = 731.19751776\n",
            "Iteration 6573, loss = 730.60351569\n",
            "Iteration 6574, loss = 730.29006479\n",
            "Iteration 6575, loss = 730.72016160\n",
            "Iteration 6576, loss = 730.24579680\n",
            "Iteration 6577, loss = 729.68752487\n",
            "Iteration 6578, loss = 730.41116693\n",
            "Iteration 6579, loss = 729.35498154\n",
            "Iteration 6580, loss = 729.05939606\n",
            "Iteration 6581, loss = 729.17160096\n",
            "Iteration 6582, loss = 728.93513444\n",
            "Iteration 6583, loss = 728.91280165\n",
            "Iteration 6584, loss = 728.50630102\n",
            "Iteration 6585, loss = 728.84915776\n",
            "Iteration 6586, loss = 728.17733492\n",
            "Iteration 6587, loss = 727.77501186\n",
            "Iteration 6588, loss = 728.10166776\n",
            "Iteration 6589, loss = 727.73066093\n",
            "Iteration 6590, loss = 727.53486571\n",
            "Iteration 6591, loss = 727.25156947\n",
            "Iteration 6592, loss = 727.22461009\n",
            "Iteration 6593, loss = 727.15099795\n",
            "Iteration 6594, loss = 727.33838664\n",
            "Iteration 6595, loss = 726.26572811\n",
            "Iteration 6596, loss = 726.10351459\n",
            "Iteration 6597, loss = 726.07270121\n",
            "Iteration 6598, loss = 726.05801614\n",
            "Iteration 6599, loss = 725.95664103\n",
            "Iteration 6600, loss = 725.40435398\n",
            "Iteration 6601, loss = 725.23125296\n",
            "Iteration 6602, loss = 725.31718740\n",
            "Iteration 6603, loss = 725.25834478\n",
            "Iteration 6604, loss = 724.67639404\n",
            "Iteration 6605, loss = 725.04005054\n",
            "Iteration 6606, loss = 724.78672828\n",
            "Iteration 6607, loss = 724.60767410\n",
            "Iteration 6608, loss = 724.68919465\n",
            "Iteration 6609, loss = 724.10769010\n",
            "Iteration 6610, loss = 723.54027131\n",
            "Iteration 6611, loss = 723.70778099\n",
            "Iteration 6612, loss = 723.25926516\n",
            "Iteration 6613, loss = 723.42294726\n",
            "Iteration 6614, loss = 723.31884010\n",
            "Iteration 6615, loss = 722.72618475\n",
            "Iteration 6616, loss = 722.73396298\n",
            "Iteration 6617, loss = 722.59850381\n",
            "Iteration 6618, loss = 722.50868119\n",
            "Iteration 6619, loss = 722.22246713\n",
            "Iteration 6620, loss = 722.28318380\n",
            "Iteration 6621, loss = 721.70686409\n",
            "Iteration 6622, loss = 722.22083803\n",
            "Iteration 6623, loss = 721.49805801\n",
            "Iteration 6624, loss = 721.42930328\n",
            "Iteration 6625, loss = 721.82754260\n",
            "Iteration 6626, loss = 720.89575844\n",
            "Iteration 6627, loss = 721.24623554\n",
            "Iteration 6628, loss = 720.66999748\n",
            "Iteration 6629, loss = 720.60157745\n",
            "Iteration 6630, loss = 720.43428953\n",
            "Iteration 6631, loss = 719.82976316\n",
            "Iteration 6632, loss = 719.92249435\n",
            "Iteration 6633, loss = 719.80063991\n",
            "Iteration 6634, loss = 719.11479039\n",
            "Iteration 6635, loss = 719.56216756\n",
            "Iteration 6636, loss = 719.06150622\n",
            "Iteration 6637, loss = 719.95429139\n",
            "Iteration 6638, loss = 719.33898439\n",
            "Iteration 6639, loss = 718.72587986\n",
            "Iteration 6640, loss = 718.63323087\n",
            "Iteration 6641, loss = 718.25394810\n",
            "Iteration 6642, loss = 718.11396830\n",
            "Iteration 6643, loss = 717.76955807\n",
            "Iteration 6644, loss = 717.80934987\n",
            "Iteration 6645, loss = 717.43765603\n",
            "Iteration 6646, loss = 717.31444254\n",
            "Iteration 6647, loss = 717.28768985\n",
            "Iteration 6648, loss = 716.80096797\n",
            "Iteration 6649, loss = 716.83283260\n",
            "Iteration 6650, loss = 716.64430701\n",
            "Iteration 6651, loss = 716.46105043\n",
            "Iteration 6652, loss = 716.38500279\n",
            "Iteration 6653, loss = 716.00826294\n",
            "Iteration 6654, loss = 715.95398329\n",
            "Iteration 6655, loss = 716.02551989\n",
            "Iteration 6656, loss = 715.46528767\n",
            "Iteration 6657, loss = 715.88595325\n",
            "Iteration 6658, loss = 716.16753395\n",
            "Iteration 6659, loss = 715.73114557\n",
            "Iteration 6660, loss = 715.34681919\n",
            "Iteration 6661, loss = 714.82242761\n",
            "Iteration 6662, loss = 714.48488657\n",
            "Iteration 6663, loss = 715.14134260\n",
            "Iteration 6664, loss = 714.59103634\n",
            "Iteration 6665, loss = 714.10394747\n",
            "Iteration 6666, loss = 714.06740966\n",
            "Iteration 6667, loss = 713.56162591\n",
            "Iteration 6668, loss = 713.59822968\n",
            "Iteration 6669, loss = 713.55701885\n",
            "Iteration 6670, loss = 713.42172168\n",
            "Iteration 6671, loss = 713.82232963\n",
            "Iteration 6672, loss = 712.76088208\n",
            "Iteration 6673, loss = 713.87825172\n",
            "Iteration 6674, loss = 712.58901201\n",
            "Iteration 6675, loss = 712.35949326\n",
            "Iteration 6676, loss = 712.66222638\n",
            "Iteration 6677, loss = 711.95496104\n",
            "Iteration 6678, loss = 711.71610393\n",
            "Iteration 6679, loss = 712.43830433\n",
            "Iteration 6680, loss = 711.81568236\n",
            "Iteration 6681, loss = 712.03837923\n",
            "Iteration 6682, loss = 711.33218266\n",
            "Iteration 6683, loss = 711.37057016\n",
            "Iteration 6684, loss = 711.15821229\n",
            "Iteration 6685, loss = 710.88229470\n",
            "Iteration 6686, loss = 710.31548840\n",
            "Iteration 6687, loss = 710.13630700\n",
            "Iteration 6688, loss = 710.66368291\n",
            "Iteration 6689, loss = 710.11549602\n",
            "Iteration 6690, loss = 709.87882562\n",
            "Iteration 6691, loss = 709.42366058\n",
            "Iteration 6692, loss = 709.15576017\n",
            "Iteration 6693, loss = 709.39550516\n",
            "Iteration 6694, loss = 709.38679178\n",
            "Iteration 6695, loss = 709.41145592\n",
            "Iteration 6696, loss = 708.83738481\n",
            "Iteration 6697, loss = 709.41284578\n",
            "Iteration 6698, loss = 708.35346881\n",
            "Iteration 6699, loss = 708.36856481\n",
            "Iteration 6700, loss = 707.79530294\n",
            "Iteration 6701, loss = 707.53771578\n",
            "Iteration 6702, loss = 708.81600809\n",
            "Iteration 6703, loss = 707.58087951\n",
            "Iteration 6704, loss = 707.71150788\n",
            "Iteration 6705, loss = 707.57957272\n",
            "Iteration 6706, loss = 706.78294693\n",
            "Iteration 6707, loss = 706.44349288\n",
            "Iteration 6708, loss = 706.97866045\n",
            "Iteration 6709, loss = 706.62436432\n",
            "Iteration 6710, loss = 706.13742244\n",
            "Iteration 6711, loss = 706.08461029\n",
            "Iteration 6712, loss = 705.88054462\n",
            "Iteration 6713, loss = 705.68119707\n",
            "Iteration 6714, loss = 705.56405433\n",
            "Iteration 6715, loss = 705.91006306\n",
            "Iteration 6716, loss = 705.39238016\n",
            "Iteration 6717, loss = 705.16701398\n",
            "Iteration 6718, loss = 704.72387316\n",
            "Iteration 6719, loss = 705.18019309\n",
            "Iteration 6720, loss = 704.62950416\n",
            "Iteration 6721, loss = 704.33787601\n",
            "Iteration 6722, loss = 704.10189976\n",
            "Iteration 6723, loss = 704.21148343\n",
            "Iteration 6724, loss = 703.84145877\n",
            "Iteration 6725, loss = 703.56375019\n",
            "Iteration 6726, loss = 703.70679349\n",
            "Iteration 6727, loss = 703.60964892\n",
            "Iteration 6728, loss = 702.97269314\n",
            "Iteration 6729, loss = 703.84569347\n",
            "Iteration 6730, loss = 703.28020208\n",
            "Iteration 6731, loss = 702.70837620\n",
            "Iteration 6732, loss = 702.27241831\n",
            "Iteration 6733, loss = 702.37365758\n",
            "Iteration 6734, loss = 702.54786232\n",
            "Iteration 6735, loss = 702.19994823\n",
            "Iteration 6736, loss = 702.11749481\n",
            "Iteration 6737, loss = 701.72057923\n",
            "Iteration 6738, loss = 701.82984986\n",
            "Iteration 6739, loss = 701.36434469\n",
            "Iteration 6740, loss = 701.09224891\n",
            "Iteration 6741, loss = 700.96318585\n",
            "Iteration 6742, loss = 700.74874417\n",
            "Iteration 6743, loss = 700.49407178\n",
            "Iteration 6744, loss = 700.37284549\n",
            "Iteration 6745, loss = 700.33729909\n",
            "Iteration 6746, loss = 700.20274455\n",
            "Iteration 6747, loss = 699.88515330\n",
            "Iteration 6748, loss = 700.02847115\n",
            "Iteration 6749, loss = 699.65959513\n",
            "Iteration 6750, loss = 699.67791189\n",
            "Iteration 6751, loss = 699.69782254\n",
            "Iteration 6752, loss = 699.41285191\n",
            "Iteration 6753, loss = 699.44185328\n",
            "Iteration 6754, loss = 699.16419629\n",
            "Iteration 6755, loss = 698.69448267\n",
            "Iteration 6756, loss = 698.64692161\n",
            "Iteration 6757, loss = 698.15636808\n",
            "Iteration 6758, loss = 698.58959426\n",
            "Iteration 6759, loss = 698.13355014\n",
            "Iteration 6760, loss = 697.75003602\n",
            "Iteration 6761, loss = 697.75854821\n",
            "Iteration 6762, loss = 697.43137130\n",
            "Iteration 6763, loss = 697.43126662\n",
            "Iteration 6764, loss = 697.47808495\n",
            "Iteration 6765, loss = 697.39656051\n",
            "Iteration 6766, loss = 697.13848101\n",
            "Iteration 6767, loss = 697.09878853\n",
            "Iteration 6768, loss = 696.57721103\n",
            "Iteration 6769, loss = 697.16256459\n",
            "Iteration 6770, loss = 696.89144974\n",
            "Iteration 6771, loss = 695.73704590\n",
            "Iteration 6772, loss = 695.96060387\n",
            "Iteration 6773, loss = 695.69632274\n",
            "Iteration 6774, loss = 695.18391800\n",
            "Iteration 6775, loss = 695.21221866\n",
            "Iteration 6776, loss = 694.96032137\n",
            "Iteration 6777, loss = 694.90370603\n",
            "Iteration 6778, loss = 695.11383995\n",
            "Iteration 6779, loss = 694.55019529\n",
            "Iteration 6780, loss = 694.51405288\n",
            "Iteration 6781, loss = 694.79543855\n",
            "Iteration 6782, loss = 694.19620114\n",
            "Iteration 6783, loss = 694.14182227\n",
            "Iteration 6784, loss = 694.10729026\n",
            "Iteration 6785, loss = 693.35108275\n",
            "Iteration 6786, loss = 693.64256461\n",
            "Iteration 6787, loss = 693.12398386\n",
            "Iteration 6788, loss = 693.04634512\n",
            "Iteration 6789, loss = 692.80418357\n",
            "Iteration 6790, loss = 692.73644023\n",
            "Iteration 6791, loss = 692.59223652\n",
            "Iteration 6792, loss = 693.00292616\n",
            "Iteration 6793, loss = 692.46598782\n",
            "Iteration 6794, loss = 692.51544861\n",
            "Iteration 6795, loss = 692.13121070\n",
            "Iteration 6796, loss = 692.37576380\n",
            "Iteration 6797, loss = 691.40343323\n",
            "Iteration 6798, loss = 691.41686915\n",
            "Iteration 6799, loss = 691.76698304\n",
            "Iteration 6800, loss = 691.65285124\n",
            "Iteration 6801, loss = 690.81438455\n",
            "Iteration 6802, loss = 690.66107595\n",
            "Iteration 6803, loss = 690.76389698\n",
            "Iteration 6804, loss = 690.47755433\n",
            "Iteration 6805, loss = 690.02913609\n",
            "Iteration 6806, loss = 690.04169499\n",
            "Iteration 6807, loss = 689.96942371\n",
            "Iteration 6808, loss = 689.72390454\n",
            "Iteration 6809, loss = 689.68561357\n",
            "Iteration 6810, loss = 689.31288713\n",
            "Iteration 6811, loss = 689.42390679\n",
            "Iteration 6812, loss = 688.94090808\n",
            "Iteration 6813, loss = 689.21529051\n",
            "Iteration 6814, loss = 689.15731795\n",
            "Iteration 6815, loss = 688.75991635\n",
            "Iteration 6816, loss = 688.29733856\n",
            "Iteration 6817, loss = 687.84567793\n",
            "Iteration 6818, loss = 688.00107015\n",
            "Iteration 6819, loss = 688.04560501\n",
            "Iteration 6820, loss = 687.50212343\n",
            "Iteration 6821, loss = 687.42407276\n",
            "Iteration 6822, loss = 687.21024664\n",
            "Iteration 6823, loss = 687.21398241\n",
            "Iteration 6824, loss = 686.98461152\n",
            "Iteration 6825, loss = 687.11198852\n",
            "Iteration 6826, loss = 686.54842101\n",
            "Iteration 6827, loss = 686.54461754\n",
            "Iteration 6828, loss = 686.35349929\n",
            "Iteration 6829, loss = 686.38301125\n",
            "Iteration 6830, loss = 685.93703503\n",
            "Iteration 6831, loss = 685.72034303\n",
            "Iteration 6832, loss = 685.71315282\n",
            "Iteration 6833, loss = 685.38933696\n",
            "Iteration 6834, loss = 685.26986027\n",
            "Iteration 6835, loss = 685.12412297\n",
            "Iteration 6836, loss = 684.80013515\n",
            "Iteration 6837, loss = 684.48189991\n",
            "Iteration 6838, loss = 684.59824877\n",
            "Iteration 6839, loss = 684.93986763\n",
            "Iteration 6840, loss = 684.56856514\n",
            "Iteration 6841, loss = 684.40151945\n",
            "Iteration 6842, loss = 683.85094168\n",
            "Iteration 6843, loss = 683.50867496\n",
            "Iteration 6844, loss = 684.18455776\n",
            "Iteration 6845, loss = 684.03586228\n",
            "Iteration 6846, loss = 683.45120537\n",
            "Iteration 6847, loss = 682.96994114\n",
            "Iteration 6848, loss = 682.69918853\n",
            "Iteration 6849, loss = 683.44429446\n",
            "Iteration 6850, loss = 682.73744666\n",
            "Iteration 6851, loss = 682.62311288\n",
            "Iteration 6852, loss = 682.76237667\n",
            "Iteration 6853, loss = 681.94025138\n",
            "Iteration 6854, loss = 681.98150641\n",
            "Iteration 6855, loss = 681.82746406\n",
            "Iteration 6856, loss = 681.76795087\n",
            "Iteration 6857, loss = 681.06558438\n",
            "Iteration 6858, loss = 681.21997423\n",
            "Iteration 6859, loss = 681.74895390\n",
            "Iteration 6860, loss = 681.28980319\n",
            "Iteration 6861, loss = 680.62905177\n",
            "Iteration 6862, loss = 681.38191170\n",
            "Iteration 6863, loss = 680.52309561\n",
            "Iteration 6864, loss = 680.35449644\n",
            "Iteration 6865, loss = 680.49476204\n",
            "Iteration 6866, loss = 679.94043189\n",
            "Iteration 6867, loss = 679.59762160\n",
            "Iteration 6868, loss = 679.47204786\n",
            "Iteration 6869, loss = 679.53187040\n",
            "Iteration 6870, loss = 679.17946553\n",
            "Iteration 6871, loss = 679.14908705\n",
            "Iteration 6872, loss = 678.72909411\n",
            "Iteration 6873, loss = 678.87681840\n",
            "Iteration 6874, loss = 678.27302482\n",
            "Iteration 6875, loss = 678.58203219\n",
            "Iteration 6876, loss = 678.13271792\n",
            "Iteration 6877, loss = 677.95319008\n",
            "Iteration 6878, loss = 677.94094163\n",
            "Iteration 6879, loss = 678.07229188\n",
            "Iteration 6880, loss = 677.76878058\n",
            "Iteration 6881, loss = 677.87776448\n",
            "Iteration 6882, loss = 677.02634316\n",
            "Iteration 6883, loss = 677.06643669\n",
            "Iteration 6884, loss = 677.27505025\n",
            "Iteration 6885, loss = 676.91904849\n",
            "Iteration 6886, loss = 676.34113535\n",
            "Iteration 6887, loss = 676.24908553\n",
            "Iteration 6888, loss = 676.13268566\n",
            "Iteration 6889, loss = 676.14920686\n",
            "Iteration 6890, loss = 676.26560519\n",
            "Iteration 6891, loss = 675.52437243\n",
            "Iteration 6892, loss = 676.37262756\n",
            "Iteration 6893, loss = 675.20766137\n",
            "Iteration 6894, loss = 675.14281961\n",
            "Iteration 6895, loss = 675.15391029\n",
            "Iteration 6896, loss = 674.97406000\n",
            "Iteration 6897, loss = 674.65865438\n",
            "Iteration 6898, loss = 674.70619838\n",
            "Iteration 6899, loss = 674.88419667\n",
            "Iteration 6900, loss = 674.21774322\n",
            "Iteration 6901, loss = 674.05453312\n",
            "Iteration 6902, loss = 673.68149791\n",
            "Iteration 6903, loss = 673.92508953\n",
            "Iteration 6904, loss = 673.45491374\n",
            "Iteration 6905, loss = 673.35419705\n",
            "Iteration 6906, loss = 673.67043791\n",
            "Iteration 6907, loss = 673.73862511\n",
            "Iteration 6908, loss = 673.29952327\n",
            "Iteration 6909, loss = 672.74343864\n",
            "Iteration 6910, loss = 674.41640361\n",
            "Iteration 6911, loss = 672.60948983\n",
            "Iteration 6912, loss = 671.85805265\n",
            "Iteration 6913, loss = 671.92303656\n",
            "Iteration 6914, loss = 672.34502383\n",
            "Iteration 6915, loss = 672.41511874\n",
            "Iteration 6916, loss = 671.43212214\n",
            "Iteration 6917, loss = 671.32755905\n",
            "Iteration 6918, loss = 671.84215997\n",
            "Iteration 6919, loss = 671.70238526\n",
            "Iteration 6920, loss = 671.31702913\n",
            "Iteration 6921, loss = 671.88802232\n",
            "Iteration 6922, loss = 671.01755809\n",
            "Iteration 6923, loss = 670.42329181\n",
            "Iteration 6924, loss = 670.39360448\n",
            "Iteration 6925, loss = 670.25659891\n",
            "Iteration 6926, loss = 670.93287318\n",
            "Iteration 6927, loss = 670.20208492\n",
            "Iteration 6928, loss = 669.54024053\n",
            "Iteration 6929, loss = 669.49280578\n",
            "Iteration 6930, loss = 669.71296549\n",
            "Iteration 6931, loss = 669.48569846\n",
            "Iteration 6932, loss = 669.41000876\n",
            "Iteration 6933, loss = 668.96152827\n",
            "Iteration 6934, loss = 668.77974768\n",
            "Iteration 6935, loss = 668.48650318\n",
            "Iteration 6936, loss = 668.60966314\n",
            "Iteration 6937, loss = 668.38104946\n",
            "Iteration 6938, loss = 668.52141065\n",
            "Iteration 6939, loss = 668.20340016\n",
            "Iteration 6940, loss = 667.80257556\n",
            "Iteration 6941, loss = 668.46419425\n",
            "Iteration 6942, loss = 667.29928259\n",
            "Iteration 6943, loss = 667.15242200\n",
            "Iteration 6944, loss = 667.46793362\n",
            "Iteration 6945, loss = 667.64454682\n",
            "Iteration 6946, loss = 667.06834920\n",
            "Iteration 6947, loss = 667.20871111\n",
            "Iteration 6948, loss = 666.38789073\n",
            "Iteration 6949, loss = 666.31793584\n",
            "Iteration 6950, loss = 666.83214493\n",
            "Iteration 6951, loss = 665.90736765\n",
            "Iteration 6952, loss = 666.61100762\n",
            "Iteration 6953, loss = 666.06078676\n",
            "Iteration 6954, loss = 665.80823396\n",
            "Iteration 6955, loss = 665.49434013\n",
            "Iteration 6956, loss = 664.94798477\n",
            "Iteration 6957, loss = 665.67825154\n",
            "Iteration 6958, loss = 665.36326186\n",
            "Iteration 6959, loss = 664.68281759\n",
            "Iteration 6960, loss = 665.86889673\n",
            "Iteration 6961, loss = 664.54064612\n",
            "Iteration 6962, loss = 664.18948193\n",
            "Iteration 6963, loss = 663.86874105\n",
            "Iteration 6964, loss = 664.30494037\n",
            "Iteration 6965, loss = 663.85592538\n",
            "Iteration 6966, loss = 663.74849561\n",
            "Iteration 6967, loss = 663.24487044\n",
            "Iteration 6968, loss = 663.31120484\n",
            "Iteration 6969, loss = 663.12847512\n",
            "Iteration 6970, loss = 663.16848050\n",
            "Iteration 6971, loss = 662.89672448\n",
            "Iteration 6972, loss = 662.88132332\n",
            "Iteration 6973, loss = 662.24372856\n",
            "Iteration 6974, loss = 662.60276268\n",
            "Iteration 6975, loss = 662.26085449\n",
            "Iteration 6976, loss = 661.97016599\n",
            "Iteration 6977, loss = 661.86795564\n",
            "Iteration 6978, loss = 661.92920526\n",
            "Iteration 6979, loss = 661.47038552\n",
            "Iteration 6980, loss = 661.24092833\n",
            "Iteration 6981, loss = 661.10444501\n",
            "Iteration 6982, loss = 660.86343434\n",
            "Iteration 6983, loss = 660.93911380\n",
            "Iteration 6984, loss = 660.83331522\n",
            "Iteration 6985, loss = 660.53728287\n",
            "Iteration 6986, loss = 660.23803991\n",
            "Iteration 6987, loss = 660.23083769\n",
            "Iteration 6988, loss = 660.10723827\n",
            "Iteration 6989, loss = 659.64679288\n",
            "Iteration 6990, loss = 659.99939204\n",
            "Iteration 6991, loss = 659.83599599\n",
            "Iteration 6992, loss = 659.86732687\n",
            "Iteration 6993, loss = 659.38345510\n",
            "Iteration 6994, loss = 659.83240020\n",
            "Iteration 6995, loss = 659.22892819\n",
            "Iteration 6996, loss = 658.62494217\n",
            "Iteration 6997, loss = 658.80095657\n",
            "Iteration 6998, loss = 658.93920549\n",
            "Iteration 6999, loss = 658.26846360\n",
            "Iteration 7000, loss = 658.08829389\n",
            "Iteration 7001, loss = 658.16869462\n",
            "Iteration 7002, loss = 657.94027323\n",
            "Iteration 7003, loss = 657.60588484\n",
            "Iteration 7004, loss = 657.35945277\n",
            "Iteration 7005, loss = 657.21214311\n",
            "Iteration 7006, loss = 657.16799110\n",
            "Iteration 7007, loss = 657.19839290\n",
            "Iteration 7008, loss = 656.84743844\n",
            "Iteration 7009, loss = 656.66817227\n",
            "Iteration 7010, loss = 656.89874951\n",
            "Iteration 7011, loss = 656.11850966\n",
            "Iteration 7012, loss = 655.97021681\n",
            "Iteration 7013, loss = 655.99734304\n",
            "Iteration 7014, loss = 656.10396842\n",
            "Iteration 7015, loss = 656.27865516\n",
            "Iteration 7016, loss = 655.80319945\n",
            "Iteration 7017, loss = 655.36375532\n",
            "Iteration 7018, loss = 655.57668135\n",
            "Iteration 7019, loss = 655.25178271\n",
            "Iteration 7020, loss = 655.75729720\n",
            "Iteration 7021, loss = 654.93205306\n",
            "Iteration 7022, loss = 654.64779320\n",
            "Iteration 7023, loss = 654.95949392\n",
            "Iteration 7024, loss = 654.64511710\n",
            "Iteration 7025, loss = 654.11650578\n",
            "Iteration 7026, loss = 653.92305905\n",
            "Iteration 7027, loss = 654.19083027\n",
            "Iteration 7028, loss = 653.79175330\n",
            "Iteration 7029, loss = 654.33715149\n",
            "Iteration 7030, loss = 653.86900187\n",
            "Iteration 7031, loss = 653.38579162\n",
            "Iteration 7032, loss = 653.13783329\n",
            "Iteration 7033, loss = 652.90566843\n",
            "Iteration 7034, loss = 653.04283845\n",
            "Iteration 7035, loss = 652.84187520\n",
            "Iteration 7036, loss = 652.59184042\n",
            "Iteration 7037, loss = 652.79551343\n",
            "Iteration 7038, loss = 652.57454923\n",
            "Iteration 7039, loss = 652.26714721\n",
            "Iteration 7040, loss = 651.67632332\n",
            "Iteration 7041, loss = 651.57269593\n",
            "Iteration 7042, loss = 651.59643004\n",
            "Iteration 7043, loss = 652.85046848\n",
            "Iteration 7044, loss = 651.20811992\n",
            "Iteration 7045, loss = 651.35389988\n",
            "Iteration 7046, loss = 651.29581168\n",
            "Iteration 7047, loss = 650.87313727\n",
            "Iteration 7048, loss = 650.39994504\n",
            "Iteration 7049, loss = 650.54069009\n",
            "Iteration 7050, loss = 650.16266080\n",
            "Iteration 7051, loss = 650.30315950\n",
            "Iteration 7052, loss = 650.19037489\n",
            "Iteration 7053, loss = 650.69959652\n",
            "Iteration 7054, loss = 650.16602986\n",
            "Iteration 7055, loss = 649.95999272\n",
            "Iteration 7056, loss = 649.75763106\n",
            "Iteration 7057, loss = 649.38646993\n",
            "Iteration 7058, loss = 649.89273802\n",
            "Iteration 7059, loss = 649.12185088\n",
            "Iteration 7060, loss = 648.91542693\n",
            "Iteration 7061, loss = 648.89123941\n",
            "Iteration 7062, loss = 648.70902733\n",
            "Iteration 7063, loss = 648.45991135\n",
            "Iteration 7064, loss = 648.71133970\n",
            "Iteration 7065, loss = 648.80632197\n",
            "Iteration 7066, loss = 648.39588271\n",
            "Iteration 7067, loss = 647.65257594\n",
            "Iteration 7068, loss = 647.46081705\n",
            "Iteration 7069, loss = 647.47437508\n",
            "Iteration 7070, loss = 647.43135367\n",
            "Iteration 7071, loss = 647.72626292\n",
            "Iteration 7072, loss = 647.54956814\n",
            "Iteration 7073, loss = 647.17816008\n",
            "Iteration 7074, loss = 647.47362939\n",
            "Iteration 7075, loss = 646.40201966\n",
            "Iteration 7076, loss = 646.54935742\n",
            "Iteration 7077, loss = 646.29471435\n",
            "Iteration 7078, loss = 646.01994585\n",
            "Iteration 7079, loss = 646.32711447\n",
            "Iteration 7080, loss = 646.46465290\n",
            "Iteration 7081, loss = 645.83722538\n",
            "Iteration 7082, loss = 645.25119988\n",
            "Iteration 7083, loss = 645.13119518\n",
            "Iteration 7084, loss = 645.37728020\n",
            "Iteration 7085, loss = 645.06499220\n",
            "Iteration 7086, loss = 644.93158758\n",
            "Iteration 7087, loss = 644.56266010\n",
            "Iteration 7088, loss = 644.40735501\n",
            "Iteration 7089, loss = 644.57567932\n",
            "Iteration 7090, loss = 644.07430794\n",
            "Iteration 7091, loss = 644.30941537\n",
            "Iteration 7092, loss = 643.56063743\n",
            "Iteration 7093, loss = 643.72920791\n",
            "Iteration 7094, loss = 643.90851224\n",
            "Iteration 7095, loss = 643.69188365\n",
            "Iteration 7096, loss = 643.86879887\n",
            "Iteration 7097, loss = 643.29136872\n",
            "Iteration 7098, loss = 643.42234649\n",
            "Iteration 7099, loss = 643.01600715\n",
            "Iteration 7100, loss = 643.03599621\n",
            "Iteration 7101, loss = 642.34635572\n",
            "Iteration 7102, loss = 642.26733157\n",
            "Iteration 7103, loss = 642.78610738\n",
            "Iteration 7104, loss = 642.30905246\n",
            "Iteration 7105, loss = 643.13751429\n",
            "Iteration 7106, loss = 642.00252996\n",
            "Iteration 7107, loss = 641.75613101\n",
            "Iteration 7108, loss = 641.38412323\n",
            "Iteration 7109, loss = 641.31115142\n",
            "Iteration 7110, loss = 641.09727794\n",
            "Iteration 7111, loss = 641.18317774\n",
            "Iteration 7112, loss = 641.24801758\n",
            "Iteration 7113, loss = 641.06034259\n",
            "Iteration 7114, loss = 640.43808201\n",
            "Iteration 7115, loss = 640.49757980\n",
            "Iteration 7116, loss = 640.38580586\n",
            "Iteration 7117, loss = 640.11080622\n",
            "Iteration 7118, loss = 640.34308233\n",
            "Iteration 7119, loss = 639.92985721\n",
            "Iteration 7120, loss = 639.81519237\n",
            "Iteration 7121, loss = 640.30602492\n",
            "Iteration 7122, loss = 639.54901467\n",
            "Iteration 7123, loss = 639.86343325\n",
            "Iteration 7124, loss = 639.31703551\n",
            "Iteration 7125, loss = 639.21110467\n",
            "Iteration 7126, loss = 639.25168778\n",
            "Iteration 7127, loss = 638.74073609\n",
            "Iteration 7128, loss = 638.39637391\n",
            "Iteration 7129, loss = 638.68901715\n",
            "Iteration 7130, loss = 638.78161189\n",
            "Iteration 7131, loss = 638.33772206\n",
            "Iteration 7132, loss = 638.88831967\n",
            "Iteration 7133, loss = 637.85575392\n",
            "Iteration 7134, loss = 637.10781352\n",
            "Iteration 7135, loss = 637.87526913\n",
            "Iteration 7136, loss = 638.23056409\n",
            "Iteration 7137, loss = 638.01956585\n",
            "Iteration 7138, loss = 637.35188108\n",
            "Iteration 7139, loss = 637.22160009\n",
            "Iteration 7140, loss = 637.95281661\n",
            "Iteration 7141, loss = 637.08305328\n",
            "Iteration 7142, loss = 636.55632365\n",
            "Iteration 7143, loss = 636.45435827\n",
            "Iteration 7144, loss = 636.87446733\n",
            "Iteration 7145, loss = 636.58393673\n",
            "Iteration 7146, loss = 635.82159398\n",
            "Iteration 7147, loss = 635.75378147\n",
            "Iteration 7148, loss = 635.96025283\n",
            "Iteration 7149, loss = 636.05244622\n",
            "Iteration 7150, loss = 635.50987082\n",
            "Iteration 7151, loss = 635.38347538\n",
            "Iteration 7152, loss = 635.22135925\n",
            "Iteration 7153, loss = 635.09015854\n",
            "Iteration 7154, loss = 634.92579515\n",
            "Iteration 7155, loss = 634.67280798\n",
            "Iteration 7156, loss = 634.41779620\n",
            "Iteration 7157, loss = 634.30408903\n",
            "Iteration 7158, loss = 634.41353266\n",
            "Iteration 7159, loss = 634.17369999\n",
            "Iteration 7160, loss = 634.08678254\n",
            "Iteration 7161, loss = 633.73195081\n",
            "Iteration 7162, loss = 633.82759235\n",
            "Iteration 7163, loss = 633.58576644\n",
            "Iteration 7164, loss = 634.33044213\n",
            "Iteration 7165, loss = 633.53271783\n",
            "Iteration 7166, loss = 632.89118961\n",
            "Iteration 7167, loss = 633.22240855\n",
            "Iteration 7168, loss = 633.12057278\n",
            "Iteration 7169, loss = 632.71273485\n",
            "Iteration 7170, loss = 632.60647161\n",
            "Iteration 7171, loss = 633.15974047\n",
            "Iteration 7172, loss = 632.08926131\n",
            "Iteration 7173, loss = 632.12285632\n",
            "Iteration 7174, loss = 632.16161478\n",
            "Iteration 7175, loss = 632.09212446\n",
            "Iteration 7176, loss = 631.51956808\n",
            "Iteration 7177, loss = 631.35269287\n",
            "Iteration 7178, loss = 631.51390698\n",
            "Iteration 7179, loss = 630.97849857\n",
            "Iteration 7180, loss = 631.49472606\n",
            "Iteration 7181, loss = 630.77821076\n",
            "Iteration 7182, loss = 631.21847405\n",
            "Iteration 7183, loss = 630.98035041\n",
            "Iteration 7184, loss = 630.37175583\n",
            "Iteration 7185, loss = 630.33008890\n",
            "Iteration 7186, loss = 630.22228197\n",
            "Iteration 7187, loss = 630.28544621\n",
            "Iteration 7188, loss = 629.86046916\n",
            "Iteration 7189, loss = 629.64201230\n",
            "Iteration 7190, loss = 630.10600148\n",
            "Iteration 7191, loss = 629.32794600\n",
            "Iteration 7192, loss = 631.75654856\n",
            "Iteration 7193, loss = 629.24327185\n",
            "Iteration 7194, loss = 629.14871257\n",
            "Iteration 7195, loss = 629.12462879\n",
            "Iteration 7196, loss = 628.94296809\n",
            "Iteration 7197, loss = 629.17409894\n",
            "Iteration 7198, loss = 628.93126435\n",
            "Iteration 7199, loss = 628.57403650\n",
            "Iteration 7200, loss = 628.54551459\n",
            "Iteration 7201, loss = 628.11637930\n",
            "Iteration 7202, loss = 628.15526369\n",
            "Iteration 7203, loss = 628.02808683\n",
            "Iteration 7204, loss = 627.45666951\n",
            "Iteration 7205, loss = 628.26965439\n",
            "Iteration 7206, loss = 627.37346328\n",
            "Iteration 7207, loss = 627.04360783\n",
            "Iteration 7208, loss = 626.70788269\n",
            "Iteration 7209, loss = 627.15151861\n",
            "Iteration 7210, loss = 627.01985461\n",
            "Iteration 7211, loss = 626.63854429\n",
            "Iteration 7212, loss = 626.35342176\n",
            "Iteration 7213, loss = 626.39222281\n",
            "Iteration 7214, loss = 626.31849471\n",
            "Iteration 7215, loss = 626.38141619\n",
            "Iteration 7216, loss = 625.86478014\n",
            "Iteration 7217, loss = 625.86510277\n",
            "Iteration 7218, loss = 625.64087282\n",
            "Iteration 7219, loss = 625.56972369\n",
            "Iteration 7220, loss = 625.84170674\n",
            "Iteration 7221, loss = 625.14132458\n",
            "Iteration 7222, loss = 625.01480065\n",
            "Iteration 7223, loss = 625.68462747\n",
            "Iteration 7224, loss = 624.55208013\n",
            "Iteration 7225, loss = 625.05847859\n",
            "Iteration 7226, loss = 625.27955698\n",
            "Iteration 7227, loss = 624.12127311\n",
            "Iteration 7228, loss = 624.51237680\n",
            "Iteration 7229, loss = 624.54332619\n",
            "Iteration 7230, loss = 624.14964900\n",
            "Iteration 7231, loss = 624.33402397\n",
            "Iteration 7232, loss = 623.73585445\n",
            "Iteration 7233, loss = 623.96978416\n",
            "Iteration 7234, loss = 623.95036456\n",
            "Iteration 7235, loss = 623.49234245\n",
            "Iteration 7236, loss = 623.24973856\n",
            "Iteration 7237, loss = 623.59298349\n",
            "Iteration 7238, loss = 622.82048888\n",
            "Iteration 7239, loss = 622.72682972\n",
            "Iteration 7240, loss = 622.50223905\n",
            "Iteration 7241, loss = 623.01277710\n",
            "Iteration 7242, loss = 622.64429940\n",
            "Iteration 7243, loss = 622.06587150\n",
            "Iteration 7244, loss = 621.85712175\n",
            "Iteration 7245, loss = 621.96180678\n",
            "Iteration 7246, loss = 621.92524115\n",
            "Iteration 7247, loss = 621.57398453\n",
            "Iteration 7248, loss = 621.26213458\n",
            "Iteration 7249, loss = 621.89435439\n",
            "Iteration 7250, loss = 621.21784422\n",
            "Iteration 7251, loss = 621.34628137\n",
            "Iteration 7252, loss = 620.71191114\n",
            "Iteration 7253, loss = 620.83186924\n",
            "Iteration 7254, loss = 620.81761406\n",
            "Iteration 7255, loss = 620.65179265\n",
            "Iteration 7256, loss = 620.18960660\n",
            "Iteration 7257, loss = 620.04925787\n",
            "Iteration 7258, loss = 621.23468458\n",
            "Iteration 7259, loss = 620.14211576\n",
            "Iteration 7260, loss = 620.12582797\n",
            "Iteration 7261, loss = 620.12685318\n",
            "Iteration 7262, loss = 620.03533945\n",
            "Iteration 7263, loss = 619.37271197\n",
            "Iteration 7264, loss = 620.19747705\n",
            "Iteration 7265, loss = 619.35439785\n",
            "Iteration 7266, loss = 620.07721714\n",
            "Iteration 7267, loss = 619.23826242\n",
            "Iteration 7268, loss = 618.90679116\n",
            "Iteration 7269, loss = 618.39701748\n",
            "Iteration 7270, loss = 618.29864763\n",
            "Iteration 7271, loss = 619.29566181\n",
            "Iteration 7272, loss = 618.25305861\n",
            "Iteration 7273, loss = 617.79464656\n",
            "Iteration 7274, loss = 617.76982319\n",
            "Iteration 7275, loss = 617.81419676\n",
            "Iteration 7276, loss = 618.17422765\n",
            "Iteration 7277, loss = 617.25695774\n",
            "Iteration 7278, loss = 617.15840168\n",
            "Iteration 7279, loss = 617.05959637\n",
            "Iteration 7280, loss = 616.76632454\n",
            "Iteration 7281, loss = 616.96568011\n",
            "Iteration 7282, loss = 616.94038830\n",
            "Iteration 7283, loss = 616.78157854\n",
            "Iteration 7284, loss = 616.36225867\n",
            "Iteration 7285, loss = 616.03690946\n",
            "Iteration 7286, loss = 616.16354267\n",
            "Iteration 7287, loss = 616.06930727\n",
            "Iteration 7288, loss = 615.95206534\n",
            "Iteration 7289, loss = 616.42141599\n",
            "Iteration 7290, loss = 615.69916953\n",
            "Iteration 7291, loss = 615.23394590\n",
            "Iteration 7292, loss = 615.36387145\n",
            "Iteration 7293, loss = 615.51308074\n",
            "Iteration 7294, loss = 615.47205529\n",
            "Iteration 7295, loss = 614.83234292\n",
            "Iteration 7296, loss = 614.59713285\n",
            "Iteration 7297, loss = 614.89072400\n",
            "Iteration 7298, loss = 614.86484878\n",
            "Iteration 7299, loss = 614.25733789\n",
            "Iteration 7300, loss = 614.18411720\n",
            "Iteration 7301, loss = 613.82120185\n",
            "Iteration 7302, loss = 613.87559262\n",
            "Iteration 7303, loss = 613.42172357\n",
            "Iteration 7304, loss = 613.47422634\n",
            "Iteration 7305, loss = 613.72410322\n",
            "Iteration 7306, loss = 613.46328603\n",
            "Iteration 7307, loss = 613.08976319\n",
            "Iteration 7308, loss = 613.10715415\n",
            "Iteration 7309, loss = 612.99763200\n",
            "Iteration 7310, loss = 612.63254048\n",
            "Iteration 7311, loss = 612.65068107\n",
            "Iteration 7312, loss = 612.47685882\n",
            "Iteration 7313, loss = 612.67500602\n",
            "Iteration 7314, loss = 611.97822810\n",
            "Iteration 7315, loss = 612.15720781\n",
            "Iteration 7316, loss = 612.02971434\n",
            "Iteration 7317, loss = 612.01634834\n",
            "Iteration 7318, loss = 611.72063051\n",
            "Iteration 7319, loss = 611.49300489\n",
            "Iteration 7320, loss = 611.52668067\n",
            "Iteration 7321, loss = 611.74962925\n",
            "Iteration 7322, loss = 611.36204079\n",
            "Iteration 7323, loss = 610.91197063\n",
            "Iteration 7324, loss = 611.03524903\n",
            "Iteration 7325, loss = 610.57811190\n",
            "Iteration 7326, loss = 610.93749436\n",
            "Iteration 7327, loss = 610.44468904\n",
            "Iteration 7328, loss = 610.30432076\n",
            "Iteration 7329, loss = 610.56036598\n",
            "Iteration 7330, loss = 609.66508606\n",
            "Iteration 7331, loss = 609.64925962\n",
            "Iteration 7332, loss = 609.65677836\n",
            "Iteration 7333, loss = 609.47288486\n",
            "Iteration 7334, loss = 609.22057499\n",
            "Iteration 7335, loss = 609.41467934\n",
            "Iteration 7336, loss = 609.25086115\n",
            "Iteration 7337, loss = 609.24873965\n",
            "Iteration 7338, loss = 609.12819417\n",
            "Iteration 7339, loss = 608.90267749\n",
            "Iteration 7340, loss = 608.97508040\n",
            "Iteration 7341, loss = 608.53345038\n",
            "Iteration 7342, loss = 608.26927021\n",
            "Iteration 7343, loss = 608.27303896\n",
            "Iteration 7344, loss = 608.11352031\n",
            "Iteration 7345, loss = 608.31828203\n",
            "Iteration 7346, loss = 607.55584512\n",
            "Iteration 7347, loss = 607.49277859\n",
            "Iteration 7348, loss = 607.77849231\n",
            "Iteration 7349, loss = 607.63012747\n",
            "Iteration 7350, loss = 607.71108840\n",
            "Iteration 7351, loss = 606.94850984\n",
            "Iteration 7352, loss = 607.04889915\n",
            "Iteration 7353, loss = 606.84343166\n",
            "Iteration 7354, loss = 606.92552671\n",
            "Iteration 7355, loss = 607.57572953\n",
            "Iteration 7356, loss = 606.62849259\n",
            "Iteration 7357, loss = 606.36932540\n",
            "Iteration 7358, loss = 606.09915327\n",
            "Iteration 7359, loss = 606.24200733\n",
            "Iteration 7360, loss = 605.99100586\n",
            "Iteration 7361, loss = 605.78841447\n",
            "Iteration 7362, loss = 606.20132422\n",
            "Iteration 7363, loss = 605.25170127\n",
            "Iteration 7364, loss = 606.02372649\n",
            "Iteration 7365, loss = 605.69776056\n",
            "Iteration 7366, loss = 606.32297670\n",
            "Iteration 7367, loss = 605.16540768\n",
            "Iteration 7368, loss = 605.73202940\n",
            "Iteration 7369, loss = 604.80244572\n",
            "Iteration 7370, loss = 604.73694324\n",
            "Iteration 7371, loss = 604.49371502\n",
            "Iteration 7372, loss = 604.72929297\n",
            "Iteration 7373, loss = 604.97578348\n",
            "Iteration 7374, loss = 604.23336004\n",
            "Iteration 7375, loss = 604.16350624\n",
            "Iteration 7376, loss = 604.06322261\n",
            "Iteration 7377, loss = 603.56350020\n",
            "Iteration 7378, loss = 603.36386210\n",
            "Iteration 7379, loss = 603.20886922\n",
            "Iteration 7380, loss = 603.21220514\n",
            "Iteration 7381, loss = 603.00256260\n",
            "Iteration 7382, loss = 602.77380499\n",
            "Iteration 7383, loss = 602.92059054\n",
            "Iteration 7384, loss = 603.31047508\n",
            "Iteration 7385, loss = 603.44914891\n",
            "Iteration 7386, loss = 602.46440176\n",
            "Iteration 7387, loss = 602.11192420\n",
            "Iteration 7388, loss = 602.60471358\n",
            "Iteration 7389, loss = 602.17183181\n",
            "Iteration 7390, loss = 602.00054351\n",
            "Iteration 7391, loss = 601.81890845\n",
            "Iteration 7392, loss = 601.93224559\n",
            "Iteration 7393, loss = 601.71115169\n",
            "Iteration 7394, loss = 601.57696491\n",
            "Iteration 7395, loss = 601.66543138\n",
            "Iteration 7396, loss = 601.53392532\n",
            "Iteration 7397, loss = 600.95680467\n",
            "Iteration 7398, loss = 600.74564731\n",
            "Iteration 7399, loss = 600.85629122\n",
            "Iteration 7400, loss = 600.68767122\n",
            "Iteration 7401, loss = 600.65136136\n",
            "Iteration 7402, loss = 600.27199265\n",
            "Iteration 7403, loss = 600.25992969\n",
            "Iteration 7404, loss = 600.04939592\n",
            "Iteration 7405, loss = 599.79122701\n",
            "Iteration 7406, loss = 599.77498370\n",
            "Iteration 7407, loss = 599.99445527\n",
            "Iteration 7408, loss = 599.48938423\n",
            "Iteration 7409, loss = 599.50779762\n",
            "Iteration 7410, loss = 599.30552125\n",
            "Iteration 7411, loss = 599.26695578\n",
            "Iteration 7412, loss = 598.86167635\n",
            "Iteration 7413, loss = 599.39501599\n",
            "Iteration 7414, loss = 599.12898218\n",
            "Iteration 7415, loss = 598.35048953\n",
            "Iteration 7416, loss = 598.20497468\n",
            "Iteration 7417, loss = 598.30519589\n",
            "Iteration 7418, loss = 598.09719555\n",
            "Iteration 7419, loss = 598.09008295\n",
            "Iteration 7420, loss = 597.99238757\n",
            "Iteration 7421, loss = 597.70819490\n",
            "Iteration 7422, loss = 597.98444663\n",
            "Iteration 7423, loss = 597.59797326\n",
            "Iteration 7424, loss = 597.30584359\n",
            "Iteration 7425, loss = 597.03278228\n",
            "Iteration 7426, loss = 597.88938122\n",
            "Iteration 7427, loss = 596.93512479\n",
            "Iteration 7428, loss = 596.78205294\n",
            "Iteration 7429, loss = 596.69397581\n",
            "Iteration 7430, loss = 596.46157788\n",
            "Iteration 7431, loss = 596.46184135\n",
            "Iteration 7432, loss = 596.29342036\n",
            "Iteration 7433, loss = 596.21910573\n",
            "Iteration 7434, loss = 595.84170501\n",
            "Iteration 7435, loss = 596.07929750\n",
            "Iteration 7436, loss = 595.68441750\n",
            "Iteration 7437, loss = 595.83346401\n",
            "Iteration 7438, loss = 595.85335960\n",
            "Iteration 7439, loss = 595.66493777\n",
            "Iteration 7440, loss = 594.89187573\n",
            "Iteration 7441, loss = 595.57498505\n",
            "Iteration 7442, loss = 595.28024227\n",
            "Iteration 7443, loss = 594.75459690\n",
            "Iteration 7444, loss = 595.16921097\n",
            "Iteration 7445, loss = 594.88917764\n",
            "Iteration 7446, loss = 594.26338220\n",
            "Iteration 7447, loss = 594.47278270\n",
            "Iteration 7448, loss = 594.32759217\n",
            "Iteration 7449, loss = 594.26438784\n",
            "Iteration 7450, loss = 594.16406243\n",
            "Iteration 7451, loss = 593.71297212\n",
            "Iteration 7452, loss = 593.58975468\n",
            "Iteration 7453, loss = 593.71228057\n",
            "Iteration 7454, loss = 593.22305124\n",
            "Iteration 7455, loss = 593.46554599\n",
            "Iteration 7456, loss = 592.96370831\n",
            "Iteration 7457, loss = 593.47640811\n",
            "Iteration 7458, loss = 593.14858280\n",
            "Iteration 7459, loss = 592.62479694\n",
            "Iteration 7460, loss = 593.25633934\n",
            "Iteration 7461, loss = 592.79310367\n",
            "Iteration 7462, loss = 592.22315966\n",
            "Iteration 7463, loss = 593.27467573\n",
            "Iteration 7464, loss = 592.34560296\n",
            "Iteration 7465, loss = 592.31483088\n",
            "Iteration 7466, loss = 591.94469180\n",
            "Iteration 7467, loss = 591.67143630\n",
            "Iteration 7468, loss = 591.40641653\n",
            "Iteration 7469, loss = 591.46443881\n",
            "Iteration 7470, loss = 591.03848822\n",
            "Iteration 7471, loss = 591.09718783\n",
            "Iteration 7472, loss = 590.82572989\n",
            "Iteration 7473, loss = 591.25315089\n",
            "Iteration 7474, loss = 590.46477718\n",
            "Iteration 7475, loss = 590.33373240\n",
            "Iteration 7476, loss = 590.37803823\n",
            "Iteration 7477, loss = 590.75496748\n",
            "Iteration 7478, loss = 590.20206741\n",
            "Iteration 7479, loss = 589.88999652\n",
            "Iteration 7480, loss = 589.83824554\n",
            "Iteration 7481, loss = 589.86133781\n",
            "Iteration 7482, loss = 590.03496705\n",
            "Iteration 7483, loss = 589.92429051\n",
            "Iteration 7484, loss = 589.38370890\n",
            "Iteration 7485, loss = 590.03774454\n",
            "Iteration 7486, loss = 589.41368422\n",
            "Iteration 7487, loss = 589.58065164\n",
            "Iteration 7488, loss = 588.79522150\n",
            "Iteration 7489, loss = 589.08299720\n",
            "Iteration 7490, loss = 588.49182789\n",
            "Iteration 7491, loss = 589.63537614\n",
            "Iteration 7492, loss = 588.56720890\n",
            "Iteration 7493, loss = 587.89791967\n",
            "Iteration 7494, loss = 588.15401014\n",
            "Iteration 7495, loss = 588.53922286\n",
            "Iteration 7496, loss = 588.24425779\n",
            "Iteration 7497, loss = 587.71044821\n",
            "Iteration 7498, loss = 587.59364808\n",
            "Iteration 7499, loss = 587.92640179\n",
            "Iteration 7500, loss = 587.70302745\n",
            "0.43910471476405355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd3eBLxle2L6"
      },
      "source": [
        "## SVM <a name=\"svm\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXU9_FNnVPgv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X13uzjdOe4GR"
      },
      "source": [
        "## Decision Tree Classifier <a name=\"dtc\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J09RdbTPe5tu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}